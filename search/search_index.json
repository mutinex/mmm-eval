{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to mmm-eval","text":"<p>An open-source tool for evaluating Marketing Mix Modeling (MMM) frameworks with a comprehensive suite of tests.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Get started with mmm-eval in just a few steps:</p>"},{"location":"#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<p>Prerequisite: Poetry 2.x.x or later is required.</p> <pre><code># Install from GitHub\npoetry add git+https://github.com/Mutiny-Group/mmm-eval.git\n\n# Or clone and install locally\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npoetry install\n</code></pre>"},{"location":"#using-pip","title":"Using pip","text":"<pre><code># Install from GitHub\npip install git+https://github.com/Mutiny-Group/mmm-eval.git\n\n# Or clone and install locally\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npip install -e .\n</code></pre>"},{"location":"#run-a-basic-evaluation","title":"Run a basic evaluation","text":"<pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing\n</code></pre> <p>Note: mmm-eval is currently in development. For production use, we recommend installing from the latest release tag: <pre><code>poetry add git+https://github.com/Mutiny-Group/mmm-eval.git@v0.4.2\n</code></pre></p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>Multi-framework support: Evaluate PyMC-Marketing and other MMM frameworks</li> <li>Comprehensive test suite: Accuracy, cross-validation, refresh stability, and perturbation tests</li> <li>Standardized metrics: MAPE, RMSE, R-squared, and other industry-standard metrics</li> <li>Flexible data handling: Support for custom column names and data formats</li> <li>CLI interface: Easy-to-use command-line tool for evaluation</li> </ul>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Installation - How to install mmm-eval</li> <li>Quick Start - Get up and running quickly</li> <li>User Guide - Detailed usage instructions</li> <li>API Reference - Complete API documentation</li> <li>Examples - Practical examples and use cases</li> </ul>"},{"location":"#development","title":"\ud83d\udee0\ufe0f Development","text":"<ul> <li>Contributing - How to contribute to mmm-eval</li> <li>Development Setup - Setting up a development environment</li> <li>Testing - Running tests and quality checks</li> </ul>"},{"location":"#supported-frameworks","title":"\ud83d\udcca Supported Frameworks","text":"<p>Currently supported MMM frameworks:</p> <ul> <li>PyMC-Marketing: Bayesian MMM framework using PyMC</li> <li>More frameworks coming soon...</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details on how to get started.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the Apache 2.0 License - see the License file for details.</p> <ul> <li> <p> Quick Start</p> <p>Get up and running with mmm-eval in minutes.</p> </li> <li> <p> User Guide</p> <p>Learn how to use mmm-eval effectively.</p> </li> <li> <p> API Reference</p> <p>Explore the complete API documentation.</p> </li> <li> <p> Examples</p> <p>See practical examples and use cases.</p> </li> </ul>"},{"location":"about/license/","title":"License","text":"<p>This project is licensed under the Apache License, Version 2.0.</p>"},{"location":"about/license/#apache-license-20","title":"Apache License 2.0","text":"<p>Copyright 2024 mmm-eval Contributors</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"about/license/#key-features-of-apache-20","title":"Key Features of Apache 2.0","text":"<p>The Apache License 2.0 provides:</p> <ul> <li>Patent protection: Contributors grant patent licenses for their contributions</li> <li>Commercial use: Allows commercial use, modification, and distribution</li> <li>Patent termination: Patent licenses terminate if you file patent litigation</li> <li>Attribution: Requires preservation of copyright notices and license text</li> <li>Contributor protection: Protects contributors from liability</li> </ul>"},{"location":"about/license/#contributing","title":"Contributing","text":"<p>By contributing to this project, you agree that your contributions will be licensed under the Apache License, Version 2.0.</p>"},{"location":"about/license/#full-license-text","title":"Full License Text","text":"<p>For the complete license text, see the LICENSE file in the root of this repository. </p>"},{"location":"api/adapters/","title":"Adapters Reference","text":""},{"location":"api/adapters/#mmm_eval.adapters","title":"<code>mmm_eval.adapters</code>","text":"<p>Adapters for different MMM frameworks.</p>"},{"location":"api/adapters/#mmm_eval.adapters-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter","title":"<code>PyMCAdapter(config: PyMCConfig)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Initialize the PyMCAdapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PyMCConfig</code> <p>PyMCConfig object</p> required Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def __init__(self, config: PyMCConfig):\n    \"\"\"Initialize the PyMCAdapter.\n\n    Args:\n        config: PyMCConfig object\n\n    \"\"\"\n    self.model_kwargs = config.pymc_model_config_dict\n    self.fit_kwargs = config.fit_config_dict\n    self.date_column = config.date_column\n    self.channel_spend_columns = config.channel_columns\n    self.control_columns = config.control_columns\n    self.model = None\n    self.trace = None\n    self._channel_roi_df = None\n    self.is_fitted = False\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter-functions","title":"Functions","text":""},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.fit","title":"<code>fit(data: pd.DataFrame) -&gt; None</code>","text":"<p>Fit the model and compute ROIs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the training data adhering to the PyMCInputDataSchema.</p> required Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the model and compute ROIs.\n\n    Args:\n        data: DataFrame containing the training data adhering to the PyMCInputDataSchema.\n\n    \"\"\"\n    # Identify channel spend columns that sum to zero and remove them from modelling.\n    # We cannot reliabily make any prediction based on these channels when making\n    # predictions on new data.\n    channel_spend_data = data[self.channel_spend_columns]\n    zero_spend_channels = channel_spend_data.columns[channel_spend_data.sum() == 0].tolist()\n\n    if zero_spend_channels:\n        logger.info(f\"Dropping channels with zero spend: {zero_spend_channels}\")\n        # Remove zero-spend channels from the list passed to the MMM constructor\n        self.channel_spend_columns = [col for col in self.channel_spend_columns if col not in zero_spend_channels]\n        # also update the model config field to reflect the new channel spend columns\n        self.model_kwargs[\"channel_columns\"] = self.channel_spend_columns\n\n        # Check for vector priors that might cause shape mismatches\n        _check_vector_priors_when_dropping_channels(self.model_kwargs[\"model_config\"], zero_spend_channels)\n\n        data = data.drop(columns=zero_spend_channels)\n\n    X = data.drop(columns=[InputDataframeConstants.RESPONSE_COL, InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL])\n    y = data[InputDataframeConstants.RESPONSE_COL]\n\n    self.model = MMM(**self.model_kwargs)\n    self.trace = self.model.fit(X=X, y=y, **self.fit_kwargs)\n\n    self._channel_roi_df = self._compute_channel_contributions(data)\n    self.is_fitted = True\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.get_channel_roi","title":"<code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code>","text":"<p>Return the ROIs for each channel, optionally within a given date range.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Return the ROIs for each channel, optionally within a given date range.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted or self._channel_roi_df is None:\n        raise RuntimeError(\"Model must be fit before computing ROI.\")\n\n    _validate_start_end_dates(start_date, end_date, self._channel_roi_df.index)\n\n    # Filter the contribution DataFrame by date range\n    date_range_df = self._channel_roi_df.loc[start_date:end_date]\n\n    if date_range_df.empty:\n        raise ValueError(f\"No data found for date range {start_date} to {end_date}\")\n\n    return pd.Series(self._calculate_rois(date_range_df))\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.predict","title":"<code>predict(data: pd.DataFrame) -&gt; np.ndarray</code>","text":"<p>Predict the response variable for new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Predict the response variable for new data.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    if not self.is_fitted or self.model is None:\n        raise RuntimeError(\"Model must be fit before prediction.\")\n\n    if InputDataframeConstants.RESPONSE_COL in data.columns:\n        data = data.drop(columns=[InputDataframeConstants.RESPONSE_COL])\n    predictions = self.model.predict(data, extend_idata=False, include_last_observations=True)\n    return predictions\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters-functions","title":"Functions","text":""},{"location":"api/adapters/#mmm_eval.adapters.get_adapter","title":"<code>get_adapter(framework: str, config: PyMCConfig)</code>","text":"<p>Get an adapter instance for the specified framework.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>Name of the MMM framework</p> required <code>config</code> <code>PyMCConfig</code> <p>Framework-specific configuration</p> required <p>Returns:</p> Type Description <p>Adapter instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If framework is not supported</p> Source code in <code>mmm_eval/adapters/__init__.py</code> <pre><code>def get_adapter(framework: str, config: PyMCConfig):\n    \"\"\"Get an adapter instance for the specified framework.\n\n    Args:\n        framework: Name of the MMM framework\n        config: Framework-specific configuration\n\n    Returns:\n        Adapter instance\n\n    Raises:\n        ValueError: If framework is not supported\n\n    \"\"\"\n    if framework not in ADAPTER_REGISTRY:\n        raise ValueError(f\"Unsupported framework: {framework}. Available: {list(ADAPTER_REGISTRY.keys())}\")\n\n    adapter_class = ADAPTER_REGISTRY[framework]\n    return adapter_class(config)\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters-modules","title":"Modules","text":""},{"location":"api/adapters/#mmm_eval.adapters.base","title":"<code>base</code>","text":"<p>Base adapter class for MMM frameworks.</p>"},{"location":"api/adapters/#mmm_eval.adapters.base-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.base.BaseAdapter","title":"<code>BaseAdapter(config: dict[str, Any] | None = None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for MMM framework adapters.</p> <p>Initialize the base adapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Configuration dictionary</p> <code>None</code> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"Initialize the base adapter.\n\n    Args:\n        config: Configuration dictionary\n\n    \"\"\"\n    self.config = config or {}\n    self.is_fitted = False\n    self.channel_spend_columns: list[str] = []\n    self.date_column: str\n</code></pre> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> <code>abstractmethod</code> \u00b6 <p>Fit the model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Training data</p> required Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the model to the data.\n\n    Args:\n        data: Training data\n\n    \"\"\"\n    pass\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> <code>abstractmethod</code> \u00b6 <p>Get channel ROI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Get channel ROI estimates.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    pass\n</code></pre> <code>predict(data: pd.DataFrame) -&gt; np.ndarray</code> <code>abstractmethod</code> \u00b6 <p>Make predictions on new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Make predictions on new data.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.experimental","title":"<code>experimental</code>","text":""},{"location":"api/adapters/#mmm_eval.adapters.experimental-modules","title":"Modules","text":""},{"location":"api/adapters/#mmm_eval.adapters.experimental.pymc","title":"<code>pymc</code>","text":"<p>PyMC MMM framework adapter.</p> <p>N.B. we expect control variables to be scaled to 0-1 using maxabs scaling BEFORE being passed to the PyMCAdapter.</p> Classes\u00b6 <code>PyMCAdapter(config: PyMCConfig)</code> \u00b6 <p>               Bases: <code>BaseAdapter</code></p> <p>Initialize the PyMCAdapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PyMCConfig</code> <p>PyMCConfig object</p> required Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def __init__(self, config: PyMCConfig):\n    \"\"\"Initialize the PyMCAdapter.\n\n    Args:\n        config: PyMCConfig object\n\n    \"\"\"\n    self.model_kwargs = config.pymc_model_config_dict\n    self.fit_kwargs = config.fit_config_dict\n    self.date_column = config.date_column\n    self.channel_spend_columns = config.channel_columns\n    self.control_columns = config.control_columns\n    self.model = None\n    self.trace = None\n    self._channel_roi_df = None\n    self.is_fitted = False\n</code></pre> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> \u00b6 <p>Fit the model and compute ROIs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the training data adhering to the PyMCInputDataSchema.</p> required Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the model and compute ROIs.\n\n    Args:\n        data: DataFrame containing the training data adhering to the PyMCInputDataSchema.\n\n    \"\"\"\n    # Identify channel spend columns that sum to zero and remove them from modelling.\n    # We cannot reliabily make any prediction based on these channels when making\n    # predictions on new data.\n    channel_spend_data = data[self.channel_spend_columns]\n    zero_spend_channels = channel_spend_data.columns[channel_spend_data.sum() == 0].tolist()\n\n    if zero_spend_channels:\n        logger.info(f\"Dropping channels with zero spend: {zero_spend_channels}\")\n        # Remove zero-spend channels from the list passed to the MMM constructor\n        self.channel_spend_columns = [col for col in self.channel_spend_columns if col not in zero_spend_channels]\n        # also update the model config field to reflect the new channel spend columns\n        self.model_kwargs[\"channel_columns\"] = self.channel_spend_columns\n\n        # Check for vector priors that might cause shape mismatches\n        _check_vector_priors_when_dropping_channels(self.model_kwargs[\"model_config\"], zero_spend_channels)\n\n        data = data.drop(columns=zero_spend_channels)\n\n    X = data.drop(columns=[InputDataframeConstants.RESPONSE_COL, InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL])\n    y = data[InputDataframeConstants.RESPONSE_COL]\n\n    self.model = MMM(**self.model_kwargs)\n    self.trace = self.model.fit(X=X, y=y, **self.fit_kwargs)\n\n    self._channel_roi_df = self._compute_channel_contributions(data)\n    self.is_fitted = True\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> \u00b6 <p>Return the ROIs for each channel, optionally within a given date range.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Return the ROIs for each channel, optionally within a given date range.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted or self._channel_roi_df is None:\n        raise RuntimeError(\"Model must be fit before computing ROI.\")\n\n    _validate_start_end_dates(start_date, end_date, self._channel_roi_df.index)\n\n    # Filter the contribution DataFrame by date range\n    date_range_df = self._channel_roi_df.loc[start_date:end_date]\n\n    if date_range_df.empty:\n        raise ValueError(f\"No data found for date range {start_date} to {end_date}\")\n\n    return pd.Series(self._calculate_rois(date_range_df))\n</code></pre> <code>predict(data: pd.DataFrame) -&gt; np.ndarray</code> \u00b6 <p>Predict the response variable for new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Predict the response variable for new data.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    if not self.is_fitted or self.model is None:\n        raise RuntimeError(\"Model must be fit before prediction.\")\n\n    if InputDataframeConstants.RESPONSE_COL in data.columns:\n        data = data.drop(columns=[InputDataframeConstants.RESPONSE_COL])\n    predictions = self.model.predict(data, extend_idata=False, include_last_observations=True)\n    return predictions\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.experimental.schemas","title":"<code>schemas</code>","text":"Classes\u00b6 <code>PyMCFitSchema</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Schema for PyMC Fit Configuration.</p> <p>Defaults are all set to None so that the user can provide only the values they want to change. If a user does not provide a value, we will let the latest PYMC defaults be used in model instantiation.</p> Attributes\u00b6 <code>fit_config_dict_without_non_provided_fields: dict[str, Any]</code> <code>property</code> \u00b6 <p>Return only non-None values.</p> <p>These are the values that are provided by the user.    We don't want to include the default values as they should be set by the latest PYMC</p> <p>Returns     Dictionary of non-None values</p> <code>PyMCModelSchema</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Schema for PyMC Config Dictionary.</p> Functions\u00b6 <code>validate_adstock(v)</code> \u00b6 <p>Validate adstock component.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Adstock value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If adstock is not a valid type</p> Source code in <code>mmm_eval/adapters/experimental/schemas.py</code> <pre><code>@field_validator(\"adstock\")\ndef validate_adstock(cls, v):\n    \"\"\"Validate adstock component.\n\n    Args:\n        v: Adstock value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If adstock is not a valid type\n\n    \"\"\"\n    if v is not None:\n        assert isinstance(v, AdstockTransformation)\n    return v\n</code></pre> <code>validate_channel_columns(v)</code> \u00b6 <p>Validate channel columns are not empty.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Channel columns value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If channel columns is empty</p> Source code in <code>mmm_eval/adapters/experimental/schemas.py</code> <pre><code>@field_validator(\"channel_columns\")\ndef validate_channel_columns(cls, v):\n    \"\"\"Validate channel columns are not empty.\n\n    Args:\n        v: Channel columns value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If channel columns is empty\n\n    \"\"\"\n    if v is not None and not v:\n        raise ValueError(\"channel_columns must not be empty\")\n    return v\n</code></pre> <code>validate_saturation(v)</code> \u00b6 <p>Validate saturation component.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Saturation value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If saturation is not a valid type</p> Source code in <code>mmm_eval/adapters/experimental/schemas.py</code> <pre><code>@field_validator(\"saturation\")\ndef validate_saturation(cls, v):\n    \"\"\"Validate saturation component.\n\n    Args:\n        v: Saturation value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If saturation is not a valid type\n\n    \"\"\"\n    if v is not None:\n        assert isinstance(v, SaturationTransformation)\n    return v\n</code></pre> <code>PyMCStringConfigSchema</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Schema for PyMC Evaluation Config Dictionary.</p>"},{"location":"api/adapters/#mmm_eval.adapters.meridian","title":"<code>meridian</code>","text":"<p>Meridian adapter for MMM evaluation.</p>"},{"location":"api/adapters/#mmm_eval.adapters.meridian-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.meridian.MeridianAdapter","title":"<code>MeridianAdapter(config: dict[str, Any] | None = None)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for Google Meridian MMM framework.</p> <p>Initialize the Meridian adapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Configuration dictionary</p> <code>None</code> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"Initialize the Meridian adapter.\n\n    Args:\n        config: Configuration dictionary\n\n    \"\"\"\n    super().__init__(config)\n    self.media_columns = config.get(\"media_columns\", []) if config else []\n    self.base_columns = config.get(\"base_columns\", []) if config else []\n</code></pre> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> \u00b6 <p>Fit the Meridian model to data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Training data</p> required Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the Meridian model to data.\n\n    Args:\n        data: Training data\n\n    \"\"\"\n    # Placeholder implementation - simple linear combination\n    self.is_fitted = True\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> \u00b6 <p>Get channel ROI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Get channel ROI estimates.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted:\n        raise RuntimeError(\"Model must be fit before computing ROI\")\n    # Placeholder implementation\n    return pd.Series()\n</code></pre> <code>predict(data: pd.DataFrame) -&gt; np.ndarray</code> \u00b6 <p>Make predictions using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Make predictions using the fitted model.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    if not self.is_fitted:\n        raise RuntimeError(\"Model must be fit before prediction\")\n\n    # # Get media and base columns, or use all numeric columns\n    # media_cols = (\n    #     self.media_columns\n    #     or data.select_dtypes(include=[np.number]).columns.tolist()\n    # )\n\n    # if not media_cols:\n    #     # Fallback: create dummy predictions\n    #     return pd.Series(np.random.normal(100, 10, len(data)), index=data.index)\n\n    # # Simple placeholder prediction (weighted sum of media channels)\t    def get_channel_roi(\n    # weights = np.random.uniform(0.5, 2.0, len(media_cols))\n    # base_effect = 50  # Base level\n    # predictions = base_effect + np.dot(data[media_cols].fillna(0), weights)\n    # return pd.Series(predictions, index=data.index)\n\n    # Placeholder implementation\n    return np.zeros(len(data))\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.pymc","title":"<code>pymc</code>","text":"<p>Legacy PyMC adapter for MMM evaluation.</p>"},{"location":"api/adapters/#mmm_eval.adapters.pymc-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.pymc.PyMCAdapter","title":"<code>PyMCAdapter(config: dict[str, Any] | None = None)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Legacy adapter for PyMC MMM framework.</p> <p>Initialize the legacy PyMC adapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Configuration dictionary</p> <code>None</code> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"Initialize the legacy PyMC adapter.\n\n    Args:\n        config: Configuration dictionary\n\n    \"\"\"\n    pass\n</code></pre> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> \u00b6 <p>Fit the PyMC model to data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Training data</p> required Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the PyMC model to data.\n\n    Args:\n        data: Training data\n\n    \"\"\"\n    # Placeholder implementation\n    pass\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> \u00b6 <p>Get channel ROI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Get channel ROI estimates.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    return pd.Series()\n</code></pre> <code>predict(data: pd.DataFrame) -&gt; np.ndarray</code> \u00b6 <p>Make predictions using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Make predictions using the fitted model.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    # Placeholder implementation\n    return np.zeros(len(data))\n</code></pre>"},{"location":"api/cli/","title":"CLI reference","text":""},{"location":"api/cli/#mmm_eval.cli","title":"<code>mmm_eval.cli</code>","text":""},{"location":"api/cli/#mmm_eval.cli-modules","title":"Modules","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate","title":"<code>evaluate</code>","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate-classes","title":"Classes","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate-functions","title":"Functions","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate.main","title":"<code>main(config_path: str, input_data_path: str, test_names: tuple[str, ...], framework: str, output_path: str, verbose: bool)</code>","text":"<p>Evaluate MMM frameworks using the unified API.</p> Source code in <code>mmm_eval/cli/evaluate.py</code> <pre><code>@click.command()\n@click.option(\n    \"--framework\",\n    type=click.Choice(list(ADAPTER_REGISTRY.keys())),\n    required=True,\n    help=\"Open source MMM framework to evaluate\",\n)\n@click.option(\n    \"--input-data-path\",\n    type=str,\n    required=True,\n    help=\"Path to input data file. Supported formats: CSV, Parquet\",\n)\n@click.option(\n    \"--output-path\",\n    type=str,\n    required=True,\n    help=\"Directory to save evaluation results as a CSV file with name 'mmm_eval_&lt;framework&gt;_&lt;timestamp&gt;.csv'\",\n)\n@click.option(\n    \"--config-path\",\n    type=str,\n    required=True,\n    help=\"Path to framework-specific JSON config file\",\n)\n@click.option(\n    \"--test-names\",\n    type=click.Choice(ValidationTestNames.all_tests_as_str()),\n    multiple=True,\n    default=tuple(ValidationTestNames.all_tests_as_str()),\n    help=(\n        \"Test names to run. Can specify multiple tests as space-separated values \"\n        \"(e.g. --test-names accuracy cross_validation) or by repeating the flag \"\n        \"(e.g. --test-names accuracy --test-names cross_validation). \"\n        \"Defaults to all tests if not specified.\"\n    ),\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Enable verbose logging\",\n)\ndef main(\n    config_path: str,\n    input_data_path: str,\n    test_names: tuple[str, ...],\n    framework: str,\n    output_path: str,\n    verbose: bool,\n):\n    \"\"\"Evaluate MMM frameworks using the unified API.\"\"\"\n    # logging\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level)\n\n    logger.info(\"Loading config...\")\n    config = get_config(framework, config_path)\n\n    logger.info(\"Loading input data...\")\n    data = DataLoader(input_data_path).load()\n\n    # Run evaluation\n    logger.info(f\"Running evaluation suite for {framework} framework...\")\n    results = run_evaluation(framework, data, config, test_names)\n\n    # Save results\n    if results.empty:\n        logger.warning(\"Results df empty, nothing to save.\")\n    else:\n        save_results(results, framework, output_path)\n</code></pre>"},{"location":"api/cli/#mmm_eval.cli.evaluate-modules","title":"Modules","text":""},{"location":"api/core/","title":"Core API Reference","text":""},{"location":"api/core/#mmm_eval.core","title":"<code>mmm_eval.core</code>","text":"<p>Core validation functionality for MMM frameworks.</p>"},{"location":"api/core/#mmm_eval.core-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.BaseValidationTest","title":"<code>BaseValidationTest()</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for validation tests.</p> <p>All validation tests must inherit from this class and implement the required methods to provide a unified testing interface.</p> <p>Initialize the validation test.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.BaseValidationTest-attributes","title":"Attributes","text":""},{"location":"api/core/#mmm_eval.core.BaseValidationTest.test_name","title":"<code>test_name: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the name of the test.</p> <p>Returns     Test name (e.g., 'accuracy', 'stability')</p>"},{"location":"api/core/#mmm_eval.core.BaseValidationTest-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.BaseValidationTest.run","title":"<code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code>  <code>abstractmethod</code>","text":"<p>Run the validation test.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>@abstractmethod\ndef run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#mmm_eval.core.BaseValidationTest.run_with_error_handling","title":"<code>run_with_error_handling(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code>","text":"<p>Run the validation test with error handling.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> <p>Raises:</p> Type Description <code>MetricCalculationError</code> <p>If metric calculation fails</p> <code>TestExecutionError</code> <p>If test execution fails</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def run_with_error_handling(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test with error handling.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    Raises:\n        MetricCalculationError: If metric calculation fails\n        TestExecutionError: If test execution fails\n\n    \"\"\"\n    try:\n        return self.run(adapter, data)\n    except ZeroDivisionError as e:\n        # This is clearly a mathematical calculation issue\n        raise MetricCalculationError(f\"Metric calculation error in {self.test_name} test: {str(e)}\") from e\n    except Exception as e:\n        # All other errors - let individual tests handle specific categorization if needed\n        raise TestExecutionError(f\"Test execution error in {self.test_name} test: {str(e)}\") from e\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults","title":"<code>ValidationResults(test_results: dict[ValidationTestNames, ValidationTestResult])</code>","text":"<p>Container for complete validation results.</p> <p>This class holds the results of all validation tests run, including individual test results and overall summary.</p> <p>Initialize validation results.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <code>dict[ValidationTestNames, ValidationTestResult]</code> <p>Dictionary mapping test names to their results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(self, test_results: dict[ValidationTestNames, ValidationTestResult]):\n    \"\"\"Initialize validation results.\n\n    Args:\n        test_results: Dictionary mapping test names to their results\n\n    \"\"\"\n    self.test_results = test_results\n    self.timestamp = datetime.now()\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.ValidationResults.all_passed","title":"<code>all_passed() -&gt; bool</code>","text":"<p>Check if all tests passed.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def all_passed(self) -&gt; bool:\n    \"\"\"Check if all tests passed.\"\"\"\n    return all(result.passed for result in self.test_results.values())\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults.get_test_result","title":"<code>get_test_result(test_name: ValidationTestNames) -&gt; ValidationTestResult</code>","text":"<p>Get results for a specific test.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def get_test_result(self, test_name: ValidationTestNames) -&gt; ValidationTestResult:\n    \"\"\"Get results for a specific test.\"\"\"\n    return self.test_results[test_name]\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults.to_df","title":"<code>to_df() -&gt; pd.DataFrame</code>","text":"<p>Convert nested test results to a flat DataFrame format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert nested test results to a flat DataFrame format.\"\"\"\n    rows = []\n\n    for result in self.test_results.values():\n        test_name = result.test_name.value\n        passed = result.passed\n        test_scores_dict = result.test_scores.to_dict()\n\n        for metric_key, value in test_scores_dict.items():\n            if isinstance(value, pd.Series):\n                for subkey, subval in value.items():\n                    rows.append(\n                        {\n                            \"test_name\": test_name,\n                            \"metric_name\": f\"{metric_key}:{subkey}\",\n                            \"metric_value\": subval,\n                            \"metric_pass\": passed,\n                        }\n                    )\n            else:\n                rows.append(\n                    {\n                        \"test_name\": test_name,\n                        \"metric_name\": metric_key,\n                        \"metric_value\": value,\n                        \"metric_pass\": passed,\n                    }\n                )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert results to dictionary format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert results to dictionary format.\"\"\"\n    return {\n        ValidationResultAttributeNames.TIMESTAMP.value: self.timestamp.isoformat(),\n        ValidationResultAttributeNames.ALL_PASSED.value: self.all_passed(),\n        ValidationResultAttributeNames.RESULTS.value: {\n            result.test_name.value: result.to_dict() for result in self.test_results.values()\n        },\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestOrchestrator","title":"<code>ValidationTestOrchestrator()</code>","text":"<p>Main orchestrator for running validation tests.</p> <p>This class manages the test registry and executes tests in sequence, aggregating their results.</p> <p>Initialize the validator with standard tests pre-registered.</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validator with standard tests pre-registered.\"\"\"\n    self.tests: dict[ValidationTestNames, type[BaseValidationTest]] = {\n        ValidationTestNames.ACCURACY: AccuracyTest,\n        ValidationTestNames.CROSS_VALIDATION: CrossValidationTest,\n        ValidationTestNames.REFRESH_STABILITY: RefreshStabilityTest,\n        ValidationTestNames.PERTURBATION: PerturbationTest,\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestOrchestrator-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.ValidationTestOrchestrator.validate","title":"<code>validate(adapter: BaseAdapter, data: pd.DataFrame, test_names: list[ValidationTestNames]) -&gt; ValidationResults</code>","text":"<p>Run validation tests on the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <code>test_names</code> <code>list[ValidationTestNames]</code> <p>List of test names to run</p> required <code>adapter</code> <code>BaseAdapter</code> <p>Adapter to use for the test</p> required <p>Returns:</p> Type Description <code>ValidationResults</code> <p>ValidationResults containing all test results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any requested test is not registered</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def validate(\n    self,\n    adapter: BaseAdapter,\n    data: pd.DataFrame,\n    test_names: list[ValidationTestNames],\n) -&gt; ValidationResults:\n    \"\"\"Run validation tests on the model.\n\n    Args:\n        model: Model to validate\n        data: Input data for validation\n        test_names: List of test names to run\n        adapter: Adapter to use for the test\n\n    Returns:\n        ValidationResults containing all test results\n\n    Raises:\n        ValueError: If any requested test is not registered\n\n    \"\"\"\n    # Run tests and collect results\n    results: dict[ValidationTestNames, ValidationTestResult] = {}\n    for test_name in test_names:\n        logger.info(f\"Running test: {test_name}\")\n        test_instance = self.tests[test_name]()\n        test_result = test_instance.run_with_error_handling(adapter, data)\n        results[test_name] = test_result\n\n    return ValidationResults(results)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestResult","title":"<code>ValidationTestResult(test_name: ValidationTestNames, passed: bool, metric_names: list[str], test_scores: AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults)</code>","text":"<p>Container for individual test results.</p> <p>This class holds the results of a single validation test, including pass/fail status, metrics, and any error messages.</p> <p>Initialize test results.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>ValidationTestNames</code> <p>Name of the test</p> required <code>passed</code> <code>bool</code> <p>Whether the test passed</p> required <code>metric_names</code> <code>list[str]</code> <p>List of metric names</p> required <code>test_scores</code> <code>AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults</code> <p>Computed metric results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(\n    self,\n    test_name: ValidationTestNames,\n    passed: bool,\n    metric_names: list[str],\n    test_scores: (\n        AccuracyMetricResults\n        | CrossValidationMetricResults\n        | RefreshStabilityMetricResults\n        | PerturbationMetricResults\n    ),\n):\n    \"\"\"Initialize test results.\n\n    Args:\n        test_name: Name of the test\n        passed: Whether the test passed\n        metric_names: List of metric names\n        test_scores: Computed metric results\n\n    \"\"\"\n    self.test_name = test_name\n    self.passed = passed\n    self.metric_names = metric_names\n    self.test_scores = test_scores\n    self.timestamp = datetime.now()\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestResult-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.ValidationTestResult.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert results to dictionary format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert results to dictionary format.\"\"\"\n    return {\n        ValidationTestAttributeNames.TEST_NAME.value: self.test_name.value,\n        # todo(): Perhaps set as false permanently or dont use if we dont want thresholds\n        ValidationTestAttributeNames.PASSED.value: self.passed,\n        ValidationTestAttributeNames.METRIC_NAMES.value: self.metric_names,\n        ValidationTestAttributeNames.TEST_SCORES.value: self.test_scores.to_dict(),\n        ValidationTestAttributeNames.TIMESTAMP.value: self.timestamp.isoformat(),\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core-modules","title":"Modules","text":""},{"location":"api/core/#mmm_eval.core.base_validation_test","title":"<code>base_validation_test</code>","text":"<p>Abstract base classes for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.base_validation_test-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.base_validation_test.BaseValidationTest","title":"<code>BaseValidationTest()</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for validation tests.</p> <p>All validation tests must inherit from this class and implement the required methods to provide a unified testing interface.</p> <p>Initialize the validation test.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: str</code> <code>abstractmethod</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> <p>Returns     Test name (e.g., 'accuracy', 'stability')</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> <code>abstractmethod</code> \u00b6 <p>Run the validation test.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>@abstractmethod\ndef run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    \"\"\"\n    pass\n</code></pre> <code>run_with_error_handling(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the validation test with error handling.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> <p>Raises:</p> Type Description <code>MetricCalculationError</code> <p>If metric calculation fails</p> <code>TestExecutionError</code> <p>If test execution fails</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def run_with_error_handling(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test with error handling.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    Raises:\n        MetricCalculationError: If metric calculation fails\n        TestExecutionError: If test execution fails\n\n    \"\"\"\n    try:\n        return self.run(adapter, data)\n    except ZeroDivisionError as e:\n        # This is clearly a mathematical calculation issue\n        raise MetricCalculationError(f\"Metric calculation error in {self.test_name} test: {str(e)}\") from e\n    except Exception as e:\n        # All other errors - let individual tests handle specific categorization if needed\n        raise TestExecutionError(f\"Test execution error in {self.test_name} test: {str(e)}\") from e\n</code></pre>"},{"location":"api/core/#mmm_eval.core.constants","title":"<code>constants</code>","text":""},{"location":"api/core/#mmm_eval.core.constants-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.constants.ValidationTestConstants","title":"<code>ValidationTestConstants</code>","text":"<p>Constants for the validation tests.</p> Classes\u00b6 <code>PerturbationConstants</code> \u00b6 <p>Constants for the perturbation test.</p>"},{"location":"api/core/#mmm_eval.core.evaluator","title":"<code>evaluator</code>","text":"<p>Main evaluator for MMM frameworks.</p>"},{"location":"api/core/#mmm_eval.core.evaluator-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.evaluator.Evaluator","title":"<code>Evaluator(data: pd.DataFrame, test_names: tuple[str, ...] | None = None)</code>","text":"<p>Main evaluator class for MMM frameworks.</p> <p>This class provides a unified interface for evaluating different MMM frameworks using standardized validation tests.</p> <p>Initialize the evaluator.</p> Source code in <code>mmm_eval/core/evaluator.py</code> <pre><code>def __init__(self, data: pd.DataFrame, test_names: tuple[str, ...] | None = None):\n    \"\"\"Initialize the evaluator.\"\"\"\n    self.validation_orchestrator = ValidationTestOrchestrator()\n    self.data = data\n    self.test_names = (\n        self._get_test_names(test_names) if test_names else self.validation_orchestrator._get_all_test_names()\n    )\n</code></pre> Functions\u00b6 <code>evaluate_framework(framework: str, config: BaseConfig) -&gt; ValidationResults</code> \u00b6 <p>Evaluate an MMM framework using the unified API.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>Name of the MMM framework to evaluate</p> required <code>config</code> <code>BaseConfig</code> <p>Framework-specific configuration</p> required <p>Returns:</p> Type Description <code>ValidationResults</code> <p>ValidationResult object containing evaluation metrics and predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any test name is invalid</p> Source code in <code>mmm_eval/core/evaluator.py</code> <pre><code>def evaluate_framework(self, framework: str, config: BaseConfig) -&gt; ValidationResults:\n    \"\"\"Evaluate an MMM framework using the unified API.\n\n    Args:\n        framework: Name of the MMM framework to evaluate\n        config: Framework-specific configuration\n\n    Returns:\n        ValidationResult object containing evaluation metrics and predictions\n\n    Raises:\n        ValueError: If any test name is invalid\n\n    \"\"\"\n    # Initialize the adapter\n    adapter = get_adapter(framework, config)\n\n    # Run validation tests\n    validation_results = self.validation_orchestrator.validate(\n        adapter=adapter,\n        data=self.data,\n        test_names=self.test_names,\n    )\n\n    return validation_results\n</code></pre>"},{"location":"api/core/#mmm_eval.core.evaluator-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.exceptions-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.exceptions.InvalidTestNameError","title":"<code>InvalidTestNameError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when an invalid test name is provided.</p>"},{"location":"api/core/#mmm_eval.core.exceptions.MetricCalculationError","title":"<code>MetricCalculationError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when metric calculation fails.</p>"},{"location":"api/core/#mmm_eval.core.exceptions.TestExecutionError","title":"<code>TestExecutionError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when test execution fails.</p>"},{"location":"api/core/#mmm_eval.core.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for validation framework errors.</p>"},{"location":"api/core/#mmm_eval.core.run_evaluation","title":"<code>run_evaluation</code>","text":""},{"location":"api/core/#mmm_eval.core.run_evaluation-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.run_evaluation-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.run_evaluation.run_evaluation","title":"<code>run_evaluation(framework: str, data: pd.DataFrame, config: BaseConfig, test_names: tuple[str, ...] | None = None) -&gt; pd.DataFrame</code>","text":"<p>Evaluate an MMM framework.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>The framework to evaluate.</p> required <code>data</code> <code>DataFrame</code> <p>The data to evaluate.</p> required <code>config</code> <code>BaseConfig</code> <p>The config to use for the evaluation.</p> required <code>test_names</code> <code>tuple[str, ...] | None</code> <p>The tests to run. If not provided, all tests will be run.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame containing the evaluation results.</p> Source code in <code>mmm_eval/core/run_evaluation.py</code> <pre><code>def run_evaluation(\n    framework: str,\n    data: pd.DataFrame,\n    config: BaseConfig,\n    test_names: tuple[str, ...] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Evaluate an MMM framework.\n\n    Args:\n        framework: The framework to evaluate.\n        data: The data to evaluate.\n        config: The config to use for the evaluation.\n        test_names: The tests to run. If not provided, all tests will be run.\n\n    Returns:\n        A pandas DataFrame containing the evaluation results.\n\n    \"\"\"\n    # validate + process the input data\n    data = DataPipeline(\n        data=data,\n        date_column=config.date_column,\n        response_column=config.response_column,\n        revenue_column=config.revenue_column,\n        control_columns=config.control_columns,\n        channel_columns=config.channel_columns,\n    ).run()\n\n    # run the evaluation suite\n    results = Evaluator(\n        data=data,\n        test_names=test_names,\n    ).evaluate_framework(framework=framework, config=config)\n\n    return results.to_df()\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_test_orchestrator","title":"<code>validation_test_orchestrator</code>","text":"<p>Test orchestrator for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.validation_test_orchestrator-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_test_orchestrator.ValidationTestOrchestrator","title":"<code>ValidationTestOrchestrator()</code>","text":"<p>Main orchestrator for running validation tests.</p> <p>This class manages the test registry and executes tests in sequence, aggregating their results.</p> <p>Initialize the validator with standard tests pre-registered.</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validator with standard tests pre-registered.\"\"\"\n    self.tests: dict[ValidationTestNames, type[BaseValidationTest]] = {\n        ValidationTestNames.ACCURACY: AccuracyTest,\n        ValidationTestNames.CROSS_VALIDATION: CrossValidationTest,\n        ValidationTestNames.REFRESH_STABILITY: RefreshStabilityTest,\n        ValidationTestNames.PERTURBATION: PerturbationTest,\n    }\n</code></pre> Functions\u00b6 <code>validate(adapter: BaseAdapter, data: pd.DataFrame, test_names: list[ValidationTestNames]) -&gt; ValidationResults</code> \u00b6 <p>Run validation tests on the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <code>test_names</code> <code>list[ValidationTestNames]</code> <p>List of test names to run</p> required <code>adapter</code> <code>BaseAdapter</code> <p>Adapter to use for the test</p> required <p>Returns:</p> Type Description <code>ValidationResults</code> <p>ValidationResults containing all test results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any requested test is not registered</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def validate(\n    self,\n    adapter: BaseAdapter,\n    data: pd.DataFrame,\n    test_names: list[ValidationTestNames],\n) -&gt; ValidationResults:\n    \"\"\"Run validation tests on the model.\n\n    Args:\n        model: Model to validate\n        data: Input data for validation\n        test_names: List of test names to run\n        adapter: Adapter to use for the test\n\n    Returns:\n        ValidationResults containing all test results\n\n    Raises:\n        ValueError: If any requested test is not registered\n\n    \"\"\"\n    # Run tests and collect results\n    results: dict[ValidationTestNames, ValidationTestResult] = {}\n    for test_name in test_names:\n        logger.info(f\"Running test: {test_name}\")\n        test_instance = self.tests[test_name]()\n        test_result = test_instance.run_with_error_handling(adapter, data)\n        results[test_name] = test_result\n\n    return ValidationResults(results)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_test_results","title":"<code>validation_test_results</code>","text":"<p>Result containers for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.validation_test_results-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_test_results.ValidationResults","title":"<code>ValidationResults(test_results: dict[ValidationTestNames, ValidationTestResult])</code>","text":"<p>Container for complete validation results.</p> <p>This class holds the results of all validation tests run, including individual test results and overall summary.</p> <p>Initialize validation results.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <code>dict[ValidationTestNames, ValidationTestResult]</code> <p>Dictionary mapping test names to their results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(self, test_results: dict[ValidationTestNames, ValidationTestResult]):\n    \"\"\"Initialize validation results.\n\n    Args:\n        test_results: Dictionary mapping test names to their results\n\n    \"\"\"\n    self.test_results = test_results\n    self.timestamp = datetime.now()\n</code></pre> Functions\u00b6 <code>all_passed() -&gt; bool</code> \u00b6 <p>Check if all tests passed.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def all_passed(self) -&gt; bool:\n    \"\"\"Check if all tests passed.\"\"\"\n    return all(result.passed for result in self.test_results.values())\n</code></pre> <code>get_test_result(test_name: ValidationTestNames) -&gt; ValidationTestResult</code> \u00b6 <p>Get results for a specific test.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def get_test_result(self, test_name: ValidationTestNames) -&gt; ValidationTestResult:\n    \"\"\"Get results for a specific test.\"\"\"\n    return self.test_results[test_name]\n</code></pre> <code>to_df() -&gt; pd.DataFrame</code> \u00b6 <p>Convert nested test results to a flat DataFrame format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert nested test results to a flat DataFrame format.\"\"\"\n    rows = []\n\n    for result in self.test_results.values():\n        test_name = result.test_name.value\n        passed = result.passed\n        test_scores_dict = result.test_scores.to_dict()\n\n        for metric_key, value in test_scores_dict.items():\n            if isinstance(value, pd.Series):\n                for subkey, subval in value.items():\n                    rows.append(\n                        {\n                            \"test_name\": test_name,\n                            \"metric_name\": f\"{metric_key}:{subkey}\",\n                            \"metric_value\": subval,\n                            \"metric_pass\": passed,\n                        }\n                    )\n            else:\n                rows.append(\n                    {\n                        \"test_name\": test_name,\n                        \"metric_name\": metric_key,\n                        \"metric_value\": value,\n                        \"metric_pass\": passed,\n                    }\n                )\n\n    return pd.DataFrame(rows)\n</code></pre> <code>to_dict() -&gt; dict[str, Any]</code> \u00b6 <p>Convert results to dictionary format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert results to dictionary format.\"\"\"\n    return {\n        ValidationResultAttributeNames.TIMESTAMP.value: self.timestamp.isoformat(),\n        ValidationResultAttributeNames.ALL_PASSED.value: self.all_passed(),\n        ValidationResultAttributeNames.RESULTS.value: {\n            result.test_name.value: result.to_dict() for result in self.test_results.values()\n        },\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_test_results.ValidationTestResult","title":"<code>ValidationTestResult(test_name: ValidationTestNames, passed: bool, metric_names: list[str], test_scores: AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults)</code>","text":"<p>Container for individual test results.</p> <p>This class holds the results of a single validation test, including pass/fail status, metrics, and any error messages.</p> <p>Initialize test results.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>ValidationTestNames</code> <p>Name of the test</p> required <code>passed</code> <code>bool</code> <p>Whether the test passed</p> required <code>metric_names</code> <code>list[str]</code> <p>List of metric names</p> required <code>test_scores</code> <code>AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults</code> <p>Computed metric results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(\n    self,\n    test_name: ValidationTestNames,\n    passed: bool,\n    metric_names: list[str],\n    test_scores: (\n        AccuracyMetricResults\n        | CrossValidationMetricResults\n        | RefreshStabilityMetricResults\n        | PerturbationMetricResults\n    ),\n):\n    \"\"\"Initialize test results.\n\n    Args:\n        test_name: Name of the test\n        passed: Whether the test passed\n        metric_names: List of metric names\n        test_scores: Computed metric results\n\n    \"\"\"\n    self.test_name = test_name\n    self.passed = passed\n    self.metric_names = metric_names\n    self.test_scores = test_scores\n    self.timestamp = datetime.now()\n</code></pre> Functions\u00b6 <code>to_dict() -&gt; dict[str, Any]</code> \u00b6 <p>Convert results to dictionary format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert results to dictionary format.\"\"\"\n    return {\n        ValidationTestAttributeNames.TEST_NAME.value: self.test_name.value,\n        # todo(): Perhaps set as false permanently or dont use if we dont want thresholds\n        ValidationTestAttributeNames.PASSED.value: self.passed,\n        ValidationTestAttributeNames.METRIC_NAMES.value: self.metric_names,\n        ValidationTestAttributeNames.TEST_SCORES.value: self.test_scores.to_dict(),\n        ValidationTestAttributeNames.TIMESTAMP.value: self.timestamp.isoformat(),\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests","title":"<code>validation_tests</code>","text":""},{"location":"api/core/#mmm_eval.core.validation_tests-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_tests.AccuracyTest","title":"<code>AccuracyTest()</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for model accuracy using holdout validation.</p> <p>This test evaluates model performance by splitting data into train/test sets and calculating MAPE and R-squared metrics on the test set.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the accuracy test.</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the accuracy test.\"\"\"\n    # Split data into train/test sets\n    train, test = self._split_data_holdout(data)\n    adapter.fit(train)  # fit() modifies model in-place, returns None\n    predictions = adapter.predict(test)  # predict() on same model instance\n\n    # Calculate metrics\n    test_scores = AccuracyMetricResults.populate_object_with_metrics(\n        actual=test[InputDataframeConstants.RESPONSE_COL],\n        predicted=predictions,\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.ACCURACY,\n        passed=test_scores.check_test_passed(),\n        metric_names=AccuracyMetricNames.metrics_to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests.CrossValidationTest","title":"<code>CrossValidationTest()</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for the cross-validation of the MMM framework.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the cross-validation test using time-series splits.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to validate</p> required <code>adapter</code> <code>BaseAdapter</code> <p>Adapter to use for the test</p> required <code>data</code> <code>DataFrame</code> <p>Input data</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult containing cross-validation metrics</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the cross-validation test using time-series splits.\n\n    Args:\n        model: Model to validate\n        adapter: Adapter to use for the test\n        data: Input data\n\n    Returns:\n        TestResult containing cross-validation metrics\n\n    \"\"\"\n    # Initialize cross-validation splitter\n    cv_splits = self._split_data_time_series_cv(data)\n\n    # Store metrics for each fold\n    fold_metrics = []\n\n    # Run cross-validation\n    for i, (train_idx, test_idx) in enumerate(cv_splits):\n\n        logger.info(f\"Running cross-validation fold {i+1} of {len(cv_splits)}\")\n\n        # Get train/test data\n        train = data.iloc[train_idx]\n        test = data.iloc[test_idx]\n\n        # Get predictions\n        adapter.fit(train)\n        predictions = adapter.predict(test)\n\n        # Add in fold results\n        fold_metrics.append(\n            AccuracyMetricResults.populate_object_with_metrics(\n                actual=test[InputDataframeConstants.RESPONSE_COL],\n                predicted=predictions,\n            )\n        )\n\n    # Calculate mean and std of metrics across folds and create metric results\n    test_scores = CrossValidationMetricResults(\n        mean_mape=calculate_mean_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.MAPE\n        ),\n        std_mape=calculate_std_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.MAPE\n        ),\n        mean_r_squared=calculate_mean_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.R_SQUARED\n        ),\n        std_r_squared=calculate_std_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.R_SQUARED\n        ),\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.CROSS_VALIDATION,\n        passed=test_scores.check_test_passed(),\n        metric_names=CrossValidationMetricNames.metrics_to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests.PerturbationTest","title":"<code>PerturbationTest()</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for the perturbation of the MMM framework.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the perturbation test.</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the perturbation test.\"\"\"\n    # Train model on original data\n    adapter.fit(data)\n    original_rois = adapter.get_channel_roi()\n\n    # Add noise to spend data and retrain\n    noisy_data = self._add_gaussian_noise_to_spend(\n        df=data,\n        spend_cols=adapter.channel_spend_columns,\n    )\n    adapter.fit(noisy_data)\n    noise_rois = adapter.get_channel_roi()\n\n    # calculate the pct change in roi\n    percentage_change = calculate_absolute_percentage_change(\n        baseline_series=original_rois,\n        comparison_series=noise_rois,\n    )\n\n    # Create metric results - roi % change for each channel\n    test_scores = PerturbationMetricResults(\n        percentage_change_for_each_channel=percentage_change,\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.PERTURBATION,\n        passed=test_scores.check_test_passed(),\n        metric_names=PerturbationMetricNames.metrics_to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests.RefreshStabilityTest","title":"<code>RefreshStabilityTest()</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for the stability of the MMM framework.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the stability test.</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the stability test.\"\"\"\n    # Initialize cross-validation splitter\n    cv_splits = self._split_data_time_series_cv(data)\n\n    # Store metrics for each fold\n    fold_metrics = []\n\n    # Run cross-validation\n    for i, (train_idx, refresh_idx) in enumerate(cv_splits):\n\n        logger.info(f\"Running refresh stability test fold {i+1} of {len(cv_splits)}\")\n\n        # Get train/test data\n        # todo(): Can we somehow store these training changes in the adapter for use in time series holdout test\n        current_data = data.iloc[train_idx]\n        # Combine current data with refresh data for retraining\n        refresh_data = pd.concat([current_data, data.iloc[refresh_idx]], ignore_index=True)\n        # Get common dates for roi stability comparison\n        common_start_date, common_end_date = self._get_common_dates(\n            baseline_data=current_data,\n            comparison_data=refresh_data,\n            date_column=adapter.date_column,\n        )\n\n        # Train model and get coefficients\n        adapter.fit(current_data)\n        current_model_rois = adapter.get_channel_roi(\n            start_date=common_start_date,\n            end_date=common_end_date,\n        )\n        adapter.fit(refresh_data)\n        refreshed_model_rois = adapter.get_channel_roi(\n            start_date=common_start_date,\n            end_date=common_end_date,\n        )\n\n        # calculate the pct change in volume\n        percentage_change = calculate_absolute_percentage_change(\n            baseline_series=current_model_rois,\n            comparison_series=refreshed_model_rois,\n        )\n\n        fold_metrics.append(percentage_change)\n\n    # Calculate mean and std of percentage change for each channel across cross validation folds\n    test_scores = RefreshStabilityMetricResults(\n        mean_percentage_change_for_each_channel=calculate_means_for_series_across_cross_validation_folds(\n            fold_metrics\n        ),\n        std_percentage_change_for_each_channel=calculate_stds_for_series_across_cross_validation_folds(\n            fold_metrics\n        ),\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.REFRESH_STABILITY,\n        passed=test_scores.check_test_passed(),\n        metric_names=RefreshStabilityMetricNames.metrics_to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.validation_tests_models","title":"<code>validation_tests_models</code>","text":""},{"location":"api/core/#mmm_eval.core.validation_tests_models-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_tests_models.ValidationResultAttributeNames","title":"<code>ValidationResultAttributeNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the names of the validation result attributes.</p>"},{"location":"api/core/#mmm_eval.core.validation_tests_models.ValidationTestAttributeNames","title":"<code>ValidationTestAttributeNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the names of the validation test attributes.</p>"},{"location":"api/core/#mmm_eval.core.validation_tests_models.ValidationTestNames","title":"<code>ValidationTestNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the names of the validation tests.</p> Functions\u00b6 <code>all_tests() -&gt; list[ValidationTestNames]</code> <code>classmethod</code> \u00b6 <p>Return all validation test names as a list.</p> Source code in <code>mmm_eval/core/validation_tests_models.py</code> <pre><code>@classmethod\ndef all_tests(cls) -&gt; list[\"ValidationTestNames\"]:\n    \"\"\"Return all validation test names as a list.\"\"\"\n    return list(cls)\n</code></pre> <code>all_tests_as_str() -&gt; list[str]</code> <code>classmethod</code> \u00b6 <p>Return all validation test names as a list of strings.</p> Source code in <code>mmm_eval/core/validation_tests_models.py</code> <pre><code>@classmethod\ndef all_tests_as_str(cls) -&gt; list[str]:\n    \"\"\"Return all validation test names as a list of strings.\"\"\"\n    return [test.value for test in cls]\n</code></pre>"},{"location":"api/data/","title":"Data Reference","text":""},{"location":"api/data/#mmm_eval.data","title":"<code>mmm_eval.data</code>","text":"<p>Data loading and processing utilities.</p>"},{"location":"api/data/#mmm_eval.data-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.DataLoader","title":"<code>DataLoader(data_path: str | Path)</code>","text":"<p>Simple data loader for MMM evaluation.</p> <p>Takes a data path and loads the data.</p> <p>Initialize data loader with data path.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str | Path</code> <p>Path to the data file (CSV, Parquet, etc.)</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the data file does not exist.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def __init__(self, data_path: str | Path):\n    \"\"\"Initialize data loader with data path.\n\n    Args:\n        data_path: Path to the data file (CSV, Parquet, etc.)\n\n    Raises:\n        FileNotFoundError: If the data file does not exist.\n\n    \"\"\"\n    self.data_path = Path(data_path)\n\n    if not self.data_path.exists():\n        raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataLoader-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataLoader.load","title":"<code>load() -&gt; pd.DataFrame</code>","text":"<p>Load data from the specified path.</p> <p>Returns     Loaded DataFrame</p> <p>Raises     ValueError: If the file format is not supported.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def load(self) -&gt; pd.DataFrame:\n    \"\"\"Load data from the specified path.\n\n    Returns\n        Loaded DataFrame\n\n    Raises\n        ValueError: If the file format is not supported.\n\n    \"\"\"\n    ext = self.data_path.suffix.lower().lstrip(\".\")\n    if ext not in DataLoaderConstants.ValidDataExtensions.all():\n        raise ValueError(f\"Unsupported file format: {self.data_path.suffix}\")\n\n    if ext == DataLoaderConstants.ValidDataExtensions.CSV:\n        return self._load_csv()\n    elif ext == DataLoaderConstants.ValidDataExtensions.PARQUET:\n        return self._load_parquet()\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataPipeline","title":"<code>DataPipeline(data: pd.DataFrame, control_columns: list[str] | None, channel_columns: list[str], date_column: str, response_column: str, revenue_column: str, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Data pipeline that orchestrates loading, processing, and validation.</p> <p>Provides a simple interface to go from raw data file to validated DataFrame.</p> <p>Initialize data pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the data</p> required <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column</p> required <code>response_column</code> <code>str</code> <p>Name of the response column</p> required <code>revenue_column</code> <code>str</code> <p>Name of the revenue column</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str,\n    response_column: str,\n    revenue_column: str,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize data pipeline.\n\n    Args:\n        data: DataFrame containing the data\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column\n        response_column: Name of the response column\n        revenue_column: Name of the revenue column\n        min_number_observations: Minimum required number of observations\n\n    \"\"\"\n    # Initialize components\n    self.data = data\n    self.processor = DataProcessor(\n        date_column=date_column,\n        response_column=response_column,\n        revenue_column=revenue_column,\n        control_columns=control_columns,\n        channel_columns=channel_columns,\n    )\n    self.validator = DataValidator(\n        control_columns=control_columns,\n        min_number_observations=min_number_observations,\n    )\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataPipeline-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataPipeline.run","title":"<code>run() -&gt; pd.DataFrame</code>","text":"<p>Run the complete data pipeline: process \u2192 validate.</p> <p>Returns     Validated and processed DataFrame</p> <p>Raises     Various exceptions processing or validation steps</p> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"Run the complete data pipeline: process \u2192 validate.\n\n    Returns\n        Validated and processed DataFrame\n\n    Raises\n        Various exceptions processing or validation steps\n\n    \"\"\"\n    processed_df = self.processor.process(self.data)\n\n    self.validator.run_validations(processed_df)\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataProcessor","title":"<code>DataProcessor(control_columns: list[str] | None, channel_columns: list[str], date_column: str = InputDataframeConstants.DATE_COL, response_column: str = InputDataframeConstants.RESPONSE_COL, revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL)</code>","text":"<p>Simple data processor for MMM evaluation.</p> <p>Handles data transformations like datetime casting, column renaming, etc.</p> <p>Initialize data processor.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column to parse and rename</p> <code>DATE_COL</code> <code>response_column</code> <code>str</code> <p>Name of the response column to parse and rename</p> <code>RESPONSE_COL</code> <code>revenue_column</code> <code>str</code> <p>Name of the revenue column to parse and rename</p> <code>MEDIA_CHANNEL_REVENUE_COL</code> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str = InputDataframeConstants.DATE_COL,\n    response_column: str = InputDataframeConstants.RESPONSE_COL,\n    revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,\n):\n    \"\"\"Initialize data processor.\n\n    Args:\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column to parse and rename\n        response_column: Name of the response column to parse and rename\n        revenue_column: Name of the revenue column to parse and rename\n\n    \"\"\"\n    self.date_column = date_column\n    self.response_column = response_column\n    self.revenue_column = revenue_column\n    self.control_columns = control_columns\n    self.channel_columns = channel_columns\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataProcessor-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataProcessor.process","title":"<code>process(df: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Process the DataFrame with configured transformations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>MissingRequiredColumnsError</code> <p>If the required columns are not present.</p> <code>InvalidDateFormatError</code> <p>If the date column cannot be parsed.</p> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process the DataFrame with configured transformations.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Processed DataFrame\n\n    Raises:\n        MissingRequiredColumnsError: If the required columns are not present.\n        InvalidDateFormatError: If the date column cannot be parsed.\n\n    \"\"\"\n    processed_df = df.copy()\n\n    # Validate that all required columns exist\n    self._validate_required_columns_present(\n        df=processed_df,\n        date_column=self.date_column,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n        control_columns=self.control_columns,\n        channel_columns=self.channel_columns,\n    )\n\n    # Parse date columns\n    processed_df = self._parse_date_columns(processed_df, self.date_column)\n\n    # Rename required columns\n    processed_df = self._rename_required_columns(\n        df=processed_df,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n    )\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataValidator","title":"<code>DataValidator(control_columns: list[str] | None, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Validator for MMM data with configurable validation rules.</p> <p>Initialize validator with validation rules.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations for time series CV</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize validator with validation rules.\n\n    Args:\n        control_columns: List of control columns\n        min_number_observations: Minimum required number of observations for time series CV\n\n    \"\"\"\n    self.min_number_observations = min_number_observations\n    self.control_columns = control_columns\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataValidator-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataValidator.run_validations","title":"<code>run_validations(df: pd.DataFrame) -&gt; None</code>","text":"<p>Run all validations on the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>None</code> <p>Validation result with all errors and warnings</p> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def run_validations(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Run all validations on the DataFrame.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Validation result with all errors and warnings\n\n    \"\"\"\n    # Run each validation in order\n    self._validate_not_empty(df)\n    self._validate_schema(df)\n    self._validate_data_size(df)\n\n    if self.control_columns:\n        self._check_control_variables_between_0_and_1(df=df, cols=self.control_columns)\n</code></pre>"},{"location":"api/data/#mmm_eval.data-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.generate_data","title":"<code>generate_data()</code>","text":"<p>Generate synthetic MMM data for testing purposes.</p> <p>Returns     DataFrame containing synthetic MMM data with media channels, controls, and response variables</p> Source code in <code>mmm_eval/data/synth_data_generator.py</code> <pre><code>def generate_data():\n    \"\"\"Generate synthetic MMM data for testing purposes.\n\n    Returns\n        DataFrame containing synthetic MMM data with media channels, controls, and response variables\n\n    \"\"\"\n    seed: int = sum(map(ord, \"mmm\"))\n    rng: np.random.Generator = np.random.default_rng(seed=seed)\n\n    # date range\n    min_date = pd.to_datetime(\"2018-04-01\")\n    max_date = pd.to_datetime(\"2021-09-01\")\n\n    df = pd.DataFrame(data={\"date_week\": pd.date_range(start=min_date, end=max_date, freq=\"W-MON\")}).assign(\n        year=lambda x: x[\"date_week\"].dt.year,\n        month=lambda x: x[\"date_week\"].dt.month,\n        dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n    )\n\n    n = df.shape[0]\n\n    # media spend data\n    channel_1 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_1\"] = np.where(channel_1 &gt; 90, channel_1, channel_1 / 2)\n\n    channel_2 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_2\"] = np.where(channel_2 &gt; 80, channel_2, 0)\n\n    # apply geometric adstock transformation\n    alpha1: float = 0.4\n    alpha2: float = 0.2\n\n    df[\"channel_1_adstock\"] = (\n        geometric_adstock(x=df[\"channel_1\"].to_numpy(), alpha=alpha1, l_max=8, normalize=True).eval().flatten()\n    )\n\n    df[\"channel_2_adstock\"] = (\n        geometric_adstock(x=df[\"channel_2\"].to_numpy(), alpha=alpha2, l_max=8, normalize=True).eval().flatten()\n    )\n\n    # apply saturation transformation\n    lam1: float = 4.0\n    lam2: float = 3.0\n\n    df[\"channel_1_adstock_saturated\"] = logistic_saturation(x=df[\"channel_1_adstock\"].to_numpy(), lam=lam1).eval()\n\n    df[\"channel_2_adstock_saturated\"] = logistic_saturation(x=df[\"channel_2_adstock\"].to_numpy(), lam=lam2).eval()\n\n    # trend + seasonal\n    df[\"trend\"] = (np.linspace(start=0.0, stop=50, num=n) + 10) ** (1 / 4) - 1\n\n    df[\"cs\"] = -np.sin(2 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"cc\"] = np.cos(1 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"seasonality\"] = 0.5 * (df[\"cs\"] + df[\"cc\"])\n\n    # controls\n    df[\"event_1\"] = (df[\"date_week\"] == \"2019-05-13\").astype(float)\n    df[\"event_2\"] = (df[\"date_week\"] == \"2020-09-14\").astype(float)\n\n    # generate quantity\n    df[\"intercept\"] = 1000.0  # Base quantity\n    # noise\n    df[\"epsilon\"] = rng.normal(loc=0.0, scale=50.0, size=n)\n\n    # amplitude = 1\n    beta_1 = 400\n    beta_2 = 150\n\n    # Generate price with seasonal fluctuations\n    base_price = 5\n    price_seasonality = 0.03 * (df[\"cs\"] + df[\"cc\"])\n    price_trend = np.linspace(0, 2, n)  # Gradual price increase\n    df[\"price\"] = base_price + price_seasonality + price_trend\n\n    df[\"quantity\"] = (\n        df[\"intercept\"]\n        + df[\"trend\"] * 100\n        + df[\"seasonality\"] * 200\n        + df[\"price\"] * -50\n        + 150 * df[\"event_1\"]\n        + 250 * df[\"event_2\"]\n        + beta_1 * df[\"channel_1_adstock_saturated\"]\n        + beta_2 * df[\"channel_2_adstock_saturated\"]\n        + df[\"epsilon\"]\n    )\n    # Calculate revenue\n    df[\"revenue\"] = df[\"price\"] * df[\"quantity\"]\n\n    columns_to_keep = [\n        \"date_week\",\n        \"quantity\",\n        \"price\",\n        \"revenue\",\n        \"channel_1\",\n        \"channel_2\",\n        \"event_1\",\n        \"event_2\",\n        \"dayofyear\",\n    ]\n\n    df = df[columns_to_keep]\n    return df\n</code></pre>"},{"location":"api/data/#mmm_eval.data-modules","title":"Modules","text":""},{"location":"api/data/#mmm_eval.data.constants","title":"<code>constants</code>","text":"<p>Defines the constants for the data pipeline.</p>"},{"location":"api/data/#mmm_eval.data.constants-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.constants.DataLoaderConstants","title":"<code>DataLoaderConstants</code>","text":"<p>Constants for the data loader.</p> Classes\u00b6 <code>ValidDataExtensions</code> \u00b6 <p>Valid data extensions.</p> Functions\u00b6 <code>all()</code> <code>classmethod</code> \u00b6 <p>Return list of all supported file extensions.</p> Source code in <code>mmm_eval/data/constants.py</code> <pre><code>@classmethod\ndef all(cls):\n    \"\"\"Return list of all supported file extensions.\"\"\"\n    return [cls.CSV, cls.PARQUET]\n</code></pre>"},{"location":"api/data/#mmm_eval.data.constants.DataPipelineConstants","title":"<code>DataPipelineConstants</code>","text":"<p>Constants for the data pipeline.</p>"},{"location":"api/data/#mmm_eval.data.constants.InputDataframeConstants","title":"<code>InputDataframeConstants</code>","text":"<p>Constants for the dataframe.</p>"},{"location":"api/data/#mmm_eval.data.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for data validation and processing.</p>"},{"location":"api/data/#mmm_eval.data.exceptions-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.exceptions.DataValidationError","title":"<code>DataValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when data validation fails.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.EmptyDataFrameError","title":"<code>EmptyDataFrameError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when DataFrame is empty.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.InvalidDateFormatError","title":"<code>InvalidDateFormatError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when date parsing fails.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.MissingRequiredColumnsError","title":"<code>MissingRequiredColumnsError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when required columns are missing.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for validation errors.</p>"},{"location":"api/data/#mmm_eval.data.loaders","title":"<code>loaders</code>","text":"<p>Data loading utilities for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.loaders-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.loaders.DataLoader","title":"<code>DataLoader(data_path: str | Path)</code>","text":"<p>Simple data loader for MMM evaluation.</p> <p>Takes a data path and loads the data.</p> <p>Initialize data loader with data path.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str | Path</code> <p>Path to the data file (CSV, Parquet, etc.)</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the data file does not exist.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def __init__(self, data_path: str | Path):\n    \"\"\"Initialize data loader with data path.\n\n    Args:\n        data_path: Path to the data file (CSV, Parquet, etc.)\n\n    Raises:\n        FileNotFoundError: If the data file does not exist.\n\n    \"\"\"\n    self.data_path = Path(data_path)\n\n    if not self.data_path.exists():\n        raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n</code></pre> Functions\u00b6 <code>load() -&gt; pd.DataFrame</code> \u00b6 <p>Load data from the specified path.</p> <p>Returns     Loaded DataFrame</p> <p>Raises     ValueError: If the file format is not supported.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def load(self) -&gt; pd.DataFrame:\n    \"\"\"Load data from the specified path.\n\n    Returns\n        Loaded DataFrame\n\n    Raises\n        ValueError: If the file format is not supported.\n\n    \"\"\"\n    ext = self.data_path.suffix.lower().lstrip(\".\")\n    if ext not in DataLoaderConstants.ValidDataExtensions.all():\n        raise ValueError(f\"Unsupported file format: {self.data_path.suffix}\")\n\n    if ext == DataLoaderConstants.ValidDataExtensions.CSV:\n        return self._load_csv()\n    elif ext == DataLoaderConstants.ValidDataExtensions.PARQUET:\n        return self._load_parquet()\n</code></pre>"},{"location":"api/data/#mmm_eval.data.pipeline","title":"<code>pipeline</code>","text":"<p>Data pipeline for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.pipeline-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.pipeline.DataPipeline","title":"<code>DataPipeline(data: pd.DataFrame, control_columns: list[str] | None, channel_columns: list[str], date_column: str, response_column: str, revenue_column: str, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Data pipeline that orchestrates loading, processing, and validation.</p> <p>Provides a simple interface to go from raw data file to validated DataFrame.</p> <p>Initialize data pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the data</p> required <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column</p> required <code>response_column</code> <code>str</code> <p>Name of the response column</p> required <code>revenue_column</code> <code>str</code> <p>Name of the revenue column</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str,\n    response_column: str,\n    revenue_column: str,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize data pipeline.\n\n    Args:\n        data: DataFrame containing the data\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column\n        response_column: Name of the response column\n        revenue_column: Name of the revenue column\n        min_number_observations: Minimum required number of observations\n\n    \"\"\"\n    # Initialize components\n    self.data = data\n    self.processor = DataProcessor(\n        date_column=date_column,\n        response_column=response_column,\n        revenue_column=revenue_column,\n        control_columns=control_columns,\n        channel_columns=channel_columns,\n    )\n    self.validator = DataValidator(\n        control_columns=control_columns,\n        min_number_observations=min_number_observations,\n    )\n</code></pre> Functions\u00b6 <code>run() -&gt; pd.DataFrame</code> \u00b6 <p>Run the complete data pipeline: process \u2192 validate.</p> <p>Returns     Validated and processed DataFrame</p> <p>Raises     Various exceptions processing or validation steps</p> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"Run the complete data pipeline: process \u2192 validate.\n\n    Returns\n        Validated and processed DataFrame\n\n    Raises\n        Various exceptions processing or validation steps\n\n    \"\"\"\n    processed_df = self.processor.process(self.data)\n\n    self.validator.run_validations(processed_df)\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.processor","title":"<code>processor</code>","text":"<p>Data processing utilities for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.processor-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.processor.DataProcessor","title":"<code>DataProcessor(control_columns: list[str] | None, channel_columns: list[str], date_column: str = InputDataframeConstants.DATE_COL, response_column: str = InputDataframeConstants.RESPONSE_COL, revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL)</code>","text":"<p>Simple data processor for MMM evaluation.</p> <p>Handles data transformations like datetime casting, column renaming, etc.</p> <p>Initialize data processor.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column to parse and rename</p> <code>DATE_COL</code> <code>response_column</code> <code>str</code> <p>Name of the response column to parse and rename</p> <code>RESPONSE_COL</code> <code>revenue_column</code> <code>str</code> <p>Name of the revenue column to parse and rename</p> <code>MEDIA_CHANNEL_REVENUE_COL</code> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str = InputDataframeConstants.DATE_COL,\n    response_column: str = InputDataframeConstants.RESPONSE_COL,\n    revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,\n):\n    \"\"\"Initialize data processor.\n\n    Args:\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column to parse and rename\n        response_column: Name of the response column to parse and rename\n        revenue_column: Name of the revenue column to parse and rename\n\n    \"\"\"\n    self.date_column = date_column\n    self.response_column = response_column\n    self.revenue_column = revenue_column\n    self.control_columns = control_columns\n    self.channel_columns = channel_columns\n</code></pre> Functions\u00b6 <code>process(df: pd.DataFrame) -&gt; pd.DataFrame</code> \u00b6 <p>Process the DataFrame with configured transformations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>MissingRequiredColumnsError</code> <p>If the required columns are not present.</p> <code>InvalidDateFormatError</code> <p>If the date column cannot be parsed.</p> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process the DataFrame with configured transformations.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Processed DataFrame\n\n    Raises:\n        MissingRequiredColumnsError: If the required columns are not present.\n        InvalidDateFormatError: If the date column cannot be parsed.\n\n    \"\"\"\n    processed_df = df.copy()\n\n    # Validate that all required columns exist\n    self._validate_required_columns_present(\n        df=processed_df,\n        date_column=self.date_column,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n        control_columns=self.control_columns,\n        channel_columns=self.channel_columns,\n    )\n\n    # Parse date columns\n    processed_df = self._parse_date_columns(processed_df, self.date_column)\n\n    # Rename required columns\n    processed_df = self._rename_required_columns(\n        df=processed_df,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n    )\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.schemas","title":"<code>schemas</code>","text":"<p>Pydantic schemas for MMM data validation.</p>"},{"location":"api/data/#mmm_eval.data.schemas-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.schemas.ValidatedDataSchema","title":"<code>ValidatedDataSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Schema for MMM data validation.</p> <p>Defines the bare minimum columns for MMM evaluation.</p> Classes\u00b6 <code>Config</code> \u00b6 <p>Config for the schema.</p>"},{"location":"api/data/#mmm_eval.data.synth_data_generator","title":"<code>synth_data_generator</code>","text":"<p>Generate synthetic data for testing.</p> <p>Based on: https://www.pymc-marketing.io/en/stable/notebooks/mmm/mmm_example.html</p>"},{"location":"api/data/#mmm_eval.data.synth_data_generator-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.synth_data_generator.generate_data","title":"<code>generate_data()</code>","text":"<p>Generate synthetic MMM data for testing purposes.</p> <p>Returns     DataFrame containing synthetic MMM data with media channels, controls, and response variables</p> Source code in <code>mmm_eval/data/synth_data_generator.py</code> <pre><code>def generate_data():\n    \"\"\"Generate synthetic MMM data for testing purposes.\n\n    Returns\n        DataFrame containing synthetic MMM data with media channels, controls, and response variables\n\n    \"\"\"\n    seed: int = sum(map(ord, \"mmm\"))\n    rng: np.random.Generator = np.random.default_rng(seed=seed)\n\n    # date range\n    min_date = pd.to_datetime(\"2018-04-01\")\n    max_date = pd.to_datetime(\"2021-09-01\")\n\n    df = pd.DataFrame(data={\"date_week\": pd.date_range(start=min_date, end=max_date, freq=\"W-MON\")}).assign(\n        year=lambda x: x[\"date_week\"].dt.year,\n        month=lambda x: x[\"date_week\"].dt.month,\n        dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n    )\n\n    n = df.shape[0]\n\n    # media spend data\n    channel_1 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_1\"] = np.where(channel_1 &gt; 90, channel_1, channel_1 / 2)\n\n    channel_2 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_2\"] = np.where(channel_2 &gt; 80, channel_2, 0)\n\n    # apply geometric adstock transformation\n    alpha1: float = 0.4\n    alpha2: float = 0.2\n\n    df[\"channel_1_adstock\"] = (\n        geometric_adstock(x=df[\"channel_1\"].to_numpy(), alpha=alpha1, l_max=8, normalize=True).eval().flatten()\n    )\n\n    df[\"channel_2_adstock\"] = (\n        geometric_adstock(x=df[\"channel_2\"].to_numpy(), alpha=alpha2, l_max=8, normalize=True).eval().flatten()\n    )\n\n    # apply saturation transformation\n    lam1: float = 4.0\n    lam2: float = 3.0\n\n    df[\"channel_1_adstock_saturated\"] = logistic_saturation(x=df[\"channel_1_adstock\"].to_numpy(), lam=lam1).eval()\n\n    df[\"channel_2_adstock_saturated\"] = logistic_saturation(x=df[\"channel_2_adstock\"].to_numpy(), lam=lam2).eval()\n\n    # trend + seasonal\n    df[\"trend\"] = (np.linspace(start=0.0, stop=50, num=n) + 10) ** (1 / 4) - 1\n\n    df[\"cs\"] = -np.sin(2 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"cc\"] = np.cos(1 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"seasonality\"] = 0.5 * (df[\"cs\"] + df[\"cc\"])\n\n    # controls\n    df[\"event_1\"] = (df[\"date_week\"] == \"2019-05-13\").astype(float)\n    df[\"event_2\"] = (df[\"date_week\"] == \"2020-09-14\").astype(float)\n\n    # generate quantity\n    df[\"intercept\"] = 1000.0  # Base quantity\n    # noise\n    df[\"epsilon\"] = rng.normal(loc=0.0, scale=50.0, size=n)\n\n    # amplitude = 1\n    beta_1 = 400\n    beta_2 = 150\n\n    # Generate price with seasonal fluctuations\n    base_price = 5\n    price_seasonality = 0.03 * (df[\"cs\"] + df[\"cc\"])\n    price_trend = np.linspace(0, 2, n)  # Gradual price increase\n    df[\"price\"] = base_price + price_seasonality + price_trend\n\n    df[\"quantity\"] = (\n        df[\"intercept\"]\n        + df[\"trend\"] * 100\n        + df[\"seasonality\"] * 200\n        + df[\"price\"] * -50\n        + 150 * df[\"event_1\"]\n        + 250 * df[\"event_2\"]\n        + beta_1 * df[\"channel_1_adstock_saturated\"]\n        + beta_2 * df[\"channel_2_adstock_saturated\"]\n        + df[\"epsilon\"]\n    )\n    # Calculate revenue\n    df[\"revenue\"] = df[\"price\"] * df[\"quantity\"]\n\n    columns_to_keep = [\n        \"date_week\",\n        \"quantity\",\n        \"price\",\n        \"revenue\",\n        \"channel_1\",\n        \"channel_2\",\n        \"event_1\",\n        \"event_2\",\n        \"dayofyear\",\n    ]\n\n    df = df[columns_to_keep]\n    return df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.validation","title":"<code>validation</code>","text":"<p>Data validation for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.validation-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.validation.DataValidator","title":"<code>DataValidator(control_columns: list[str] | None, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Validator for MMM data with configurable validation rules.</p> <p>Initialize validator with validation rules.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations for time series CV</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize validator with validation rules.\n\n    Args:\n        control_columns: List of control columns\n        min_number_observations: Minimum required number of observations for time series CV\n\n    \"\"\"\n    self.min_number_observations = min_number_observations\n    self.control_columns = control_columns\n</code></pre> Functions\u00b6 <code>run_validations(df: pd.DataFrame) -&gt; None</code> \u00b6 <p>Run all validations on the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>None</code> <p>Validation result with all errors and warnings</p> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def run_validations(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Run all validations on the DataFrame.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Validation result with all errors and warnings\n\n    \"\"\"\n    # Run each validation in order\n    self._validate_not_empty(df)\n    self._validate_schema(df)\n    self._validate_data_size(df)\n\n    if self.control_columns:\n        self._check_control_variables_between_0_and_1(df=df, cols=self.control_columns)\n</code></pre>"},{"location":"api/metrics/","title":"Metrics Reference","text":""},{"location":"api/metrics/#mmm_eval.metrics","title":"<code>mmm_eval.metrics</code>","text":"<p>Accuracy metrics for MMM evaluation.</p>"},{"location":"api/metrics/#mmm_eval.metrics-functions","title":"Functions","text":""},{"location":"api/metrics/#mmm_eval.metrics.calculate_absolute_percentage_change","title":"<code>calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series</code>","text":"<p>Calculate the absolute percentage change between two series.</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series:\n    \"\"\"Calculate the absolute percentage change between two series.\"\"\"\n    return np.abs((comparison_series - baseline_series) / baseline_series)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_mean_for_singular_values_across_cross_validation_folds","title":"<code>calculate_mean_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the mean of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_mean_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the mean of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Mean value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.mean([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_means_for_series_across_cross_validation_folds","title":"<code>calculate_means_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the mean of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Mean Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_means_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the mean of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Mean Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).mean(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_std_for_singular_values_across_cross_validation_folds","title":"<code>calculate_std_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the standard deviation of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Standard deviation value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_std_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the standard deviation of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Standard deviation value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.std([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_stds_for_series_across_cross_validation_folds","title":"<code>calculate_stds_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the standard deviation of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Standard deviation Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_stds_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the standard deviation of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Standard deviation Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).std(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics-modules","title":"Modules","text":""},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions","title":"<code>accuracy_functions</code>","text":"<p>Accuracy metrics for MMM evaluation.</p>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions-functions","title":"Functions","text":""},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_absolute_percentage_change","title":"<code>calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series</code>","text":"<p>Calculate the absolute percentage change between two series.</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series:\n    \"\"\"Calculate the absolute percentage change between two series.\"\"\"\n    return np.abs((comparison_series - baseline_series) / baseline_series)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_mean_for_singular_values_across_cross_validation_folds","title":"<code>calculate_mean_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the mean of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_mean_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the mean of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Mean value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.mean([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_means_for_series_across_cross_validation_folds","title":"<code>calculate_means_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the mean of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Mean Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_means_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the mean of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Mean Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).mean(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_std_for_singular_values_across_cross_validation_folds","title":"<code>calculate_std_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the standard deviation of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Standard deviation value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_std_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the standard deviation of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Standard deviation value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.std([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_stds_for_series_across_cross_validation_folds","title":"<code>calculate_stds_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the standard deviation of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Standard deviation Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_stds_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the standard deviation of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Standard deviation Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).std(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models","title":"<code>metric_models</code>","text":""},{"location":"api/metrics/#mmm_eval.metrics.metric_models-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.metric_models.AccuracyMetricNames","title":"<code>AccuracyMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the accuracy metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.AccuracyMetricResults","title":"<code>AccuracyMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the accuracy metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    return self.mape &lt;= AccuracyThresholdConstants.MAPE and self.r_squared &gt;= AccuracyThresholdConstants.R_SQUARED\n</code></pre> <code>populate_object_with_metrics(actual: pd.Series, predicted: pd.Series) -&gt; AccuracyMetricResults</code> <code>classmethod</code> \u00b6 <p>Populate the object with the calculated metrics.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>Series</code> <p>The actual values</p> required <code>predicted</code> <code>Series</code> <p>The predicted values</p> required <p>Returns:</p> Type Description <code>AccuracyMetricResults</code> <p>AccuracyMetricResults object with the metrics</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>@classmethod\ndef populate_object_with_metrics(cls, actual: pd.Series, predicted: pd.Series) -&gt; \"AccuracyMetricResults\":\n    \"\"\"Populate the object with the calculated metrics.\n\n    Args:\n        actual: The actual values\n        predicted: The predicted values\n\n    Returns:\n        AccuracyMetricResults object with the metrics\n\n    \"\"\"\n    return cls(\n        mape=mean_absolute_percentage_error(actual, predicted),\n        r_squared=r2_score(actual, predicted),\n    )\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.CrossValidationMetricNames","title":"<code>CrossValidationMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the cross-validation metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.CrossValidationMetricResults","title":"<code>CrossValidationMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the cross-validation metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    return (\n        self.mean_mape &lt;= CrossValidationThresholdConstants.MEAN_MAPE\n        and self.std_mape &lt;= CrossValidationThresholdConstants.STD_MAPE\n        and self.mean_r_squared &gt;= CrossValidationThresholdConstants.MEAN_R_SQUARED\n    )\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.MetricNamesBase","title":"<code>MetricNamesBase</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Base class for metric name enums.</p> Functions\u00b6 <code>metrics_to_list() -&gt; list[str]</code> <code>classmethod</code> \u00b6 <p>Convert the enum to a list of strings.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>@classmethod\ndef metrics_to_list(cls) -&gt; list[str]:\n    \"\"\"Convert the enum to a list of strings.\"\"\"\n    return [member.value for member in cls]\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.MetricResults","title":"<code>MetricResults</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Define the results of the metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    raise NotImplementedError(\"Child classes must implement test_passed()\")\n</code></pre> <code>to_dict() -&gt; dict[str, Any]</code> \u00b6 <p>Convert the class of test results to dictionary format.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the class of test results to dictionary format.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.PerturbationMetricNames","title":"<code>PerturbationMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the perturbation metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.PerturbationMetricResults","title":"<code>PerturbationMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the perturbation metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    return bool(\n        (self.percentage_change_for_each_channel &lt;= PerturbationThresholdConstants.MEAN_PERCENTAGE_CHANGE).all()\n    )\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.RefreshStabilityMetricNames","title":"<code>RefreshStabilityMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the stability metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.RefreshStabilityMetricResults","title":"<code>RefreshStabilityMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the refresh stability metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    return bool(\n        (\n            self.mean_percentage_change_for_each_channel\n            &lt;= RefreshStabilityThresholdConstants.MEAN_PERCENTAGE_CHANGE\n        ).all()\n    )\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants","title":"<code>threshold_constants</code>","text":""},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.AccuracyThresholdConstants","title":"<code>AccuracyThresholdConstants</code>","text":"<p>Constants for the accuracy threshold.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.CrossValidationThresholdConstants","title":"<code>CrossValidationThresholdConstants</code>","text":"<p>Constants for the cross-validation threshold.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.PerturbationThresholdConstants","title":"<code>PerturbationThresholdConstants</code>","text":"<p>Constants for the perturbation threshold.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.RefreshStabilityThresholdConstants","title":"<code>RefreshStabilityThresholdConstants</code>","text":"<p>Constants for the refresh stability threshold.</p>"},{"location":"development/contributing/","title":"Contributing to mmm-eval","text":"<p>We welcome contributions from the community! This guide will help you get started with contributing to mmm-eval.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ol> <li>Python 3.11+ - Required for development</li> <li>Git - For version control</li> <li>Poetry - For dependency management (recommended)</li> </ol>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository on GitHub</li> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/YOUR_USERNAME/mmm-eval.git\ncd mmm-eval\n</code></pre></p> </li> <li> <p>Set up the development environment:    <pre><code># Install dependencies\npoetry install\n\n# Activate the environment\npoetry shell\n</code></pre></p> </li> <li> <p>Install pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write your code following the coding standards</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> </ul>"},{"location":"development/contributing/#3-test-your-changes","title":"3. Test Your Changes","text":"<pre><code># Run all tests\ntox\n\n# Run specific test categories\npytest tests/unit/\npytest tests/integration/\n\n# Run linting and formatting\nblack mmm_eval tests\nisort mmm_eval tests\nruff check mmm_eval tests\n</code></pre>"},{"location":"development/contributing/#4-commit-your-changes","title":"4. Commit Your Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add new feature description\"\n</code></pre> <p>Follow the conventional commits format:</p> <ul> <li><code>feat:</code> New features</li> <li><code>fix:</code> Bug fixes</li> <li><code>docs:</code> Documentation changes</li> <li><code>style:</code> Code style changes</li> <li><code>refactor:</code> Code refactoring</li> <li><code>test:</code> Test changes</li> <li><code>chore:</code> Maintenance tasks</li> </ul>"},{"location":"development/contributing/#5-push-and-create-a-pull-request","title":"5. Push and Create a Pull Request","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"development/contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"development/contributing/#python-code-style","title":"Python Code Style","text":"<p>We use several tools to maintain code quality:</p> <ul> <li>Black - Code formatting</li> <li>isort - Import sorting</li> <li>Ruff - Linting and additional checks</li> <li>Pyright - Type checking</li> </ul>"},{"location":"development/contributing/#code-formatting","title":"Code Formatting","text":"<pre><code># Format code\nblack mmm_eval tests\n\n# Sort imports\nisort mmm_eval tests\n\n# Run linting\nruff check mmm_eval tests\nruff check --fix mmm_eval tests\n</code></pre>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>We use type hints throughout the codebase:</p> <pre><code>from typing import List, Optional, Dict, Any\n\ndef process_data(data: List[Dict[str, Any]]) -&gt; Optional[Dict[str, float]]:\n    \"\"\"Process the input data and return results.\"\"\"\n    pass\n</code></pre>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def calculate_mape(actual: List[float], predicted: List[float]) -&gt; float:\n    \"\"\"Calculate Mean Absolute Percentage Error.\n\n    Args:\n        actual: List of actual values\n        predicted: List of predicted values\n\n    Returns:\n        MAPE value as a float\n\n    Raises:\n        ValueError: If inputs are empty or have different lengths\n    \"\"\"\n    pass\n</code></pre>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=mmm_eval\n\n# Run specific test file\npytest tests/test_metrics.py\n\n# Run tests in parallel\npytest -n auto\n</code></pre>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Use descriptive test names</li> <li>Test both success and failure cases</li> <li>Use fixtures for common test data</li> </ul> <p>Example test:</p> <pre><code>import pytest\nfrom mmm_eval.metrics import calculate_mape\n\ndef test_calculate_mape_basic():\n    \"\"\"Test basic MAPE calculation.\"\"\"\n    actual = [100, 200, 300]\n    predicted = [110, 190, 310]\n\n    mape = calculate_mape(actual, predicted)\n\n    assert isinstance(mape, float)\n    assert mape &gt; 0\n\ndef test_calculate_mape_empty_input():\n    \"\"\"Test MAPE calculation with empty input.\"\"\"\n    with pytest.raises(ValueError):\n        calculate_mape([], [])\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#updating-documentation","title":"Updating Documentation","text":"<ol> <li>Update docstrings in the code</li> <li>Update markdown files in the <code>docs/</code> directory</li> <li>Build and test documentation:    <pre><code>mkdocs serve\n</code></pre></li> </ol>"},{"location":"development/contributing/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Keep documentation up to date with code changes</li> <li>Use proper markdown formatting</li> </ul>"},{"location":"development/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Ensure all tests pass</li> <li>Update documentation if needed</li> <li>Add tests for new functionality</li> <li>Follow coding standards</li> <li>Update CHANGELOG.md if adding new features</li> </ol>"},{"location":"development/contributing/#pull-request-template","title":"Pull Request Template","text":"<p>Use the provided pull request template and fill in all sections:</p> <ul> <li>Description - What does this PR do?</li> <li>Type of change - Bug fix, feature, documentation, etc.</li> <li>Testing - How was this tested?</li> <li>Checklist - Ensure all items are completed</li> </ul>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated checks must pass</li> <li>Code review by maintainers</li> <li>Documentation review if needed</li> <li>Merge after approval</li> </ol>"},{"location":"development/contributing/#issue-reporting","title":"Issue Reporting","text":""},{"location":"development/contributing/#before-creating-an-issue","title":"Before Creating an Issue","text":"<ol> <li>Search existing issues to avoid duplicates</li> <li>Check documentation for solutions</li> <li>Try the latest version of mmm-eval</li> </ol>"},{"location":"development/contributing/#issue-template","title":"Issue Template","text":"<p>Use the provided issue template and include:</p> <ul> <li>Description - Clear description of the problem</li> <li>Steps to reproduce - Detailed steps</li> <li>Expected behavior - What should happen</li> <li>Actual behavior - What actually happens</li> <li>Environment - OS, Python version, mmm-eval version</li> <li>Additional context - Any other relevant information</li> </ul>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive environment. Please:</p> <ul> <li>Be respectful and inclusive</li> <li>Use welcoming and inclusive language</li> <li>Be collaborative and constructive</li> <li>Focus on what is best for the community</li> </ul>"},{"location":"development/contributing/#communication","title":"Communication","text":"<ul> <li>GitHub Issues - For bug reports and feature requests</li> <li>GitHub Discussions - For questions and general discussion</li> <li>Pull Requests - For code contributions</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with contributing:</p> <ol> <li>Check the documentation first</li> <li>Search existing issues and discussions</li> <li>Create a new discussion for questions</li> <li>Join our community channels</li> </ol>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in:</p> <ul> <li>README.md - For significant contributions</li> <li>CHANGELOG.md - For all contributions</li> <li>GitHub contributors page</li> </ul> <p>Thank you for contributing to mmm-eval! \ud83c\udf89 </p>"},{"location":"development/setup/","title":"Development Setup","text":"<p>This guide will help you set up a development environment for contributing to mmm-eval.</p>"},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<p>Before setting up the development environment, ensure you have the following installed:</p> <ul> <li>Python 3.11+: The minimum supported Python version</li> <li>Poetry 2.x.x: For dependency management and packaging</li> <li>Git: For version control</li> </ul>"},{"location":"development/setup/#quick-setup","title":"Quick Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>poetry install\n</code></pre></p> </li> <li> <p>Verify the installation:    <pre><code>poetry run mmm-eval --help\n</code></pre></p> </li> </ol>"},{"location":"development/setup/#development-environment","title":"Development Environment","text":""},{"location":"development/setup/#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<p>Poetry automatically creates and manages a virtual environment for the project:</p> <pre><code># Activate the virtual environment\npoetry shell\n\n# Run commands within the environment\npoetry run python -m pytest\n\n# Install additional development dependencies\npoetry add --group dev package-name\n</code></pre>"},{"location":"development/setup/#using-pip-alternative","title":"Using pip (Alternative)","text":"<p>If you prefer using pip directly:</p> <pre><code># Create a virtual environment\npython -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -r requirements-dev.txt  # If available\n</code></pre>"},{"location":"development/setup/#project-structure","title":"Project Structure","text":"<pre><code>mmm-eval/\n\u251c\u2500\u2500 mmm_eval/              # Main package\n\u2502   \u251c\u2500\u2500 adapters/          # Framework adapters\n\u2502   \u251c\u2500\u2500 cli/               # Command-line interface\n\u2502   \u251c\u2500\u2500 core/              # Core evaluation logic\n\u2502   \u251c\u2500\u2500 data/              # Data handling and validation\n\u2502   \u2514\u2500\u2500 metrics/           # Evaluation metrics\n\u251c\u2500\u2500 tests/                 # Test suite\n\u251c\u2500\u2500 docs/                  # Documentation\n\u251c\u2500\u2500 pyproject.toml         # Project configuration\n\u2514\u2500\u2500 README.md             # Project overview\n</code></pre>"},{"location":"development/setup/#code-quality-tools","title":"Code Quality Tools","text":"<p>The project uses several tools to maintain code quality:</p>"},{"location":"development/setup/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks to automatically format and lint your code:</p> <pre><code># Install pre-commit\npoetry add --group dev pre-commit\n\n# Install the git hook scripts\npre-commit install\n\n# Run against all files\npre-commit run --all-files\n</code></pre>"},{"location":"development/setup/#code-formatting","title":"Code Formatting","text":"<p>The project uses Black for code formatting:</p> <pre><code># Format all Python files\npoetry run black .\n\n# Check formatting without making changes\npoetry run black --check .\n</code></pre>"},{"location":"development/setup/#import-sorting","title":"Import Sorting","text":"<p>isort is used to organize imports:</p> <pre><code># Sort imports\npoetry run isort .\n\n# Check import sorting\npoetry run isort --check-only .\n</code></pre>"},{"location":"development/setup/#linting","title":"Linting","text":"<p>Multiple linters are configured:</p> <pre><code># Run all linters\npoetry run ruff check .\npoetry run flake8 .\n\n# Auto-fix issues where possible\npoetry run ruff check --fix .\n</code></pre>"},{"location":"development/setup/#type-checking","title":"Type Checking","text":"<p>Pyright is used for static type checking:</p> <pre><code># Run type checker\npoetry run pyright\n</code></pre>"},{"location":"development/setup/#running-tests","title":"Running Tests","text":""},{"location":"development/setup/#unit-tests","title":"Unit Tests","text":"<pre><code># Run all tests\npoetry run pytest\n\n# Run tests with coverage\npoetry run pytest --cov=mmm_eval\n\n# Run tests in parallel\npoetry run pytest -n auto\n\n# Run specific test file\npoetry run pytest tests/test_core.py\n</code></pre>"},{"location":"development/setup/#integration-tests","title":"Integration Tests","text":"<pre><code># Run integration tests\npoetry run pytest -m integration\n</code></pre>"},{"location":"development/setup/#test-coverage","title":"Test Coverage","text":"<pre><code># Generate coverage report\npoetry run pytest --cov=mmm_eval --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html  # On macOS\n# or\nstart htmlcov/index.html  # On Windows\n</code></pre>"},{"location":"development/setup/#documentation","title":"Documentation","text":""},{"location":"development/setup/#building-documentation","title":"Building Documentation","text":"<pre><code># Install documentation dependencies\npoetry install --with docs\n\n# Build documentation\npoetry run mkdocs build\n\n# Serve documentation locally\npoetry run mkdocs serve\n</code></pre> <p>The documentation will be available at <code>http://localhost:8000</code>.</p>"},{"location":"development/setup/#api-documentation","title":"API Documentation","text":"<p>API documentation is automatically generated from docstrings using mkdocstrings. To update the API docs:</p> <ol> <li>Ensure your code has proper docstrings</li> <li>Build the documentation: <code>poetry run mkdocs build</code></li> <li>The API reference will be updated automatically</li> </ol>"},{"location":"development/setup/#ide-configuration","title":"IDE Configuration","text":""},{"location":"development/setup/#vs-code","title":"VS Code","text":"<p>Recommended VS Code extensions:</p> <ul> <li>Python</li> <li>Pylance</li> <li>Black Formatter</li> <li>isort</li> <li>Ruff</li> </ul> <p>Add to your <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"python.defaultInterpreterPath\": \"./venv/bin/python\",\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.ruffEnabled\": true,\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.organizeImports\": true\n    }\n}\n</code></pre>"},{"location":"development/setup/#pycharm","title":"PyCharm","text":"<ol> <li>Open the project in PyCharm</li> <li>Configure the Python interpreter to use the Poetry virtual environment</li> <li>Enable auto-import organization</li> <li>Configure Black as the code formatter</li> </ol>"},{"location":"development/setup/#common-issues","title":"Common Issues","text":""},{"location":"development/setup/#poetry-installation-issues","title":"Poetry Installation Issues","text":"<p>If you encounter issues with Poetry:</p> <pre><code># Update Poetry to the latest version\npoetry self update\n\n# Clear Poetry cache\npoetry cache clear --all pypi\n</code></pre>"},{"location":"development/setup/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>If you encounter dependency conflicts:</p> <pre><code># Update all dependencies\npoetry update\n\n# Remove and reinstall dependencies\npoetry lock --no-update\npoetry install\n</code></pre>"},{"location":"development/setup/#test-failures","title":"Test Failures","text":"<p>If tests are failing:</p> <ol> <li>Ensure you're using the correct Python version (3.11+)</li> <li>Check that all dependencies are installed: <code>poetry install</code></li> <li>Run tests with verbose output: <code>poetry run pytest -v</code></li> <li>Check for any environment-specific issues</li> </ol>"},{"location":"development/setup/#next-steps","title":"Next Steps","text":"<p>Once your development environment is set up:</p> <ol> <li>Read the Contributing Guide for guidelines</li> <li>Check the Testing Guide for testing practices</li> <li>Look at existing issues and pull requests</li> <li>Start with a small contribution to get familiar with the codebase</li> </ol>"},{"location":"development/setup/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during setup:</p> <ol> <li>Check the GitHub Issues</li> <li>Review the Contributing Guide</li> <li>Ask questions in the project discussions </li> </ol>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>This guide covers testing practices and procedures for the mmm-eval project.</p>"},{"location":"development/testing/#testing-philosophy","title":"Testing Philosophy","text":"<p>We follow these testing principles:</p> <ul> <li>Comprehensive coverage: Aim for high test coverage across all modules</li> <li>Fast feedback: Tests should run quickly to enable rapid development</li> <li>Reliable: Tests should be deterministic and not flaky</li> <li>Maintainable: Tests should be easy to understand and modify</li> <li>Realistic: Tests should reflect real-world usage patterns</li> </ul>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<p>The test suite is organized as follows:</p> <pre><code>tests/\n\u251c\u2500\u2500 unit/                    # Unit tests for individual components\n\u2502   \u251c\u2500\u2500 test_core/          # Core functionality tests\n\u2502   \u251c\u2500\u2500 test_adapters/      # Framework adapter tests\n\u2502   \u251c\u2500\u2500 test_data/          # Data handling tests\n\u2502   \u2514\u2500\u2500 test_metrics/       # Metrics calculation tests\n\u251c\u2500\u2500 integration/            # Integration tests\n\u251c\u2500\u2500 fixtures/               # Test data and fixtures\n\u2514\u2500\u2500 conftest.py            # Pytest configuration and shared fixtures\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npoetry run pytest\n\n# Run tests with verbose output\npoetry run pytest -v\n\n# Run tests with coverage\npoetry run pytest --cov=mmm_eval\n\n# Run tests in parallel\npoetry run pytest -n auto\n</code></pre>"},{"location":"development/testing/#running-specific-test-categories","title":"Running Specific Test Categories","text":"<pre><code># Run only unit tests\npoetry run pytest tests/unit/\n\n# Run only integration tests\npoetry run pytest tests/integration/\n\n# Run tests for a specific module\npoetry run pytest tests/unit/test_core/\n\n# Run tests matching a pattern\npoetry run pytest -k \"test_accuracy\"\n</code></pre>"},{"location":"development/testing/#running-tests-with-markers","title":"Running Tests with Markers","text":"<pre><code># Run integration tests only\npoetry run pytest -m integration\n\n# Run slow tests only\npoetry run pytest -m slow\n\n# Skip slow tests\npoetry run pytest -m \"not slow\"\n</code></pre>"},{"location":"development/testing/#test-types","title":"Test Types","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Unit tests verify individual functions and classes in isolation. They should:</p> <ul> <li>Test one specific behavior or functionality</li> <li>Use mocks for external dependencies</li> <li>Be fast and deterministic</li> <li>Have clear, descriptive names</li> </ul> <p>Example unit test:</p> <pre><code>def test_calculate_mape_returns_correct_value():\n    \"\"\"Test that MAPE calculation returns expected results.\"\"\"\n    actual = [100, 200, 300]\n    predicted = [110, 190, 310]\n\n    result = calculate_mape(actual, predicted)\n\n    expected = 10.0  # 10% average error\n    assert result == pytest.approx(expected, rel=1e-2)\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests verify that multiple components work together correctly. They:</p> <ul> <li>Test the interaction between different modules</li> <li>Use real data and minimal mocking</li> <li>May take longer to run</li> <li>Are marked with the <code>@pytest.mark.integration</code> decorator</li> </ul> <p>Example integration test:</p> <pre><code>@pytest.mark.integration\ndef test_pymc_marketing_evaluation_workflow():\n    \"\"\"Test complete PyMC Marketing evaluation workflow.\"\"\"\n    # Setup test data\n    data = load_test_data()\n\n    # Run evaluation\n    result = evaluate_framework(\n        data=data,\n        framework=\"pymc-marketing\",\n        config=test_config\n    )\n\n    # Verify results\n    assert result.accuracy &gt; 0.8\n    assert result.cross_validation_score &gt; 0.7\n    assert result.refresh_stability &gt; 0.6\n</code></pre>"},{"location":"development/testing/#property-based-tests","title":"Property-Based Tests","text":"<p>For complex calculations, we use property-based testing with Hypothesis:</p> <pre><code>from hypothesis import given, strategies as st\n\n@given(\n    actual=st.lists(st.floats(min_value=0.1, max_value=1000), min_size=1),\n    predicted=st.lists(st.floats(min_value=0.1, max_value=1000), min_size=1)\n)\ndef test_mape_properties(actual, predicted):\n    \"\"\"Test MAPE calculation properties.\"\"\"\n    if len(actual) == len(predicted):\n        mape = calculate_mape(actual, predicted)\n\n        # MAPE should always be non-negative\n        assert mape &gt;= 0\n\n        # MAPE should be 0 when predictions are perfect\n        if actual == predicted:\n            assert mape == 0\n</code></pre>"},{"location":"development/testing/#test-data-and-fixtures","title":"Test Data and Fixtures","text":""},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<p>Pytest fixtures provide reusable test data and setup:</p> <pre><code>@pytest.fixture\ndef sample_mmm_data():\n    \"\"\"Provide sample MMM data for testing.\"\"\"\n    return pd.DataFrame({\n        'date': pd.date_range('2023-01-01', periods=100),\n        'sales': np.random.normal(1000, 100, 100),\n        'tv_spend': np.random.uniform(0, 1000, 100),\n        'radio_spend': np.random.uniform(0, 500, 100),\n        'digital_spend': np.random.uniform(0, 800, 100)\n    })\n\ndef test_data_validation(sample_mmm_data):\n    \"\"\"Test data validation with sample data.\"\"\"\n    validator = DataValidator()\n    result = validator.validate(sample_mmm_data)\n    assert result.is_valid\n</code></pre>"},{"location":"development/testing/#test-data-management","title":"Test Data Management","text":"<ul> <li>Store test data in <code>tests/fixtures/</code></li> <li>Use realistic but synthetic data</li> <li>Keep test data files small and focused</li> <li>Document the structure and purpose of test data</li> </ul>"},{"location":"development/testing/#mocking-and-stubbing","title":"Mocking and Stubbing","text":""},{"location":"development/testing/#when-to-mock","title":"When to Mock","text":"<p>Mock external dependencies to:</p> <ul> <li>Speed up tests</li> <li>Avoid network calls</li> <li>Control test conditions</li> <li>Test error scenarios</li> </ul>"},{"location":"development/testing/#mocking-examples","title":"Mocking Examples","text":"<pre><code>from unittest.mock import Mock, patch\n\ndef test_api_call_with_mock():\n    \"\"\"Test API call with mocked response.\"\"\"\n    with patch('requests.get') as mock_get:\n        mock_get.return_value.json.return_value = {'status': 'success'}\n        mock_get.return_value.status_code = 200\n\n        result = fetch_data_from_api()\n\n        assert result['status'] == 'success'\n        mock_get.assert_called_once()\n</code></pre>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":""},{"location":"development/testing/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Minimum coverage: 80% for all modules</li> <li>Target coverage: 90% for critical modules</li> <li>Critical modules: Core evaluation logic, data validation, metrics calculation</li> </ul>"},{"location":"development/testing/#coverage-reports","title":"Coverage Reports","text":"<pre><code># Generate HTML coverage report\npoetry run pytest --cov=mmm_eval --cov-report=html\n\n# Generate XML coverage report (for CI)\npoetry run pytest --cov=mmm_eval --cov-report=xml\n\n# View coverage summary\npoetry run pytest --cov=mmm_eval --cov-report=term-missing\n</code></pre>"},{"location":"development/testing/#coverage-configuration","title":"Coverage Configuration","text":"<p>Configure coverage in <code>pyproject.toml</code>:</p> <pre><code>[tool.coverage.run]\nsource = [\"mmm_eval\"]\nomit = [\n    \"*/tests/*\",\n    \"*/test_*\",\n    \"*/__pycache__/*\"\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"if self.debug:\",\n    \"if settings.DEBUG\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n    \"if 0:\",\n    \"if __name__ == .__main__.:\",\n    \"class .*\\\\bProtocol\\\\):\",\n    \"@(abc\\\\.)?abstractmethod\"\n]\n</code></pre>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"development/testing/#benchmark-tests","title":"Benchmark Tests","text":"<p>For performance-critical code, use benchmark tests:</p> <pre><code>def test_mape_calculation_performance(benchmark):\n    \"\"\"Benchmark MAPE calculation performance.\"\"\"\n    actual = np.random.normal(1000, 100, 10000)\n    predicted = np.random.normal(1000, 100, 10000)\n\n    result = benchmark(lambda: calculate_mape(actual, predicted))\n\n    assert result &gt; 0\n</code></pre>"},{"location":"development/testing/#memory-usage-tests","title":"Memory Usage Tests","text":"<p>Monitor memory usage in tests:</p> <pre><code>import psutil\nimport os\n\ndef test_memory_usage():\n    \"\"\"Test that operations don't use excessive memory.\"\"\"\n    process = psutil.Process(os.getpid())\n    initial_memory = process.memory_info().rss\n\n    # Run memory-intensive operation\n    result = process_large_dataset()\n\n    final_memory = process.memory_info().rss\n    memory_increase = final_memory - initial_memory\n\n    # Memory increase should be reasonable (&lt; 100MB)\n    assert memory_increase &lt; 100 * 1024 * 1024\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions","title":"GitHub Actions","text":"<p>Tests run automatically on:</p> <ul> <li>Every pull request</li> <li>Every push to main branch</li> <li>Scheduled runs (nightly)</li> </ul>"},{"location":"development/testing/#ci-configuration","title":"CI Configuration","text":"<p>The CI pipeline includes:</p> <ol> <li>Linting: Code style and quality checks</li> <li>Type checking: Static type analysis</li> <li>Unit tests: Fast feedback on basic functionality</li> <li>Integration tests: Verify component interactions</li> <li>Coverage reporting: Track test coverage trends</li> </ol>"},{"location":"development/testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks to catch issues early:</p> <pre><code># Install pre-commit\npoetry add --group dev pre-commit\n\n# Install hooks\npre-commit install\n\n# Run all hooks\npre-commit run --all-files\n</code></pre>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":""},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code># Run with maximum verbosity\npoetry run pytest -vvv\n\n# Show local variables on failures\npoetry run pytest -l\n\n# Stop on first failure\npoetry run pytest -x\n</code></pre>"},{"location":"development/testing/#debugging-with-pdb","title":"Debugging with pdb","text":"<pre><code>def test_debug_example():\n    \"\"\"Example of using pdb for debugging.\"\"\"\n    import pdb; pdb.set_trace()  # Breakpoint\n    result = complex_calculation()\n    assert result &gt; 0\n</code></pre>"},{"location":"development/testing/#test-isolation","title":"Test Isolation","text":"<p>Ensure tests don't interfere with each other:</p> <pre><code>@pytest.fixture(autouse=True)\ndef reset_global_state():\n    \"\"\"Reset global state before each test.\"\"\"\n    # Setup\n    yield\n    # Teardown\n    cleanup_global_state()\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":""},{"location":"development/testing/#test-naming","title":"Test Naming","text":"<ul> <li>Use descriptive test names that explain the expected behavior</li> <li>Follow the pattern: <code>test_[function]_[scenario]_[expected_result]</code></li> <li>Include edge cases and error conditions</li> </ul>"},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<ul> <li>Group related tests in classes</li> <li>Use fixtures for common setup</li> <li>Keep tests focused and single-purpose</li> </ul>"},{"location":"development/testing/#assertions","title":"Assertions","text":"<ul> <li>Use specific assertions (<code>assert result == expected</code>)</li> <li>Avoid complex logic in assertions</li> <li>Use appropriate assertion methods (<code>assertIn</code>, <code>assertRaises</code>, etc.)</li> </ul>"},{"location":"development/testing/#test-data","title":"Test Data","text":"<ul> <li>Use realistic test data</li> <li>Avoid hardcoded magic numbers</li> <li>Document test data assumptions</li> </ul>"},{"location":"development/testing/#documentation","title":"Documentation","text":"<ul> <li>Write clear docstrings for test functions</li> <li>Explain complex test scenarios</li> <li>Document test data sources and assumptions</li> </ul>"},{"location":"development/testing/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"development/testing/#flaky-tests","title":"Flaky Tests","text":"<p>Avoid flaky tests by:</p> <ul> <li>Not relying on timing or external services</li> <li>Using deterministic random seeds</li> <li>Properly mocking external dependencies</li> <li>Avoiding shared state between tests</li> </ul>"},{"location":"development/testing/#slow-tests","title":"Slow Tests","text":"<p>Keep tests fast by:</p> <ul> <li>Using appropriate mocks</li> <li>Minimizing I/O operations</li> <li>Using efficient test data</li> <li>Running tests in parallel when possible</li> </ul>"},{"location":"development/testing/#over-mocking","title":"Over-Mocking","text":"<p>Don't over-mock:</p> <ul> <li>Test the actual behavior, not the implementation</li> <li>Mock only external dependencies</li> <li>Use real objects when possible</li> </ul>"},{"location":"development/testing/#getting-help","title":"Getting Help","text":"<p>If you encounter testing issues:</p> <ol> <li>Check the pytest documentation</li> <li>Review existing tests for examples</li> <li>Ask questions in project discussions</li> <li>Consult the Contributing Guide </li> </ol>"},{"location":"examples/basic-usage/","title":"Basic Usage Examples","text":"<p>This guide provides practical examples of how to use mmm-eval for different scenarios.</p>"},{"location":"examples/basic-usage/#example-1-basic-evaluation","title":"Example 1: Basic Evaluation","text":"<p>The simplest way to run an evaluation:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path ./results/\n</code></pre> <p>This assumes your data has standard column names and a valid configuration file.</p>"},{"location":"examples/basic-usage/#example-2-custom-configuration","title":"Example 2: Custom Configuration","text":"<p>Create a configuration file <code>config.json</code>:</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\", \"holiday\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\",\n    \"yearly_seasonality\": 2\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.9,\n    \"draws\": 100,\n    \"tune\": 50,\n    \"chains\": 2,\n    \"random_seed\": 42\n  },\n  \"revenue_column\": \"revenue\",\n  \"response_column\": \"sales\"\n}\n</code></pre> <p>Run the evaluation:</p> <pre><code>mmm-eval \\\n  --input-data-path marketing_data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path ./results/\n</code></pre>"},{"location":"examples/basic-usage/#example-3-specific-tests-only","title":"Example 3: Specific Tests Only","text":"<p>Run only certain validation tests:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path ./results/ \\\n  --test-names accuracy cross_validation\n</code></pre> <p>Available tests: - <code>accuracy</code> - Model accuracy using holdout validation - <code>cross_validation</code> - Time series cross-validation - <code>refresh_stability</code> - Model stability over time - <code>perturbation</code> - Sensitivity to data changes</p>"},{"location":"examples/basic-usage/#example-4-verbose-output","title":"Example 4: Verbose Output","text":"<p>Get detailed information during execution:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path ./results/ \\\n  --verbose\n</code></pre>"},{"location":"examples/basic-usage/#example-5-advanced-configuration","title":"Example 5: Advanced Configuration","text":"<p>For more complex models, use an advanced configuration:</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\", \"radio_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\", \"holiday\", \"competitor_promo\"],\n    \"adstock\": \"WeibullAdstock(l_max=6)\",\n    \"saturation\": \"HillSaturation()\",\n    \"yearly_seasonality\": 4,\n    \"time_varying_intercept\": true,\n    \"time_varying_media\": false\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.95,\n    \"draws\": 2000,\n    \"tune\": 1000,\n    \"chains\": 4,\n    \"random_seed\": 123,\n    \"progress_bar\": true,\n    \"return_inferencedata\": true\n  },\n  \"revenue_column\": \"revenue\",\n  \"response_column\": \"sales\"\n}\n</code></pre> <p>Run with verbose output:</p> <pre><code>mmm-eval \\\n  --input-data-path marketing_data.csv \\\n  --framework pymc-marketing \\\n  --config-path advanced_config.json \\\n  --test-names accuracy cross_validation refresh_stability perturbation \\\n  --output-path ./advanced_results/ \\\n  --verbose\n</code></pre>"},{"location":"examples/basic-usage/#example-6-minimal-configuration","title":"Example 6: Minimal Configuration","text":"<p>For quick testing, use a minimal configuration:</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date\",\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\"\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.9,\n    \"draws\": 50,\n    \"tune\": 25,\n    \"chains\": 1,\n    \"random_seed\": 42\n  },\n  \"revenue_column\": \"revenue\"\n}\n</code></pre> <p>Run minimal evaluation:</p> <pre><code>mmm-eval \\\n  --input-data-path test_data.csv \\\n  --framework pymc-marketing \\\n  --config-path minimal_config.json \\\n  --output-path ./test_results/ \\\n  --test-names accuracy\n</code></pre>"},{"location":"examples/basic-usage/#data-format-examples","title":"Data Format Examples","text":""},{"location":"examples/basic-usage/#basic-csv-structure","title":"Basic CSV Structure","text":"<pre><code>date_week,quantity,revenue,channel_1,channel_2,price,event_1,event_2\n2023-01-01,1000,7000,5000,2000,10.99,0,0\n2023-01-08,1200,8000,5500,2200,10.99,0,0\n2023-01-15,1100,7500,5200,2100,11.99,1,0\n2023-01-22,1300,9000,6000,2400,11.99,0,1\n2023-01-29,1400,9500,6500,2600,12.99,0,0\n2023-02-05,1500,10000,7000,2800,12.99,0,0\n2023-02-12,1600,10500,7500,3000,13.99,1,0\n2023-02-19,1700,11000,8000,3200,13.99,0,1\n2023-02-26,1800,11500,8500,3400,14.99,0,0\n2023-03-05,1900,12000,9000,3600,14.99,0,0\n</code></pre>"},{"location":"examples/basic-usage/#with-more-channels","title":"With More Channels","text":"<pre><code>date_week,sales,revenue,tv_spend,digital_spend,print_spend,radio_spend,price,seasonality,holiday\n2023-01-01,1000,7000,5000,2000,1000,500,10.99,0.8,0\n2023-01-08,1200,8000,5500,2200,1100,550,10.99,0.9,0\n2023-01-15,1100,7500,5200,2100,1050,520,11.99,0.7,1\n2023-01-22,1300,9000,6000,2400,1200,600,11.99,0.8,0\n2023-01-29,1400,9500,6500,2600,1300,650,12.99,0.9,0\n</code></pre>"},{"location":"examples/basic-usage/#expected-output","title":"Expected Output","text":"<p>After running an evaluation, you'll find a CSV file in your output directory:</p> <pre><code>results/\n\u2514\u2500\u2500 mmm_eval_pymc-marketing_20241201_143022.csv\n</code></pre>"},{"location":"examples/basic-usage/#sample-results","title":"Sample Results","text":"<pre><code>test_name,metric_name,metric_value,metric_pass\naccuracy,mape,0.15,True\naccuracy,r_squared,0.85,True\ncross_validation,mape,0.18,True\ncross_validation,r_squared,0.82,True\nrefresh_stability,mean_percentage_change_for_each_channel:channel_1,0.05,True\nrefresh_stability,mean_percentage_change_for_each_channel:channel_2,0.03,True\nrefresh_stability,std_percentage_change_for_each_channel:channel_1,0.02,True\nrefresh_stability,std_percentage_change_for_each_channel:channel_2,0.01,True\nperturbation,percentage_change_for_each_channel:channel_1,0.02,True\nperturbation,percentage_change_for_each_channel:channel_2,0.01,True\n</code></pre>"},{"location":"examples/basic-usage/#performance-examples","title":"Performance Examples","text":""},{"location":"examples/basic-usage/#quick-testing","title":"Quick Testing","text":"<p>For development and testing:</p> <pre><code>mmm-eval \\\n  --input-data-path small_test_data.csv \\\n  --framework pymc-marketing \\\n  --config-path test_config.json \\\n  --output-path ./test_results/ \\\n  --test-names accuracy\n</code></pre> <p>Use minimal sampling parameters in your config: <pre><code>{\n  \"fit_config\": {\n    \"draws\": 50,\n    \"tune\": 25,\n    \"chains\": 1\n  }\n}\n</code></pre></p>"},{"location":"examples/basic-usage/#production-evaluation","title":"Production Evaluation","text":"<p>For production use:</p> <pre><code>mmm-eval \\\n  --input-data-path production_data.csv \\\n  --framework pymc-marketing \\\n  --config-path production_config.json \\\n  --output-path ./production_results/ \\\n  --test-names accuracy cross_validation refresh_stability perturbation \\\n  --verbose\n</code></pre> <p>Use robust sampling parameters: <pre><code>{\n  \"fit_config\": {\n    \"draws\": 2000,\n    \"tune\": 1000,\n    \"chains\": 4,\n    \"target_accept\": 0.95\n  }\n}\n</code></pre></p>"},{"location":"examples/basic-usage/#troubleshooting-examples","title":"Troubleshooting Examples","text":""},{"location":"examples/basic-usage/#missing-configuration-file","title":"Missing Configuration File","text":"<p>If you get a configuration error:</p> <pre><code># Create a basic config file\ncat &gt; config.json &lt;&lt; 'EOF'\n{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"channel_1\", \"channel_2\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\"\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.9,\n    \"draws\": 100,\n    \"tune\": 50,\n    \"chains\": 2,\n    \"random_seed\": 42\n  },\n  \"revenue_column\": \"revenue\"\n}\nEOF\n\n# Run evaluation\nmmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path ./results/\n</code></pre>"},{"location":"examples/basic-usage/#data-format-issues","title":"Data Format Issues","text":"<p>If you get data format errors, check your CSV structure:</p> <pre><code># Check your data format\nhead -5 data.csv\n\n# Ensure required columns exist\npython -c \"\nimport pandas as pd\ndf = pd.read_csv('data.csv')\nprint('Columns:', df.columns.tolist())\nprint('Shape:', df.shape)\nprint('Date range:', df['date_week'].min(), 'to', df['date_week'].max())\n\"\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data for different data structures</li> <li>Explore Configuration for advanced settings</li> <li>Check the CLI Reference for all available options </li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>mmm-eval uses framework-specific configurations to control model parameters, fitting settings, and data mappings. This guide explains how to create and use configurations for the PyMC-Marketing framework.</p>"},{"location":"getting-started/configuration/#creating-configurations","title":"Creating Configurations","text":"<p>There are two ways to create a configuration (config):</p> <ol> <li>From a model object (preferred).</li> </ol> <p><pre><code>from pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\nfrom mmm_eval.configs import PyMCConfig\n\n# Create a PyMC model\nmodel = MMM(\n    date_column=\"date_week\",\n    channel_columns=[\"channel_1\", \"channel_2\"],\n    adstock=GeometricAdstock(l_max=4),\n    saturation=LogisticSaturation(),\n    yearly_seasonality=2\n)\n\n# Create configuration\nconfig = PyMCConfig.from_model_object(\n    model_object=model,\n    fit_kwargs={\"target_accept\": 0.9, \"draws\": 100, \"chains\": 2},\n    revenue_column=\"revenue\",\n    response_column=\"quantity\"\n)\n\n# Save to JSON if you want\nconfig.save_model_object_to_json(\"./\", \"my_config\")\n</code></pre> 2. Manually, in a JSON file. <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"channel_1\", \"channel_2\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\",\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.9,\n    \"chains\": 2,\n  },\n  \"revenue_column\": \"revenue\",\n}\n</code></pre></p> <p>If you want to run mmm-eval from the CLI, you will need to save the config saved to a JSON file. We recommend Option 1 above to avoid any errors related to improper stringifiation of the model object in the manual approach.</p>"},{"location":"getting-started/configuration/#using-a-configuration","title":"Using a Configuration","text":"<p>If you have the config created from the model object, you can pass that directly to the evaluation suite (see Quick Start).</p> <p>Alternately, if you have the config saved to a JSON, you can pass the filepath via the CLI.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json --output-path results/\n</code></pre> <p>If you have a saved config and you're in a notebook, you can load the config from the path, then run the evaluation. <pre><code>new_config = PyMCConfig.load_model_config_from_json(\"path/to/config.json\")\nresults = run_evaluation(new_config, ...)\n</code></pre></p>"},{"location":"getting-started/configuration/#pymc-marketing-configuration-structure","title":"PyMC-Marketing Configuration Structure","text":"<p>A PyMC-Marketing configuration file has the following structure:</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"channel_1\", \"channel_2\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\",\n  },\n  \"fit_config\": {\n    // Optional \n  },\n  \"revenue_column\": \"revenue\",\n}\n</code></pre>"},{"location":"getting-started/configuration/#model-configuration-pymc_model_config","title":"Model Configuration (pymc_model_config)","text":"<p>The <code>pymc_model_config</code> section defines the PyMC model structure and parameters:</p>"},{"location":"getting-started/configuration/#required-fields","title":"Required Fields","text":""},{"location":"getting-started/configuration/#date_column","title":"date_column","text":"<p>The column name containing the date/time variable.</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#channel_columns","title":"channel_columns","text":"<p>List of column names for marketing channels (media spend).</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#adstock","title":"adstock","text":"<p>The adstock transformation to apply to media channels. Must be a valid member of the AdstockTransformation class.</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"adstock\": \"GeometricAdstock(l_max=4)\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#saturation","title":"saturation","text":"<p>The saturation transformation to apply to media channels. Must be valid member of the SaturationTransformation class.</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"saturation\": \"LogisticSaturation()\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#optional-fields","title":"Optional Fields","text":"<p>The set of optional inputs matches the optional inputs to the MMM class in PyMC.</p>"},{"location":"getting-started/configuration/#fit-configuration-fit_config","title":"Fit Configuration (fit_config)","text":"<p>The <code>fit_config</code> section defines the MCMC sampling parameters. These parameters will be passed to <code>.fit()</code>. Note, we do not require the <code>X</code> and <code>y</code> inputs as we derive those from the data you provide. Therefore, all parameters in this config are optional.</p>"},{"location":"getting-started/configuration/#sampling-parameters","title":"Sampling Parameters","text":""},{"location":"getting-started/configuration/#draws","title":"draws","text":"<p>Number of posterior samples to draw.</p> <pre><code>{\n  \"fit_config\": {\n    \"draws\": 100\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#tune","title":"tune","text":"<p>Number of tuning (warm-up) steps.</p> <pre><code>{\n  \"fit_config\": {\n    \"tune\": 50\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#chains","title":"chains","text":"<p>Number of MCMC chains to run.</p> <pre><code>{\n  \"fit_config\": {\n    \"chains\": 2\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#target_accept","title":"target_accept","text":"<p>Target acceptance rate for the sampler (0.0 to 1.0).</p> <pre><code>{\n  \"fit_config\": {\n    \"target_accept\": 0.9\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#random_seed","title":"random_seed","text":"<p>Random seed for reproducibility.</p> <pre><code>{\n  \"fit_config\": {\n    \"random_seed\": 42\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#progress_bar","title":"progress_bar","text":"<p>Whether to display the progress bar (default: false).</p> <pre><code>{\n  \"fit_config\": {\n    \"progress_bar\": false\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#return_inferencedata","title":"return_inferencedata","text":"<p>Whether to return arviz.InferenceData (default: false).</p> <pre><code>{\n  \"fit_config\": {\n    \"return_inferencedata\": true\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#data-mapping-configuration","title":"Data Mapping Configuration","text":""},{"location":"getting-started/configuration/#revenue_column","title":"revenue_column","text":"<p>The column name containing revenue data for ROI calculations.</p> <pre><code>{\n  \"revenue_column\": \"revenue\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#response_column","title":"response_column","text":"<p>The column name containing the target variable (optional, defaults to first non-date/non-channel column).</p> <pre><code>{\n  \"response_column\": \"quantity\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#example-configurations","title":"Example Configurations","text":""},{"location":"getting-started/configuration/#minimal-configuration","title":"Minimal Configuration","text":"<pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date\",\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\"\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.9,\n    \"draws\": 100,\n    \"tune\": 50,\n    \"chains\": 2,\n    \"random_seed\": 42\n  },\n  \"revenue_column\": \"revenue\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\", \"radio_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\", \"holiday\", \"competitor_promo\"],\n    \"adstock\": \"WeibullAdstock(l_max=6)\",\n    \"saturation\": \"HillSaturation()\",\n    \"yearly_seasonality\": 4,\n    \"time_varying_intercept\": true,\n    \"time_varying_media\": false\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.95,\n    \"draws\": 2000,\n    \"tune\": 1000,\n    \"chains\": 4,\n    \"random_seed\": 123,\n    \"progress_bar\": true,\n    \"return_inferencedata\": true\n  },\n  \"revenue_column\": \"revenue\",\n  \"response_column\": \"sales\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#configuration-best-practices","title":"Configuration Best Practices","text":"<p>It is recommended to create the config programmatically and then save to JSON with our built-in methods. This ensures proper stringification of the PyMC model objects (eg. adstock and saturation functions), reducing the risk of errors when loading the config and fitting models within the evaluation suite. Example:</p> <pre><code>from pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\nfrom mmm_eval.configs import PyMCConfig\n\n# Create a PyMC model\nmodel = MMM(\n    date_column=\"date_week\",\n    channel_columns=[\"channel_1\", \"channel_2\"],\n    adstock=GeometricAdstock(l_max=4),\n    saturation=LogisticSaturation(),\n    yearly_seasonality=2\n)\n\n# Create configuration\nconfig = PyMCConfig.from_model_object(\n    model_object=model,\n    fit_kwargs={\"target_accept\": 0.9, \"draws\": 100, \"chains\": 2},\n    revenue_column=\"revenue\",\n    response_column=\"quantity\"\n)\n\n# Save to JSON\nconfig.save_model_object_to_json(\"./\", \"my_config\")\n</code></pre>"},{"location":"getting-started/configuration/#sampling-parameters_1","title":"Sampling Parameters","text":"<ul> <li>More draws: Use 1000+ draws for production models</li> <li>Multiple chains: Use 2-4 chains for reliable convergence</li> <li>Adequate tuning: Set <code>tune</code> to 50-100% of draws for complex models</li> <li>Acceptance rate: Target 0.9-0.95 for optimal sampling efficiency</li> </ul>"},{"location":"getting-started/configuration/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Data size: Larger datasets require more sampling iterations</li> <li>Model complexity: More parameters increase computation time</li> <li>Hardware: More CPU cores can speed up multi-chain sampling</li> </ul>"},{"location":"getting-started/configuration/#configuration-validation","title":"Configuration Validation","text":"<p>mmm-eval validates your configuration file and will raise errors for:</p> <ul> <li>Missing required fields</li> <li>Invalid field types</li> <li>Unsupported adstock or saturation functions</li> <li>Invalid parameter ranges</li> </ul>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data for different data structures</li> <li>Explore Examples for configuration use cases</li> <li>Check the CLI Reference for command-line options </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install mmm-eval on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>mmm-eval requires Python 3.11 or higher. Make sure you have Python installed on your system.</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<p>The recommended way to install mmm-eval is using Poetry:</p> <pre><code># Install from GitHub\npoetry add git+https://github.com/Mutiny-Group/mmm-eval.git\n\n# Or clone and install locally\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npoetry install\n</code></pre> <p>Prerequisite: Poetry 2.x.x or later is required.</p>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>If you prefer using pip directly:</p> <pre><code># Install from GitHub\npip install git+https://github.com/Mutiny-Group/mmm-eval.git\n\n# Or clone and install locally\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>To install from the latest development version:</p> <pre><code>git clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development work, install with all development dependencies:</p> <pre><code>git clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npoetry install\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>mmm-eval has the following key dependencies:</p> <ul> <li>numpy (&gt;=1.17) - Numerical computing</li> <li>pandas (^2.0.0) - Data manipulation</li> <li>google-meridian (^1.1.0) - Google's Meridian MMM framework</li> <li>pymc-marketing (^0.14.0) - PyMC-based MMM framework</li> <li>scipy (&gt;=1.13.1,&lt;2.0.0) - Scientific computing</li> <li>pytensor (^2.18.0) - Tensor operations</li> <li>pandera (^0.24.0) - Data validation</li> <li>pydantic (^2.5) - Data validation and settings</li> <li>click (^8.1.7) - Command line interface</li> <li>pyarrow (^20.0.0) - Fast data interchange</li> </ul>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify that mmm-eval is working correctly:</p> <pre><code># Check if mmm-eval is installed\npython -c \"import mmm_eval; print(mmm_eval.__version__)\"\n\n# Check CLI availability\nmmm-eval --help\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Import Error: If you get import errors, make sure you're using Python 3.11 or higher.</p> </li> <li> <p>Poetry Installation Issues: If you encounter issues with Poetry:    <pre><code># Update Poetry to the latest version\npoetry self update\n\n# Clear Poetry cache\npoetry cache clear --all pypi\n</code></pre></p> </li> <li> <p>Permission Errors: On some systems, you might need to use <code>pip install --user</code> to install without admin privileges.</p> </li> <li> <p>Version Conflicts: If you encounter dependency conflicts:    <pre><code># With Poetry\npoetry update\n\n# With pip (in a virtual environment)\npython -m venv mmm-eval-env\nsource mmm-eval-env/bin/activate  # On Windows: mmm-eval-env\\Scripts\\activate\npip install git+https://github.com/Mutiny-Group/mmm-eval.git\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter any issues during installation, please:</p> <ol> <li>Check the GitHub Issues for known problems</li> <li>Create a new issue with details about your system and the error message</li> <li>Join our Discussions for community support</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have mmm-eval installed, check out the Quick Start guide to begin using it. </p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you get started with mmm-eval quickly by walking through an example notebook. You'll learn the evaluation workflow and how to interpret the results. To see how to run mmm-eval from the command line, check out CLI.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ol> <li>mmm-eval installed - See Installation if you haven't installed it yet</li> <li>Your MMM data - A CSV or Parquet file with your marketing mix model data</li> <li>A supported framework - Currently PyMC-Marketing is supported</li> </ol>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quick-start/#prepare-your-data","title":"Prepare Your Data","text":"<p>Your data should contain the following columns (see Data Requirements for more)</p> <ul> <li>Date/time period</li> <li>Revenue variable (for calculating ROI)</li> <li>Marketing spend $ by channel (e.g., TV, digital, print)</li> <li><code>OPTIONAL</code> Response variable (e.g., sales, conversions, units)</li> <li><code>OPTIONAL</code> Control variables (e.g., price, seasonality)</li> </ul> <p>Example data structure:</p> <pre><code>date_week,quantity,revenue,TV,radio,price,event_1,event_2\n2023-01-01,1000,7000,5000,2000,10.99,0,0\n2023-01-08,1200,8000,5500,2200,10.99,0,0\n2023-01-15,1100,7500,5200,2100,11.99,1,0\n2023-01-22,1300,9000,6000,2400,11.99,0,1\n2023-01-29,1400,9500,6500,2600,12.99,0,0\n</code></pre>"},{"location":"getting-started/quick-start/#evaluating-your-pymc-mmm-follow-along-in-mmm-evalexamplespymc_evalipynb","title":"Evaluating your PyMC MMM (follow along in <code>mmm-eval/examples/pymc_eval.ipynb</code>)","text":"<p>First, load your data <pre><code>data = pd.read_csv(\"data/example_data.csv\")\n</code></pre></p> <p>and fit a PyMC-Marketing MMM <pre><code>X = data.drop(columns=[\"revenue\",\"quantity\"])\ny = data[\"quantity\"]\n\nmodel = MMM(\n    date_column=\"date_week\" ,\n    channel_columns=[\"TV\",\"radio\"],\n    adstock=GeometricAdstock(l_max=4),\n    saturation=LogisticSaturation()\n)\n\nmodel.fit(X=X, y=y, chains=4, target_accept=0.85)\n</code></pre></p>"},{"location":"getting-started/quick-start/#now-we-evaluate-just-create-a-config","title":"Now we evaluate! Just create a config","text":"<pre><code>fit_kwargs = { \n    \"chains\": 4,\n    \"target_accept\": 0.85,\n}\n\nconfig = PyMCConfig.from_model_object(base_model, fit_kwargs=fit_kwargs, response_column=\"quantity\", revenue_column=\"revenue\")\n\n# Save this for later if you want to run from CLI!\nconfig.save_model_object_to_json(save_path=\"data/\", file_name=\"saved_config\")\n</code></pre> <p>And we can run the evaluation suite, which returns a dataframe. <pre><code>result = run_evaluation(framework=\"pymc-marketing\", config=config, data=data)\n</code></pre></p>"},{"location":"getting-started/quick-start/#done","title":"\u2728 Done \u2728","text":""},{"location":"getting-started/quick-start/#whats-in-result","title":"What's in <code>result</code>?","text":"<p>The evaluation suite runs 4 tests, each of which answers a distinct question about the quality of your model: </p> <ul> <li>Accuracy Test: \"How well does my model predict on unseen data?\"</li> <li>Cross-Validation: \"How consistent are my model's predictions across different splits of unseen data?\"</li> <li>Refresh Stability: \"How much does marketing attribution change when I add new data to my model?\"</li> <li>Perturbation: \"How sensitive is my model is to noise in the marketing inputs?\"</li> </ul> <p>Details on the implementation of the tests can be found in Tests. For each test, we compute multiple metrics to give as much insight into the test result as possible. These can be viewed in detail in Metrics. For example:</p> <ul> <li> <p>MAPE (Mean Absolute Percentage Error) <code>MAPE = (100 / n) * \u03a3 |(y_i - \u0177_i) / y_i|</code></p> </li> <li> <p>R-squared (Coefficient of Determination) <code>R\u00b2 = 1 - (\u03a3 (y_i - \u0177_i)^2) / (\u03a3 (y_i - \u0233)^2)</code></p> </li> </ul> <p>If we look at the evaluation output <code>display(results)</code>, we see the following table:</p> test_name metric_name metric_value metric_pass accuracy mape 0.121 False accuracy r_squared -0.547 False cross_validation mean_mape 0.084 False cross_validation std_mape 0.058 False cross_validation mean_r_squared -7.141 False cross_validation std_r_squared 9.686 False refresh_stability mean_percentage_change_for_each_channel:TV 0.021 False refresh_stability mean_percentage_change_for_each_channel:radio 0.369 False refresh_stability std_percentage_change_for_each_channel:TV 0.021 False refresh_stability std_percentage_change_for_each_channel:radio 0.397 False perturbation percentage_change_for_each_channel:TV 0.005 False perturbation percentage_change_for_each_channel:radio 0.112 False <p>Notice that our model is failing every test. Seems we have some work to do!</p>"},{"location":"getting-started/quick-start/#changing-the-thresholds","title":"Changing the Thresholds","text":"<p>Default metric thresholds in <code>mmm_eval/metrics/threshold_constants.py</code> can be overwritten in-place to change the pass/fail cutoff for each metric.</p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first evaluation:</p> <ol> <li>Explore the User Guide for detailed CLI options</li> <li>Check out Examples for more complex scenarios</li> <li>Learn about Data for different data structures</li> <li>Review Configuration for advanced settings</li> </ol>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ul> <li>Check the CLI Reference for all available options</li> <li>Look at Examples for similar use cases</li> <li>Join our Discussions for community support</li> </ul>"},{"location":"user-guide/cli/","title":"Command Line Interface","text":"<p>mmm-eval provides a command-line interface (CLI) for running MMM evaluations. This guide covers all available options and usage patterns.</p>"},{"location":"user-guide/cli/#basic-usage","title":"Basic Usage","text":"<p>The basic command structure is:</p> <pre><code>mmm-eval [OPTIONS] --input-data-path PATH --framework FRAMEWORK --config-path PATH --output-path PATH\n</code></pre>"},{"location":"user-guide/cli/#required-arguments","title":"Required Arguments","text":""},{"location":"user-guide/cli/#-input-data-path","title":"--input-data-path","text":"<p>Path to your input data file (CSV or Parquet format).</p>"},{"location":"user-guide/cli/#-framework","title":"--framework","text":"<p>The MMM framework to use for evaluation.</p>"},{"location":"user-guide/cli/#-config-path","title":"--config-path","text":"<p>Path to a framework-specific JSON configuration file.</p>"},{"location":"user-guide/cli/#-output-path","title":"--output-path","text":"<p>Directory to save evaluation results.</p>"},{"location":"user-guide/cli/#optional-arguments","title":"Optional Arguments","text":""},{"location":"user-guide/cli/#-test-names","title":"--test-names","text":"<p>Specify which validation tests to run. Can specify multiple tests by repeating the flag.</p> <pre><code># Run all tests (default)\nmmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json --output-path results/\n\n# Run two tests\nmmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json --output-path results/ --test-names accuracy cross_validation\n\n# Available tests: accuracy, cross_validation, refresh_stability, perturbation\n</code></pre>"},{"location":"user-guide/cli/#-verbose","title":"--verbose","text":"<p>Enable verbose logging for detailed output and error info.</p>"},{"location":"user-guide/cli/#complete-example","title":"Complete Example","text":"<p>Here's a complete example with all options:</p> <pre><code>mmm-eval \\\n  --input-data-path marketing_data.csv \\\n  --framework pymc-marketing \\\n  --config-path evaluation_config.json \\\n  --test-names accuracy cross_validation refresh_stability perturbation \\\n  --output-path ./evaluation_results/ \\\n  --verbose\n</code></pre>"},{"location":"user-guide/cli/#available-tests","title":"Available Tests","text":"<p>See Tests</p>"},{"location":"user-guide/cli/#framework-support","title":"Framework Support","text":"<p>See Frameworks</p>"},{"location":"user-guide/cli/#output-files","title":"Output Files","text":"<p>After running an evaluation, you'll find the following files in your output directory:</p> <ul> <li><code>mmm_eval_{framework}_{timestamp}.csv</code> - Detailed test results in CSV format</li> </ul>"},{"location":"user-guide/cli/#results-file-structure","title":"Results File Structure","text":"<p>The results CSV file contains the following columns:</p> <ul> <li><code>test_name</code> - Name of the validation test</li> <li><code>metric_name</code> - Name of the metric calculated</li> <li><code>metric_value</code> - Value of the metric</li> <li><code>metric_pass</code> - Whether the metric passed its threshold (if applicable)</li> </ul>"},{"location":"user-guide/cli/#example-results","title":"Example Results","text":"<pre><code>test_name,metric_name,metric_value,metric_pass\naccuracy,mape,0.15,True\naccuracy,r_squared,0.85,True\ncross_validation,mape,0.18,True\ncross_validation,r_squared,0.82,True\nrefresh_stability,mean_percentage_change_for_each_channel:channel_1,0.05,True\nrefresh_stability,mean_percentage_change_for_each_channel:channel_2,0.03,True\nperturbation,percentage_change_for_each_channel:channel_1,0.02,True\nperturbation,percentage_change_for_each_channel:channel_2,0.01,True\n</code></pre>"},{"location":"user-guide/cli/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/cli/#common-errors","title":"Common Errors","text":"<ol> <li>File not found: Ensure the input data file and config file exist and paths are correct</li> <li>Invalid configuration: Check that your JSON config file follows the required format</li> <li>Missing columns: Ensure your data contains all columns specified in the configuration</li> <li>Framework errors: Check that all required dependencies are installed</li> </ol>"},{"location":"user-guide/cli/#help-and-information","title":"Help and Information","text":""},{"location":"user-guide/cli/#-help","title":"--help","text":"<p>Display all available options and their descriptions:</p> <pre><code>mmm-eval --help\n</code></pre>"},{"location":"user-guide/cli/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ul> <li>Look at at Troubleshooting for common problems and their solutions</li> <li>Check the Configuration Guide for config file format</li> <li>Review the Data Guide for data requirements</li> <li>See Examples for similar use cases</li> <li>Join our Discussions for community support</li> </ul>"},{"location":"user-guide/cli/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data for different data structures</li> <li>Explore Examples for practical use cases</li> <li>Check the Configuration Guide for advanced settings </li> </ul>"},{"location":"user-guide/data/","title":"Data","text":"<p>mmm-eval expects your data to be in a specific format. This guide explains the required structure and provides examples for preparing your marketing mix modeling data.</p>"},{"location":"user-guide/data/#data-requirements","title":"Data Requirements","text":""},{"location":"user-guide/data/#quantity-requirements","title":"Quantity Requirements","text":"<ul> <li>Observations: At least 40 data points required, but 100+ is highly recommended</li> <li>Frequency: Consistent frequency with no gaps (weekly is recommended)</li> <li>Media channels: At least 2-3 channels for meaningful analysis</li> <li>Revenue data: Required for ROI calculations</li> </ul>"},{"location":"user-guide/data/#quality-requirements","title":"Quality Requirements","text":""},{"location":"user-guide/data/#completeness","title":"Completeness","text":"<ul> <li>No missing values in required columns</li> <li>Complete time series (no gaps in dates)</li> <li>Non-negative values for spend columns</li> </ul>"},{"location":"user-guide/data/#consistency","title":"Consistency","text":"<ul> <li>Same units throughout (e.g., all spend in same currency)</li> <li>Consistent date format</li> <li>Consistent naming conventions</li> </ul>"},{"location":"user-guide/data/#reasonableness","title":"Reasonableness","text":"<ul> <li>Values within expected ranges</li> <li>No obvious outliers or errors</li> <li>Logical relationships between variables</li> </ul>"},{"location":"user-guide/data/#required-data-format","title":"Required Data Format","text":"<p>mmm-eval accepts CSV and Parquet files with the following structure:</p>"},{"location":"user-guide/data/#required-columns","title":"Required Columns","text":"<ul> <li>Date column: Time series data with consistent date format</li> <li>Target column: The variable you want to predict (e.g., sales, conversions)</li> <li>Revenue column: Revenue data for calculating ROI and efficiency metrics</li> <li>Media columns: Marketing channel spend or activity data</li> <li>Control columns (optional): Additional variables that may affect the target</li> </ul>"},{"location":"user-guide/data/#column-types","title":"Column Types","text":""},{"location":"user-guide/data/#date-column","title":"Date Column","text":"<ul> <li>Purpose: Identifies the time period for each observation</li> <li>Format: Date in a consistent format (e.g., YYYY-MM-DD, MM/DD/YYYY)</li> <li>Requirements: </li> <li>Must be in chronological order</li> <li>No missing dates in the series</li> <li>Consistent format throughout</li> </ul>"},{"location":"user-guide/data/#target-column","title":"Target Column","text":"<ul> <li>Purpose: The dependent variable you want to model (e.g., sales, conversions)</li> <li>Format: Numeric values</li> <li>Requirements:</li> <li>No missing values</li> <li>Positive values (for most use cases)</li> <li>Reasonable scale for your business</li> </ul>"},{"location":"user-guide/data/#revenue-column","title":"Revenue Column","text":"<ul> <li>Purpose: Revenue data for calculating ROI and efficiency metrics</li> <li>Format: Numeric values</li> <li>Requirements:</li> <li>No missing values</li> <li>Positive values</li> <li>Same time period as target column</li> <li>Used for ROI calculations and efficiency analysis</li> </ul>"},{"location":"user-guide/data/#media-columns","title":"Media Columns","text":"<ul> <li>Purpose: Marketing channel spend or activity data</li> <li>Format: Numeric values</li> <li>Requirements:</li> <li>No missing values (use 0 for periods with no spend)</li> <li>Non-negative values</li> <li>Consistent units (e.g., all in dollars, all in thousands)</li> </ul>"},{"location":"user-guide/data/#control-columns","title":"Control Columns","text":"<ul> <li>Purpose: Additional variables that may affect the target</li> <li>Format: Numeric or categorical values</li> <li>Examples: Price, seasonality indicators, holiday flags, competitor activity</li> </ul>"},{"location":"user-guide/data/#example-data-structure","title":"Example Data Structure","text":""},{"location":"user-guide/data/#basic-example","title":"Basic Example","text":"<pre><code>date,sales,revenue,tv_spend,digital_spend,print_spend\n2023-01-01,1000,7000,5000,2000,1000\n2023-01-02,1200,8000,5500,2200,1100\n2023-01-03,1100,7500,5200,2100,1050\n2023-01-04,1300,9000,6000,2400,1200\n2023-01-05,1400,9500,6500,2600,1300\n</code></pre>"},{"location":"user-guide/data/#advanced-example-with-controls","title":"Advanced Example with Controls","text":"<pre><code>date,sales,revenue,tv_spend,digital_spend,print_spend,price,seasonality,holiday\n2023-01-01,1000,7000,5000,2000,1000,10.99,0.8,0\n2023-01-02,1200,8000,5500,2200,1100,10.99,0.9,0\n2023-01-03,1100,7500,5200,2100,1050,11.99,0.7,1\n2023-01-04,1300,9000,6000,2400,1200,11.99,0.8,0\n2023-01-05,1400,9500,6500,2600,1300,12.99,0.9,0\n</code></pre>"},{"location":"user-guide/data/#real-world-example","title":"Real-World Example","text":"<pre><code>date,sales,revenue,tv_spend,digital_spend,social_spend,search_spend,email_spend,price,holiday_flag,competitor_promo\n2023-01-01,1250,8750,15000,8000,3000,5000,2000,12.99,0,0\n2023-01-02,1320,9240,16000,8500,3200,5200,2100,12.99,0,0\n2023-01-03,1180,8260,14000,7500,2800,4800,1900,13.99,1,1\n2023-01-04,1450,10150,18000,9500,3800,6000,2400,13.99,0,0\n2023-01-05,1520,10640,19000,10000,4000,6300,2500,14.99,0,0\n</code></pre>"},{"location":"user-guide/data/#configuration-file","title":"Configuration File","text":"<p>mmm-eval uses a configuration file to specify data column mappings and settings:</p>"},{"location":"user-guide/data/#basic-configuration","title":"Basic Configuration","text":"<pre><code>{\n  \"data\": {\n    \"date_column\": \"date\",\n    \"target_column\": \"sales\",\n    \"revenue_column\": \"revenue\",\n    \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\", \"holiday\"]\n  }\n}\n</code></pre>"},{"location":"user-guide/data/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>{\n  \"data\": {\n    \"date_column\": \"date\",\n    \"date_format\": \"%Y-%m-%d\",\n    \"target_column\": \"sales\",\n    \"revenue_column\": \"revenue\",\n    \"media_columns\": [\"tv_spend\", \"digital_spend\", \"social_spend\", \"search_spend\", \"email_spend\"],\n    \"control_columns\": [\"price\", \"holiday_flag\", \"competitor_promo\"],\n    \"validation\": {\n      \"check_missing_values\": true,\n      \"check_negative_values\": true,\n      \"check_date_range\": true,\n      \"min_date\": \"2020-01-01\",\n      \"max_date\": \"2023-12-31\"\n    }\n  },\n  \"tests\": {\n    \"accuracy\": {\n      \"train_test_split\": 0.8\n    },\n    \"cross_validation\": {\n      \"folds\": 5\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/data/#date-formats","title":"Date Formats","text":""},{"location":"user-guide/data/#supported-formats","title":"Supported Formats","text":"<p>mmm-eval supports various date formats:</p> <ul> <li><code>YYYY-MM-DD</code> (ISO format) - Recommended</li> <li><code>MM/DD/YYYY</code></li> <li><code>DD-MM-YYYY</code></li> <li><code>YYYY/MM/DD</code></li> </ul>"},{"location":"user-guide/data/#date-format-specification","title":"Date Format Specification","text":"<p>If your dates aren't in ISO format, specify the format in your configuration:</p> <pre><code>{\n  \"data\": {\n    \"date_format\": \"%m/%d/%Y\"\n  }\n}\n</code></pre>"},{"location":"user-guide/data/#data-validation","title":"Data Validation","text":"<p>mmm-eval performs several validation checks:</p>"},{"location":"user-guide/data/#automatic-validation","title":"Automatic Validation","text":"<ol> <li>Missing values: Checks for missing data in required columns</li> <li>Date consistency: Ensures dates are in chronological order</li> <li>Data types: Verifies numeric columns contain valid numbers</li> <li>Value ranges: Checks for negative values in spend columns</li> <li>Revenue consistency: Ensures revenue data is available and positive</li> </ol>"},{"location":"user-guide/data/#custom-validation","title":"Custom Validation","text":"<p>You can configure additional validation in your config file:</p> <pre><code>{\n  \"data\": {\n    \"validation\": {\n      \"check_missing_values\": true,\n      \"check_negative_values\": true,\n      \"check_date_range\": true,\n      \"min_date\": \"2020-01-01\",\n      \"max_date\": \"2023-12-31\",\n      \"min_observations\": 100,\n      \"required_columns\": [\"date\", \"sales\", \"revenue\", \"tv_spend\"]\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/data/#data-preparation-tips","title":"Data Preparation Tips","text":""},{"location":"user-guide/data/#before-running-mmm-eval","title":"Before Running mmm-eval","text":"<ol> <li>Clean your data:</li> <li>Remove any test or dummy data</li> <li>Handle missing values appropriately</li> <li> <p>Check for and remove outliers if necessary</p> </li> <li> <p>Standardize formats:</p> </li> <li>Ensure consistent date format</li> <li>Use consistent units (e.g., thousands of dollars)</li> <li> <p>Standardize column names</p> </li> <li> <p>Validate relationships:</p> </li> <li>Check that spend and sales have logical relationships</li> <li>Verify that revenue data is consistent with sales</li> <li>Ensure control variables make sense</li> </ol>"},{"location":"user-guide/data/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/data/#missing-values","title":"Missing Values","text":"<pre><code># Problem: Missing values in media columns\ndate,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,\n2023-01-02,1200,8000,,2000\n\n# Solution: Fill with zeros or appropriate values\ndate,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,0\n2023-01-02,1200,8000,0,2000\n</code></pre>"},{"location":"user-guide/data/#inconsistent-date-formats","title":"Inconsistent Date Formats","text":"<pre><code># Problem: Mixed date formats\ndate,sales,revenue\n01/01/2023,1000,7000\n2023-01-02,1200,8000\n\n# Solution: Standardize to one format\ndate,sales,revenue\n2023-01-01,1000,7000\n2023-01-02,1200,8000\n</code></pre>"},{"location":"user-guide/data/#missing-revenue-data","title":"Missing Revenue Data","text":"<pre><code># Problem: No revenue column\ndate,sales,tv_spend,digital_spend\n2023-01-01,1000,5000,2000\n2023-01-02,1200,5500,2200\n\n# Solution: Add revenue column\ndate,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,2000\n2023-01-02,1200,8000,5500,2200\n</code></pre>"},{"location":"user-guide/data/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/data/#data-collection","title":"Data Collection","text":"<ul> <li>Consistent timing: Collect data at the same time each period</li> <li>Complete coverage: Ensure all channels are captured</li> <li>Quality control: Implement data validation at source</li> <li>Documentation: Keep records of any data changes or anomalies</li> </ul>"},{"location":"user-guide/data/#data-storage","title":"Data Storage","text":"<ul> <li>Backup regularly: Keep multiple copies of your data</li> <li>Version control: Track changes to your datasets</li> <li>Metadata: Document data sources, definitions, and assumptions</li> <li>Security: Protect sensitive business data</li> </ul>"},{"location":"user-guide/data/#data-analysis","title":"Data Analysis","text":"<ul> <li>Start simple: Begin with basic models before adding complexity</li> <li>Validate assumptions: Check that your data meets model requirements</li> <li>Monitor quality: Regularly review data for issues</li> <li>Document decisions: Keep records of data preparation choices</li> </ul>"},{"location":"user-guide/data/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/data/#common-error-messages","title":"Common Error Messages","text":"<ul> <li>\"Missing required column\": Ensure all required columns are present</li> <li>\"Invalid date format\": Check your date format specification</li> <li>\"Negative values in spend columns\": Replace negative values with zeros</li> <li>\"Missing revenue data\": Add revenue column to your dataset</li> </ul>"},{"location":"user-guide/data/#getting-help","title":"Getting Help","text":"<p>If you encounter data format issues:</p> <ul> <li>Check the CLI Reference for all available options</li> <li>Review the Examples for similar use cases</li> <li>Join our Discussions for community support </li> </ul>"},{"location":"user-guide/frameworks/","title":"Frameworks","text":"<p>mmm-eval supports multiple Marketing Mix Modeling (MMM) frameworks. This guide explains each supported framework and their features.</p>"},{"location":"user-guide/frameworks/#supported-frameworks","title":"Supported Frameworks","text":""},{"location":"user-guide/frameworks/#pymc-marketing","title":"PyMC-Marketing","text":"<p>Status: \u2705 Fully Supported</p> <p>PyMC-Marketing is a Bayesian MMM framework built on PyMC that provides robust statistical modeling capabilities.</p>"},{"location":"user-guide/frameworks/#features","title":"Features","text":"<ul> <li>Bayesian inference: Probabilistic modeling with uncertainty quantification</li> <li>Flexible modeling: Customizable model structures</li> <li>Seasonality handling: Built-in seasonal components</li> <li>Media effects: Configurable adstock and saturation functions</li> <li>Control variables: Support for external factors</li> </ul>"},{"location":"user-guide/frameworks/#cli-usage","title":"CLI Usage","text":"<pre><code>mmm-eval --input-data-path data.csv --config-path config.json --framework pymc-marketing --output-path results/\n</code></pre>"},{"location":"user-guide/frameworks/#advantages","title":"Advantages","text":"<ul> <li>Statistical rigor: Bayesian approach provides uncertainty estimates</li> <li>Flexibility: Highly customizable model structures</li> <li>Interpretability: Clear parameter interpretation</li> <li>Robustness: Handles various data scenarios well</li> </ul>"},{"location":"user-guide/frameworks/#limitations","title":"Limitations","text":"<ul> <li>Computational cost: Can be slower than simpler approaches</li> <li>Complexity: Requires more expertise to configure optimally</li> <li>Data requirements: Needs sufficient data for reliable inference</li> </ul>"},{"location":"user-guide/frameworks/#framework-comparison","title":"Framework Comparison","text":"Feature PyMC-Marketing Google Meridian* Robyn* Type Bayesian Bayesian Frequentist Inference MCMC MCMC Ridge Regression + Bootstrapping Seasonality \u2705 Built-in \u2705 Built-in \u2705 Built-in Saturation \u2705 Configurable \u2705 Configurable \u2705 Configurable Controls \u2705 Supported \u2705 Supported \u2705 Supported Uncertainty \u2705 Full \u2705 Full \u26a0\ufe0f Approximate via bootstrapping Speed Medium Fast Medium Complexity High Medium Medium <p>*Planned for future releases</p>"},{"location":"user-guide/frameworks/#getting-help","title":"Getting Help","text":"<p>For framework-specific issues:</p> <ol> <li>Check documentation: Review framework-specific guides</li> <li>Review examples: Look at example configurations</li> <li>Check data: Ensure data meets requirements</li> <li>Community support: Use GitHub discussions for help</li> </ol>"},{"location":"user-guide/frameworks/#future-frameworks","title":"Future Frameworks","text":""},{"location":"user-guide/frameworks/#planned-support","title":"Planned Support","text":"<p>Google Meridian:</p> <ul> <li> <p>Google's open-source MMM framework</p> </li> <li> <p>Fast Bayesian inference</p> </li> <li> <p>Production-ready implementation</p> </li> </ul> <p>Robyn:</p> <ul> <li> <p>Meta's MMM framework</p> </li> <li> <p>Automated model selection</p> </li> <li> <p>Built-in validation</p> </li> </ul>"},{"location":"user-guide/frameworks/#framework-requests","title":"Framework Requests","text":"<p>To request support for additional frameworks:</p> <ol> <li>Create an issue on GitHub</li> <li>Provide details about the framework</li> <li>Include use cases and requirements</li> <li>Contribute if possible</li> </ol>"},{"location":"user-guide/frameworks/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn about Configuration for framework setup</p> </li> <li> <p>Check Examples for framework usage</p> </li> <li> <p>Review Tests for validation approaches </p> </li> </ul>"},{"location":"user-guide/metrics/","title":"Metrics","text":"<p>Note: To render math equations, enable <code>pymdownx.arithmatex</code> in your <code>mkdocs.yml</code> and include MathJax. See the user guide for details.</p> <p>mmm-eval provides a comprehensive set of metrics to evaluate MMM performance. This guide explains each metric and how to interpret the results.</p>"},{"location":"user-guide/metrics/#overview","title":"Overview","text":"<p>mmm-eval calculates several key metrics across different validation tests:</p> <ul> <li>Accuracy Metrics: How well the model predicts on unseen data</li> <li>Stability Metrics: How consistent the model is over time</li> <li>Robustness Metrics: How sensitive the model is to data changes</li> </ul>"},{"location":"user-guide/metrics/#accuracy-metrics","title":"Accuracy Metrics","text":""},{"location":"user-guide/metrics/#mape-mean-absolute-percentage-error","title":"MAPE (Mean Absolute Percentage Error)","text":"<p>Formula:</p> \\[ \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right| \\] <p>Interpretation:</p> <ul> <li>Lower values indicate better accuracy</li> <li>Expressed as a proportion (0 to 1, instead of 0 to 100)</li> <li>Sensitive to scale of target variable</li> </ul> <p>Example: MAPE of 15% means predictions are off by 15% on average</p>"},{"location":"user-guide/metrics/#r-squared-coefficient-of-determination","title":"R-squared (Coefficient of Determination)","text":"<p>Formula:</p> \\[ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} \\] <p>Interpretation:</p> <ul> <li>Range: 0 to 1 (or 0% to 100%)</li> <li>Higher values indicate better fit</li> <li>Represents proportion of variance explained by the model</li> </ul> <p>Example: R\u00b2 of 0.85 means the model explains 85% of the variance</p>"},{"location":"user-guide/metrics/#stability-metrics","title":"Stability Metrics","text":""},{"location":"user-guide/metrics/#refresh-stability","title":"Refresh Stability","text":"<p>Measures consistency of channel attribution when the model is trained on different time periods.</p> <p>Calculation:</p> <ol> <li>Train model on different proportions of data (e.g., 50%, 75%, 90%)</li> <li>Calculate metrics for each refresh period</li> <li>Measure average and standard deviation in metrics across periods</li> </ol> <p>Interpretation:</p> <ul> <li>Lower variation indicates more stable model</li> <li>High variation suggests model is sensitive to training data</li> </ul>"},{"location":"user-guide/metrics/#robustness-metrics","title":"Robustness Metrics","text":""},{"location":"user-guide/metrics/#perturbation-sensitivity","title":"Perturbation Sensitivity","text":"<p>Measures how sensitive the model is to small changes in the data.</p> <p>Calculation:</p> <ol> <li>Add small random perturbations to input data</li> <li>Retrain model and calculate metrics</li> <li>Measure change in performance</li> </ol> <p>Interpretation:</p> <ul> <li>Lower sensitivity indicates more robust model</li> <li>High sensitivity suggests model may not generalize well</li> </ul>"},{"location":"user-guide/metrics/#metric-comparison","title":"Metric Comparison","text":""},{"location":"user-guide/metrics/#when-to-use-each-metric","title":"When to Use Each Metric","text":"Metric Best For Considerations MAPE Relative accuracy Sensitive to scale, good for comparison RMSE Overall accuracy Penalizes large errors heavily R\u00b2 Model fit quality May be misleading with non-linear relationships MAE Robust accuracy Less sensitive to outliers MSE Mathematical optimization Harder to interpret"},{"location":"user-guide/metrics/#metric-ranges-and-benchmarks","title":"Metric Ranges and Benchmarks","text":""},{"location":"user-guide/metrics/#mape-benchmarks","title":"MAPE Benchmarks","text":"<ul> <li>Excellent: &lt; 10%</li> <li>Good: 10% - 20%</li> <li>Fair: 20% - 30%</li> <li>Poor: &gt; 30%</li> </ul>"},{"location":"user-guide/metrics/#r2-benchmarks","title":"R\u00b2 Benchmarks","text":"<ul> <li>Excellent: &gt; 0.9</li> <li>Good: 0.8 - 0.9</li> <li>Fair: 0.6 - 0.8</li> <li>Poor: &lt; 0.6</li> </ul> <p>Note: These benchmarks are general guidelines. Industry-specific benchmarks may vary.</p>"},{"location":"user-guide/metrics/#interpreting-results","title":"Interpreting Results","text":""},{"location":"user-guide/metrics/#single-model-evaluation","title":"Single Model Evaluation","text":"<p>When evaluating a single model:</p> <ol> <li>Check accuracy metrics: Are predictions reasonably accurate?</li> <li>Assess stability: Is performance consistent across different scenarios?</li> <li>Evaluate robustness: How sensitive is the model to data changes?</li> </ol>"},{"location":"user-guide/metrics/#model-comparison","title":"Model Comparison","text":"<p>When comparing multiple models:</p> <ol> <li>Compare accuracy: Which model has better predictive performance?</li> <li>Compare stability: Which model is more consistent?</li> <li>Compare robustness: Which model is less sensitive to changes?</li> </ol>"},{"location":"user-guide/metrics/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/metrics/#metric-selection","title":"Metric Selection","text":"<ol> <li>Use multiple metrics: Don't rely on a single metric</li> <li>Consider business context: Choose metrics relevant to your goals</li> <li>Account for scale: Use relative metrics for comparison across scales</li> </ol>"},{"location":"user-guide/metrics/#result-interpretation","title":"Result Interpretation","text":"<ol> <li>Set realistic expectations: MMM accuracy varies by industry</li> <li>Consider data quality: Poor data leads to poor metrics</li> <li>Validate assumptions: Ensure metrics align with business needs</li> </ol>"},{"location":"user-guide/metrics/#continuous-monitoring","title":"Continuous Monitoring","text":"<ol> <li>Track metrics over time: Monitor performance degradation</li> <li>Set up alerts: Flag when metrics fall below thresholds</li> <li>Regular re-evaluation: Periodically reassess model performance</li> </ol>"},{"location":"user-guide/metrics/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Tests to understand how metrics are calculated</li> <li>Check Examples for practical metric interpretation</li> <li>Review Configuration for customizing metric calculation </li> </ul>"},{"location":"user-guide/tests/","title":"Tests","text":"<p>mmm-eval provides a comprehensive suite of validation tests to evaluate MMM performance. This guide explains each test and how to interpret the results.</p>"},{"location":"user-guide/tests/#overview","title":"Overview","text":"<p>mmm-eval includes four main types of validation tests:</p> <ol> <li>Accuracy Test - Predictive accuracy on unseen data</li> <li>Cross-Validation Test - Predictive accuracy on k-folds of unseen data</li> <li>Refresh Stability Test - Stability of marketing attribution when additional data is added</li> <li>Perturbation Test - Stability of marketing attribution when noise is added to input data</li> </ol>"},{"location":"user-guide/tests/#accuracy-test","title":"Accuracy Test","text":""},{"location":"user-guide/tests/#purpose","title":"Purpose","text":"<p>The accuracy test evaluates how well the model predicts on unseen data.</p>"},{"location":"user-guide/tests/#methodology","title":"Methodology","text":"<ol> <li>Data Split: Divides data into training and test sets</li> <li>Model Training: Trains the model on the training set</li> <li>Prediction: Makes predictions on the test set</li> <li>Evaluation: Calculates accuracy metrics</li> </ol>"},{"location":"user-guide/tests/#interpretation","title":"Interpretation","text":"<ul> <li>Good accuracy: Low MAPE, high R\u00b2</li> <li>Poor accuracy: High MAPE, low R\u00b2</li> <li>Overfitting: Good training performance, poor test performance</li> </ul>"},{"location":"user-guide/tests/#cross-validation-test","title":"Cross-Validation Test","text":""},{"location":"user-guide/tests/#purpose_1","title":"Purpose","text":"<p>The cross-validation test assesses the consistency of model accuracy across multiple data splits.</p>"},{"location":"user-guide/tests/#methodology_1","title":"Methodology","text":"<ol> <li>K-Fold Split: Divides data into k contiguous folds (to preserve time series structure)</li> <li>Iterative Training: Trains k models, each using k-1 folds</li> <li>Performance Assessment: Evaluates each model on the held-out fold</li> <li>Stability Analysis: Measures variation in accuracy metrics across folds</li> </ol>"},{"location":"user-guide/tests/#interpretation_1","title":"Interpretation","text":"<ul> <li>Stable model: Low standard deviation across folds</li> <li>Unstable model: High standard deviation across folds</li> <li>Overfitting: High variation suggests poor generalization</li> </ul>"},{"location":"user-guide/tests/#refresh-stability-test","title":"Refresh Stability Test","text":""},{"location":"user-guide/tests/#purpose_2","title":"Purpose","text":"<p>The refresh stability test evaluates how stable marketing attribution is when new data is added (refreshed).</p>"},{"location":"user-guide/tests/#methodology_2","title":"Methodology","text":"<ol> <li>Progressive Training: Trains models on increasing proportions of data</li> <li>Performance Tracking: Measures performance at each refresh point</li> <li>Stability Assessment: Analyzes variation in performance over time</li> </ol>"},{"location":"user-guide/tests/#interpretation_2","title":"Interpretation","text":"<ul> <li>Stable model: Consistent attribution across refresh periods</li> <li>Unstable model: Attribution varies significantly over time</li> <li>Improving model: Attribution improves with more data</li> </ul>"},{"location":"user-guide/tests/#perturbation-test","title":"Perturbation Test","text":""},{"location":"user-guide/tests/#purpose_3","title":"Purpose","text":"<p>The perturbation test evaluates how sensitive the model is to small changes in the data.</p>"},{"location":"user-guide/tests/#methodology_3","title":"Methodology","text":"<ol> <li>Train a Model: Train a model on the original data.</li> <li>Data Perturbation: Adds small random noise to marketing input data</li> <li>Model Retraining: Retrains model on perturbed data</li> <li>Performance Comparison: Compares marketing attribution between perturbed and non-perturbed models</li> </ol>"},{"location":"user-guide/tests/#interpretation_3","title":"Interpretation","text":"<ul> <li>Robust model: Performance changes little with perturbations</li> <li>Sensitive model: Performance degrades significantly with perturbations</li> <li>Overfitting: High sensitivity suggests poor generalization</li> </ul>"},{"location":"user-guide/tests/#test-selection","title":"Test Selection","text":""},{"location":"user-guide/tests/#when-to-use-each-test","title":"When to Use Each Test","text":"Test Best For When to Use Accuracy Basic evaluation Initial model assessment Cross-Validation Stability assessment Model comparison Refresh Stability Temporal analysis Long-term planning Perturbation Robustness evaluation Production readiness"},{"location":"user-guide/tests/#recommended-test-combinations","title":"Recommended Test Combinations","text":"<p>All tests are run by default, which is the recommendation. However, users can specify a subset of tests to run</p> <p>CLI <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json --output-path results/ --test-names accuracy cross_validation\n</code></pre></p> <p>Python <pre><code>results = run_evaluation(config=config, data=data, framework=\"pymc-marketing\", test_names = (\"accuracy\",\"cross_validation\"))\n</code></pre></p>"},{"location":"user-guide/tests/#interpreting-test-results","title":"Interpreting Test Results","text":"<p>Each test answers a distinct question:</p> <ul> <li>Accuracy Test: \"How well does my model predict on unseen data?\"</li> <li>Cross-Validation: \"How consistent are my model's predictions across different splits of unseen data?\"</li> <li>Refresh Stability: \"How much does marketing attribution change when I add new data to my model?\"</li> <li>Perturbation: \"How sensitive is my model is to noise in the marketing inputs?\"</li> </ul> <p>For each test, we compute multiple metrics to give as much insight into the test result as possible: </p> <ul> <li> <p>MAPE (Mean Absolute Percentage Error) <code>MAPE = (100 / n) * \u03a3 |(y_i - \u0177_i) / y_i|</code></p> </li> <li> <p>R-squared (Coefficient of Determination) <code>R\u00b2 = 1 - (\u03a3 (y_i - \u0177_i)^2) / (\u03a3 (y_i - \u0233)^2)</code></p> </li> </ul>"},{"location":"user-guide/tests/#example-results","title":"Example Results","text":"test_name metric_name metric_value metric_pass accuracy mape 0.121 False accuracy r_squared -0.547 False cross_validation mean_mape 0.084 False cross_validation std_mape 0.058 False cross_validation mean_r_squared -7.141 False cross_validation std_r_squared 9.686 False refresh_stability mean_percentage_change_for_each_channel:TV 0.021 False refresh_stability mean_percentage_change_for_each_channel:radio 0.369 False refresh_stability std_percentage_change_for_each_channel:TV 0.021 False refresh_stability std_percentage_change_for_each_channel:radio 0.397 False perturbation percentage_change_for_each_channel:TV 0.005 False perturbation percentage_change_for_each_channel:radio 0.112 False"},{"location":"user-guide/tests/#changing-the-thresholds","title":"Changing the Thresholds","text":"<p>Default metric thresholds in <code>mmm_eval/metrics/threshold_constants.py</code> can be overwritten in-place to change the pass/fail cutoff for each metric.</p>"},{"location":"user-guide/tests/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Metrics to understand test outputs</li> <li>Check Examples for practical test usage</li> <li>Review Configuration for test customization </li> </ul>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":""},{"location":"user-guide/troubleshooting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/troubleshooting/#common-issues-and-solves","title":"Common Issues and Solves","text":"<ol> <li>Slow performance: Reduce sampling parameters or use fewer tests</li> <li>Memory errors: Reduce data size or model complexity</li> <li>Convergence issues: Adjust sampling parameters or model configuration</li> <li>File permission errors: Check write permissions for output directory</li> </ol>"}]}