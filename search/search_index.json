{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to mmm-eval","text":"<p>An open-source tool for evaluating Marketing Mix Modeling (MMM) frameworks with comprehensive validation tests.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Get started with mmm-eval in just a few steps:</p>"},{"location":"#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<p>Prerequisite: Poetry 2.x.x or later is required.</p> <pre><code># Install from GitHub\npoetry add git+https://github.com/Mutiny-Group/mmm-eval.git\n\n# Or clone and install locally\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npoetry install\n</code></pre>"},{"location":"#using-pip","title":"Using pip","text":"<pre><code># Install from GitHub\npip install git+https://github.com/Mutiny-Group/mmm-eval.git\n\n# Or clone and install locally\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npip install -e .\n</code></pre>"},{"location":"#run-a-basic-evaluation","title":"Run a basic evaluation","text":"<pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing\n</code></pre> <p>Note: mmm-eval is currently in development. For production use, we recommend installing from the latest release tag: <pre><code>poetry add git+https://github.com/Mutiny-Group/mmm-eval.git@v0.4.2\n</code></pre></p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>Multi-framework support: Evaluate PyMC-Marketing and other MMM frameworks</li> <li>Comprehensive validation tests: Accuracy, cross-validation, refresh stability, and perturbation tests</li> <li>Standardized metrics: MAPE, RMSE, R-squared, and other industry-standard metrics</li> <li>Flexible data handling: Support for custom column names and data formats</li> <li>CLI interface: Easy-to-use command-line tool for evaluation</li> </ul>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Installation - How to install mmm-eval</li> <li>Quick Start - Get up and running quickly</li> <li>User Guide - Detailed usage instructions</li> <li>API Reference - Complete API documentation</li> <li>Examples - Practical examples and use cases</li> </ul>"},{"location":"#development","title":"\ud83d\udee0\ufe0f Development","text":"<ul> <li>Contributing - How to contribute to mmm-eval</li> <li>Development Setup - Setting up a development environment</li> <li>Testing - Running tests and quality checks</li> </ul>"},{"location":"#supported-frameworks","title":"\ud83d\udcca Supported Frameworks","text":"<p>Currently supported MMM frameworks:</p> <ul> <li>PyMC-Marketing: Bayesian MMM framework using PyMC</li> <li>More frameworks coming soon...</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details on how to get started.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the Apache 2.0 License - see the License file for details.</p> <ul> <li> <p> Quick Start</p> <p>Get up and running with mmm-eval in minutes.</p> </li> <li> <p> User Guide</p> <p>Learn how to use mmm-eval effectively.</p> </li> <li> <p> API Reference</p> <p>Explore the complete API documentation.</p> </li> <li> <p> Examples</p> <p>See practical examples and use cases.</p> </li> </ul>"},{"location":"about/license/","title":"License","text":"<p>This project is licensed under the Apache License, Version 2.0.</p>"},{"location":"about/license/#apache-license-20","title":"Apache License 2.0","text":"<p>Copyright 2024 mmm-eval Contributors</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"about/license/#key-features-of-apache-20","title":"Key Features of Apache 2.0","text":"<p>The Apache License 2.0 provides:</p> <ul> <li>Patent protection: Contributors grant patent licenses for their contributions</li> <li>Commercial use: Allows commercial use, modification, and distribution</li> <li>Patent termination: Patent licenses terminate if you file patent litigation</li> <li>Attribution: Requires preservation of copyright notices and license text</li> <li>Contributor protection: Protects contributors from liability</li> </ul>"},{"location":"about/license/#contributing","title":"Contributing","text":"<p>By contributing to this project, you agree that your contributions will be licensed under the Apache License, Version 2.0.</p>"},{"location":"about/license/#full-license-text","title":"Full License Text","text":"<p>For the complete license text, see the LICENSE file in the root of this repository. </p>"},{"location":"api/adapters/","title":"Adapters Reference","text":""},{"location":"api/adapters/#mmm_eval.adapters","title":"<code>mmm_eval.adapters</code>","text":"<p>Adapters for different MMM frameworks.</p>"},{"location":"api/adapters/#mmm_eval.adapters-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter","title":"<code>PyMCAdapter(config: PyMCConfig)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Initialize the PyMCAdapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PyMCConfig</code> <p>PyMCConfig object</p> required Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def __init__(self, config: PyMCConfig):\n    \"\"\"Initialize the PyMCAdapter.\n\n    Args:\n        config: PyMCConfig object\n\n    \"\"\"\n    self.model_kwargs = config.pymc_model_config_dict\n    self.fit_kwargs = config.fit_config_dict\n    self.date_column = config.date_column\n    self.channel_spend_columns = config.channel_columns\n    self.control_columns = config.control_columns\n    self.model = None\n    self.trace = None\n    self._channel_roi_df = None\n    self.is_fitted = False\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter-functions","title":"Functions","text":""},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.fit","title":"<code>fit(data: pd.DataFrame) -&gt; None</code>","text":"<p>Fit the model and compute ROIs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the training data adhering to the PyMCInputDataSchema.</p> required Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the model and compute ROIs.\n\n    Args:\n        data: DataFrame containing the training data adhering to the PyMCInputDataSchema.\n\n    \"\"\"\n    # Identify channel spend columns that sum to zero and remove them from modelling.\n    # We cannot reliabily make any prediction based on these channels when making\n    # predictions on new data.\n    channel_spend_data = data[self.channel_spend_columns]\n    zero_spend_channels = channel_spend_data.columns[channel_spend_data.sum() == 0].tolist()\n\n    if zero_spend_channels:\n        logger.info(f\"Dropping channels with zero spend: {zero_spend_channels}\")\n        # Remove zero-spend channels from the list passed to the MMM constructor\n        self.channel_spend_columns = [col for col in self.channel_spend_columns if col not in zero_spend_channels]\n        # also update the model config field to reflect the new channel spend columns\n        self.model_kwargs[\"channel_columns\"] = self.channel_spend_columns\n\n        # Check for vector priors that might cause shape mismatches\n        _check_vector_priors_when_dropping_channels(self.model_kwargs[\"model_config\"], zero_spend_channels)\n\n        data = data.drop(columns=zero_spend_channels)\n\n    X = data.drop(columns=[InputDataframeConstants.RESPONSE_COL, InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL])\n    y = data[InputDataframeConstants.RESPONSE_COL]\n\n    self.model = MMM(**self.model_kwargs)\n    self.trace = self.model.fit(X=X, y=y, **self.fit_kwargs)\n\n    self._channel_roi_df = self._compute_channel_contributions(data)\n    self.is_fitted = True\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.get_channel_roi","title":"<code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code>","text":"<p>Return the ROIs for each channel, optionally within a given date range.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Return the ROIs for each channel, optionally within a given date range.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted or self._channel_roi_df is None:\n        raise RuntimeError(\"Model must be fit before computing ROI.\")\n\n    _validate_start_end_dates(start_date, end_date, self._channel_roi_df.index)\n\n    # Filter the contribution DataFrame by date range\n    date_range_df = self._channel_roi_df.loc[start_date:end_date]\n\n    if date_range_df.empty:\n        raise ValueError(f\"No data found for date range {start_date} to {end_date}\")\n\n    return pd.Series(self._calculate_rois(date_range_df))\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.predict","title":"<code>predict(data: pd.DataFrame) -&gt; np.ndarray</code>","text":"<p>Predict the response variable for new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Predict the response variable for new data.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    if not self.is_fitted or self.model is None:\n        raise RuntimeError(\"Model must be fit before prediction.\")\n\n    if InputDataframeConstants.RESPONSE_COL in data.columns:\n        data = data.drop(columns=[InputDataframeConstants.RESPONSE_COL])\n    predictions = self.model.predict(data, extend_idata=False, include_last_observations=True)\n    return predictions\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters-functions","title":"Functions","text":""},{"location":"api/adapters/#mmm_eval.adapters.get_adapter","title":"<code>get_adapter(framework: str, config: PyMCConfig)</code>","text":"<p>Get an adapter instance for the specified framework.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>Name of the MMM framework</p> required <code>config</code> <code>PyMCConfig</code> <p>Framework-specific configuration</p> required <p>Returns:</p> Type Description <p>Adapter instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If framework is not supported</p> Source code in <code>mmm_eval/adapters/__init__.py</code> <pre><code>def get_adapter(framework: str, config: PyMCConfig):\n    \"\"\"Get an adapter instance for the specified framework.\n\n    Args:\n        framework: Name of the MMM framework\n        config: Framework-specific configuration\n\n    Returns:\n        Adapter instance\n\n    Raises:\n        ValueError: If framework is not supported\n\n    \"\"\"\n    if framework not in ADAPTER_REGISTRY:\n        raise ValueError(f\"Unsupported framework: {framework}. Available: {list(ADAPTER_REGISTRY.keys())}\")\n\n    adapter_class = ADAPTER_REGISTRY[framework]\n    return adapter_class(config)\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters-modules","title":"Modules","text":""},{"location":"api/adapters/#mmm_eval.adapters.base","title":"<code>base</code>","text":"<p>Base adapter class for MMM frameworks.</p>"},{"location":"api/adapters/#mmm_eval.adapters.base-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.base.BaseAdapter","title":"<code>BaseAdapter(config: dict[str, Any] | None = None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for MMM framework adapters.</p> <p>Initialize the base adapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Configuration dictionary</p> <code>None</code> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"Initialize the base adapter.\n\n    Args:\n        config: Configuration dictionary\n\n    \"\"\"\n    self.config = config or {}\n    self.is_fitted = False\n    self.channel_spend_columns: list[str] = []\n    self.date_column: str\n</code></pre> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> <code>abstractmethod</code> \u00b6 <p>Fit the model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Training data</p> required Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the model to the data.\n\n    Args:\n        data: Training data\n\n    \"\"\"\n    pass\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> <code>abstractmethod</code> \u00b6 <p>Get channel ROI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Get channel ROI estimates.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    pass\n</code></pre> <code>predict(data: pd.DataFrame) -&gt; np.ndarray</code> <code>abstractmethod</code> \u00b6 <p>Make predictions on new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Make predictions on new data.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.experimental","title":"<code>experimental</code>","text":""},{"location":"api/adapters/#mmm_eval.adapters.experimental-modules","title":"Modules","text":""},{"location":"api/adapters/#mmm_eval.adapters.experimental.pymc","title":"<code>pymc</code>","text":"<p>PyMC MMM framework adapter.</p> <p>N.B. we expect control variables to be scaled to 0-1 using maxabs scaling BEFORE being passed to the PyMCAdapter.</p> Classes\u00b6 <code>PyMCAdapter(config: PyMCConfig)</code> \u00b6 <p>               Bases: <code>BaseAdapter</code></p> <p>Initialize the PyMCAdapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PyMCConfig</code> <p>PyMCConfig object</p> required Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def __init__(self, config: PyMCConfig):\n    \"\"\"Initialize the PyMCAdapter.\n\n    Args:\n        config: PyMCConfig object\n\n    \"\"\"\n    self.model_kwargs = config.pymc_model_config_dict\n    self.fit_kwargs = config.fit_config_dict\n    self.date_column = config.date_column\n    self.channel_spend_columns = config.channel_columns\n    self.control_columns = config.control_columns\n    self.model = None\n    self.trace = None\n    self._channel_roi_df = None\n    self.is_fitted = False\n</code></pre> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> \u00b6 <p>Fit the model and compute ROIs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the training data adhering to the PyMCInputDataSchema.</p> required Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the model and compute ROIs.\n\n    Args:\n        data: DataFrame containing the training data adhering to the PyMCInputDataSchema.\n\n    \"\"\"\n    # Identify channel spend columns that sum to zero and remove them from modelling.\n    # We cannot reliabily make any prediction based on these channels when making\n    # predictions on new data.\n    channel_spend_data = data[self.channel_spend_columns]\n    zero_spend_channels = channel_spend_data.columns[channel_spend_data.sum() == 0].tolist()\n\n    if zero_spend_channels:\n        logger.info(f\"Dropping channels with zero spend: {zero_spend_channels}\")\n        # Remove zero-spend channels from the list passed to the MMM constructor\n        self.channel_spend_columns = [col for col in self.channel_spend_columns if col not in zero_spend_channels]\n        # also update the model config field to reflect the new channel spend columns\n        self.model_kwargs[\"channel_columns\"] = self.channel_spend_columns\n\n        # Check for vector priors that might cause shape mismatches\n        _check_vector_priors_when_dropping_channels(self.model_kwargs[\"model_config\"], zero_spend_channels)\n\n        data = data.drop(columns=zero_spend_channels)\n\n    X = data.drop(columns=[InputDataframeConstants.RESPONSE_COL, InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL])\n    y = data[InputDataframeConstants.RESPONSE_COL]\n\n    self.model = MMM(**self.model_kwargs)\n    self.trace = self.model.fit(X=X, y=y, **self.fit_kwargs)\n\n    self._channel_roi_df = self._compute_channel_contributions(data)\n    self.is_fitted = True\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> \u00b6 <p>Return the ROIs for each channel, optionally within a given date range.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Return the ROIs for each channel, optionally within a given date range.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted or self._channel_roi_df is None:\n        raise RuntimeError(\"Model must be fit before computing ROI.\")\n\n    _validate_start_end_dates(start_date, end_date, self._channel_roi_df.index)\n\n    # Filter the contribution DataFrame by date range\n    date_range_df = self._channel_roi_df.loc[start_date:end_date]\n\n    if date_range_df.empty:\n        raise ValueError(f\"No data found for date range {start_date} to {end_date}\")\n\n    return pd.Series(self._calculate_rois(date_range_df))\n</code></pre> <code>predict(data: pd.DataFrame) -&gt; np.ndarray</code> \u00b6 <p>Predict the response variable for new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/experimental/pymc.py</code> <pre><code>def predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Predict the response variable for new data.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    if not self.is_fitted or self.model is None:\n        raise RuntimeError(\"Model must be fit before prediction.\")\n\n    if InputDataframeConstants.RESPONSE_COL in data.columns:\n        data = data.drop(columns=[InputDataframeConstants.RESPONSE_COL])\n    predictions = self.model.predict(data, extend_idata=False, include_last_observations=True)\n    return predictions\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.experimental.schemas","title":"<code>schemas</code>","text":"Classes\u00b6 <code>PyMCFitSchema</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Schema for PyMC Fit Configuration.</p> <p>Defaults are all set to None so that the user can provide only the values they want to change. If a user does not provide a value, we will let the latest PYMC defaults be used in model instantiation.</p> Attributes\u00b6 <code>fit_config_dict_without_non_provided_fields: dict[str, Any]</code> <code>property</code> \u00b6 <p>Return only non-None values.</p> <p>These are the values that are provided by the user.    We don't want to include the default values as they should be set by the latest PYMC</p> <p>Returns     Dictionary of non-None values</p> <code>PyMCModelSchema</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Schema for PyMC Config Dictionary.</p> Functions\u00b6 <code>validate_adstock(v)</code> \u00b6 <p>Validate adstock component.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Adstock value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If adstock is not a valid type</p> Source code in <code>mmm_eval/adapters/experimental/schemas.py</code> <pre><code>@field_validator(\"adstock\")\ndef validate_adstock(cls, v):\n    \"\"\"Validate adstock component.\n\n    Args:\n        v: Adstock value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If adstock is not a valid type\n\n    \"\"\"\n    if v is not None:\n        assert isinstance(v, AdstockTransformation)\n    return v\n</code></pre> <code>validate_channel_columns(v)</code> \u00b6 <p>Validate channel columns are not empty.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Channel columns value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If channel columns is empty</p> Source code in <code>mmm_eval/adapters/experimental/schemas.py</code> <pre><code>@field_validator(\"channel_columns\")\ndef validate_channel_columns(cls, v):\n    \"\"\"Validate channel columns are not empty.\n\n    Args:\n        v: Channel columns value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If channel columns is empty\n\n    \"\"\"\n    if v is not None and not v:\n        raise ValueError(\"channel_columns must not be empty\")\n    return v\n</code></pre> <code>validate_saturation(v)</code> \u00b6 <p>Validate saturation component.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Saturation value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If saturation is not a valid type</p> Source code in <code>mmm_eval/adapters/experimental/schemas.py</code> <pre><code>@field_validator(\"saturation\")\ndef validate_saturation(cls, v):\n    \"\"\"Validate saturation component.\n\n    Args:\n        v: Saturation value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If saturation is not a valid type\n\n    \"\"\"\n    if v is not None:\n        assert isinstance(v, SaturationTransformation)\n    return v\n</code></pre> <code>PyMCStringConfigSchema</code> \u00b6 <p>               Bases: <code>BaseModel</code></p> <p>Schema for PyMC Evaluation Config Dictionary.</p>"},{"location":"api/adapters/#mmm_eval.adapters.meridian","title":"<code>meridian</code>","text":"<p>Meridian adapter for MMM evaluation.</p>"},{"location":"api/adapters/#mmm_eval.adapters.meridian-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.meridian.MeridianAdapter","title":"<code>MeridianAdapter(config: dict[str, Any] | None = None)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for Google Meridian MMM framework.</p> <p>Initialize the Meridian adapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Configuration dictionary</p> <code>None</code> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"Initialize the Meridian adapter.\n\n    Args:\n        config: Configuration dictionary\n\n    \"\"\"\n    super().__init__(config)\n    self.media_columns = config.get(\"media_columns\", []) if config else []\n    self.base_columns = config.get(\"base_columns\", []) if config else []\n</code></pre> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> \u00b6 <p>Fit the Meridian model to data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Training data</p> required Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the Meridian model to data.\n\n    Args:\n        data: Training data\n\n    \"\"\"\n    # Placeholder implementation - simple linear combination\n    self.is_fitted = True\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> \u00b6 <p>Get channel ROI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Get channel ROI estimates.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted:\n        raise RuntimeError(\"Model must be fit before computing ROI\")\n    # Placeholder implementation\n    return pd.Series()\n</code></pre> <code>predict(data: pd.DataFrame) -&gt; np.ndarray</code> \u00b6 <p>Make predictions using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Make predictions using the fitted model.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    if not self.is_fitted:\n        raise RuntimeError(\"Model must be fit before prediction\")\n\n    # # Get media and base columns, or use all numeric columns\n    # media_cols = (\n    #     self.media_columns\n    #     or data.select_dtypes(include=[np.number]).columns.tolist()\n    # )\n\n    # if not media_cols:\n    #     # Fallback: create dummy predictions\n    #     return pd.Series(np.random.normal(100, 10, len(data)), index=data.index)\n\n    # # Simple placeholder prediction (weighted sum of media channels)\t    def get_channel_roi(\n    # weights = np.random.uniform(0.5, 2.0, len(media_cols))\n    # base_effect = 50  # Base level\n    # predictions = base_effect + np.dot(data[media_cols].fillna(0), weights)\n    # return pd.Series(predictions, index=data.index)\n\n    # Placeholder implementation\n    return np.zeros(len(data))\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.pymc","title":"<code>pymc</code>","text":"<p>Legacy PyMC adapter for MMM evaluation.</p>"},{"location":"api/adapters/#mmm_eval.adapters.pymc-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.pymc.PyMCAdapter","title":"<code>PyMCAdapter(config: dict[str, Any] | None = None)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Legacy adapter for PyMC MMM framework.</p> <p>Initialize the legacy PyMC adapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Configuration dictionary</p> <code>None</code> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"Initialize the legacy PyMC adapter.\n\n    Args:\n        config: Configuration dictionary\n\n    \"\"\"\n    pass\n</code></pre> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> \u00b6 <p>Fit the PyMC model to data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Training data</p> required Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the PyMC model to data.\n\n    Args:\n        data: Training data\n\n    \"\"\"\n    # Placeholder implementation\n    pass\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> \u00b6 <p>Get channel ROI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Get channel ROI estimates.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    return pd.Series()\n</code></pre> <code>predict(data: pd.DataFrame) -&gt; np.ndarray</code> \u00b6 <p>Make predictions using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data for prediction</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def predict(self, data: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Make predictions using the fitted model.\n\n    Args:\n        data: Input data for prediction\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    # Placeholder implementation\n    return np.zeros(len(data))\n</code></pre>"},{"location":"api/cli/","title":"CLI reference","text":""},{"location":"api/cli/#mmm_eval.cli","title":"<code>mmm_eval.cli</code>","text":""},{"location":"api/cli/#mmm_eval.cli-modules","title":"Modules","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate","title":"<code>evaluate</code>","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate-classes","title":"Classes","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate-functions","title":"Functions","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate.main","title":"<code>main(config_path: str, input_data_path: str, test_names: tuple[str, ...], framework: str, output_path: str, verbose: bool)</code>","text":"<p>Evaluate MMM frameworks using the unified API.</p> Source code in <code>mmm_eval/cli/evaluate.py</code> <pre><code>@click.command()\n@click.option(\n    \"--framework\",\n    type=click.Choice(list(ADAPTER_REGISTRY.keys())),\n    required=True,\n    help=\"Open source MMM framework to evaluate\",\n)\n@click.option(\n    \"--input-data-path\",\n    type=str,\n    required=True,\n    help=\"Path to input data file. Supported formats: CSV, Parquet\",\n)\n@click.option(\n    \"--output-path\",\n    type=str,\n    required=True,\n    help=\"Directory to save evaluation results as a CSV file with name 'mmm_eval_&lt;framework&gt;_&lt;timestamp&gt;.csv'\",\n)\n@click.option(\n    \"--config-path\",\n    type=str,\n    required=True,\n    help=\"Path to framework-specific JSON config file\",\n)\n@click.option(\n    \"--test-names\",\n    type=click.Choice(ValidationTestNames.all_tests_as_str()),\n    multiple=True,\n    default=tuple(ValidationTestNames.all_tests_as_str()),\n    help=(\n        \"Test names to run. Can specify multiple tests as space-separated values \"\n        \"(e.g. --test-names accuracy cross_validation) or by repeating the flag \"\n        \"(e.g. --test-names accuracy --test-names cross_validation). \"\n        \"Defaults to all tests if not specified.\"\n    ),\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Enable verbose logging\",\n)\ndef main(\n    config_path: str,\n    input_data_path: str,\n    test_names: tuple[str, ...],\n    framework: str,\n    output_path: str,\n    verbose: bool,\n):\n    \"\"\"Evaluate MMM frameworks using the unified API.\"\"\"\n    # logging\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level)\n\n    logger.info(\"Loading config...\")\n    config = get_config(framework, config_path)\n\n    logger.info(\"Loading input data...\")\n    data = DataLoader(input_data_path).load()\n\n    # Run evaluation\n    logger.info(f\"Running evaluation suite for {framework} framework...\")\n    results = run_evaluation(framework, data, config, test_names)\n\n    # Save results\n    if results.empty:\n        logger.warning(\"Results df empty, nothing to save.\")\n    else:\n        save_results(results, framework, output_path)\n</code></pre>"},{"location":"api/cli/#mmm_eval.cli.evaluate-modules","title":"Modules","text":""},{"location":"api/core/","title":"Core API Reference","text":""},{"location":"api/core/#mmm_eval.core","title":"<code>mmm_eval.core</code>","text":"<p>Core validation functionality for MMM frameworks.</p>"},{"location":"api/core/#mmm_eval.core-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.BaseValidationTest","title":"<code>BaseValidationTest()</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for validation tests.</p> <p>All validation tests must inherit from this class and implement the required methods to provide a unified testing interface.</p> <p>Initialize the validation test.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.BaseValidationTest-attributes","title":"Attributes","text":""},{"location":"api/core/#mmm_eval.core.BaseValidationTest.test_name","title":"<code>test_name: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the name of the test.</p> <p>Returns     Test name (e.g., 'accuracy', 'stability')</p>"},{"location":"api/core/#mmm_eval.core.BaseValidationTest-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.BaseValidationTest.run","title":"<code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code>  <code>abstractmethod</code>","text":"<p>Run the validation test.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>@abstractmethod\ndef run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#mmm_eval.core.BaseValidationTest.run_with_error_handling","title":"<code>run_with_error_handling(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code>","text":"<p>Run the validation test with error handling.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> <p>Raises:</p> Type Description <code>MetricCalculationError</code> <p>If metric calculation fails</p> <code>TestExecutionError</code> <p>If test execution fails</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def run_with_error_handling(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test with error handling.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    Raises:\n        MetricCalculationError: If metric calculation fails\n        TestExecutionError: If test execution fails\n\n    \"\"\"\n    try:\n        return self.run(adapter, data)\n    except ZeroDivisionError as e:\n        # This is clearly a mathematical calculation issue\n        raise MetricCalculationError(f\"Metric calculation error in {self.test_name} test: {str(e)}\") from e\n    except Exception as e:\n        # All other errors - let individual tests handle specific categorization if needed\n        raise TestExecutionError(f\"Test execution error in {self.test_name} test: {str(e)}\") from e\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults","title":"<code>ValidationResults(test_results: dict[ValidationTestNames, ValidationTestResult])</code>","text":"<p>Container for complete validation results.</p> <p>This class holds the results of all validation tests run, including individual test results and overall summary.</p> <p>Initialize validation results.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <code>dict[ValidationTestNames, ValidationTestResult]</code> <p>Dictionary mapping test names to their results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(self, test_results: dict[ValidationTestNames, ValidationTestResult]):\n    \"\"\"Initialize validation results.\n\n    Args:\n        test_results: Dictionary mapping test names to their results\n\n    \"\"\"\n    self.test_results = test_results\n    self.timestamp = datetime.now()\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.ValidationResults.all_passed","title":"<code>all_passed() -&gt; bool</code>","text":"<p>Check if all tests passed.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def all_passed(self) -&gt; bool:\n    \"\"\"Check if all tests passed.\"\"\"\n    return all(result.passed for result in self.test_results.values())\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults.get_test_result","title":"<code>get_test_result(test_name: ValidationTestNames) -&gt; ValidationTestResult</code>","text":"<p>Get results for a specific test.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def get_test_result(self, test_name: ValidationTestNames) -&gt; ValidationTestResult:\n    \"\"\"Get results for a specific test.\"\"\"\n    return self.test_results[test_name]\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults.to_df","title":"<code>to_df() -&gt; pd.DataFrame</code>","text":"<p>Convert nested test results to a flat DataFrame format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert nested test results to a flat DataFrame format.\"\"\"\n    rows = []\n\n    for result in self.test_results.values():\n        test_name = result.test_name.value\n        passed = result.passed\n        test_scores_dict = result.test_scores.to_dict()\n\n        for metric_key, value in test_scores_dict.items():\n            if isinstance(value, pd.Series):\n                for subkey, subval in value.items():\n                    rows.append(\n                        {\n                            \"test_name\": test_name,\n                            \"metric_name\": f\"{metric_key}:{subkey}\",\n                            \"metric_value\": subval,\n                            \"metric_pass\": passed,\n                        }\n                    )\n            else:\n                rows.append(\n                    {\n                        \"test_name\": test_name,\n                        \"metric_name\": metric_key,\n                        \"metric_value\": value,\n                        \"metric_pass\": passed,\n                    }\n                )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert results to dictionary format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert results to dictionary format.\"\"\"\n    return {\n        ValidationResultAttributeNames.TIMESTAMP.value: self.timestamp.isoformat(),\n        ValidationResultAttributeNames.ALL_PASSED.value: self.all_passed(),\n        ValidationResultAttributeNames.RESULTS.value: {\n            result.test_name.value: result.to_dict() for result in self.test_results.values()\n        },\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestOrchestrator","title":"<code>ValidationTestOrchestrator()</code>","text":"<p>Main orchestrator for running validation tests.</p> <p>This class manages the test registry and executes tests in sequence, aggregating their results.</p> <p>Initialize the validator with standard tests pre-registered.</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validator with standard tests pre-registered.\"\"\"\n    self.tests: dict[ValidationTestNames, type[BaseValidationTest]] = {\n        ValidationTestNames.ACCURACY: AccuracyTest,\n        ValidationTestNames.CROSS_VALIDATION: CrossValidationTest,\n        ValidationTestNames.REFRESH_STABILITY: RefreshStabilityTest,\n        ValidationTestNames.PERTURBATION: PerturbationTest,\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestOrchestrator-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.ValidationTestOrchestrator.validate","title":"<code>validate(adapter: BaseAdapter, data: pd.DataFrame, test_names: list[ValidationTestNames]) -&gt; ValidationResults</code>","text":"<p>Run validation tests on the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <code>test_names</code> <code>list[ValidationTestNames]</code> <p>List of test names to run</p> required <code>adapter</code> <code>BaseAdapter</code> <p>Adapter to use for the test</p> required <p>Returns:</p> Type Description <code>ValidationResults</code> <p>ValidationResults containing all test results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any requested test is not registered</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def validate(\n    self,\n    adapter: BaseAdapter,\n    data: pd.DataFrame,\n    test_names: list[ValidationTestNames],\n) -&gt; ValidationResults:\n    \"\"\"Run validation tests on the model.\n\n    Args:\n        model: Model to validate\n        data: Input data for validation\n        test_names: List of test names to run\n        adapter: Adapter to use for the test\n\n    Returns:\n        ValidationResults containing all test results\n\n    Raises:\n        ValueError: If any requested test is not registered\n\n    \"\"\"\n    # Run tests and collect results\n    results: dict[ValidationTestNames, ValidationTestResult] = {}\n    for test_name in test_names:\n        logger.info(f\"Running test: {test_name}\")\n        test_instance = self.tests[test_name]()\n        test_result = test_instance.run_with_error_handling(adapter, data)\n        results[test_name] = test_result\n\n    return ValidationResults(results)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestResult","title":"<code>ValidationTestResult(test_name: ValidationTestNames, passed: bool, metric_names: list[str], test_scores: AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults)</code>","text":"<p>Container for individual test results.</p> <p>This class holds the results of a single validation test, including pass/fail status, metrics, and any error messages.</p> <p>Initialize test results.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>ValidationTestNames</code> <p>Name of the test</p> required <code>passed</code> <code>bool</code> <p>Whether the test passed</p> required <code>metric_names</code> <code>list[str]</code> <p>List of metric names</p> required <code>test_scores</code> <code>AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults</code> <p>Computed metric results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(\n    self,\n    test_name: ValidationTestNames,\n    passed: bool,\n    metric_names: list[str],\n    test_scores: (\n        AccuracyMetricResults\n        | CrossValidationMetricResults\n        | RefreshStabilityMetricResults\n        | PerturbationMetricResults\n    ),\n):\n    \"\"\"Initialize test results.\n\n    Args:\n        test_name: Name of the test\n        passed: Whether the test passed\n        metric_names: List of metric names\n        test_scores: Computed metric results\n\n    \"\"\"\n    self.test_name = test_name\n    self.passed = passed\n    self.metric_names = metric_names\n    self.test_scores = test_scores\n    self.timestamp = datetime.now()\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestResult-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.ValidationTestResult.to_dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert results to dictionary format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert results to dictionary format.\"\"\"\n    return {\n        ValidationTestAttributeNames.TEST_NAME.value: self.test_name.value,\n        # todo(): Perhaps set as false permanently or dont use if we dont want thresholds\n        ValidationTestAttributeNames.PASSED.value: self.passed,\n        ValidationTestAttributeNames.METRIC_NAMES.value: self.metric_names,\n        ValidationTestAttributeNames.TEST_SCORES.value: self.test_scores.to_dict(),\n        ValidationTestAttributeNames.TIMESTAMP.value: self.timestamp.isoformat(),\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core-modules","title":"Modules","text":""},{"location":"api/core/#mmm_eval.core.base_validation_test","title":"<code>base_validation_test</code>","text":"<p>Abstract base classes for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.base_validation_test-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.base_validation_test.BaseValidationTest","title":"<code>BaseValidationTest()</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for validation tests.</p> <p>All validation tests must inherit from this class and implement the required methods to provide a unified testing interface.</p> <p>Initialize the validation test.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: str</code> <code>abstractmethod</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> <p>Returns     Test name (e.g., 'accuracy', 'stability')</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> <code>abstractmethod</code> \u00b6 <p>Run the validation test.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>@abstractmethod\ndef run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    \"\"\"\n    pass\n</code></pre> <code>run_with_error_handling(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the validation test with error handling.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> <p>Raises:</p> Type Description <code>MetricCalculationError</code> <p>If metric calculation fails</p> <code>TestExecutionError</code> <p>If test execution fails</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def run_with_error_handling(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test with error handling.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    Raises:\n        MetricCalculationError: If metric calculation fails\n        TestExecutionError: If test execution fails\n\n    \"\"\"\n    try:\n        return self.run(adapter, data)\n    except ZeroDivisionError as e:\n        # This is clearly a mathematical calculation issue\n        raise MetricCalculationError(f\"Metric calculation error in {self.test_name} test: {str(e)}\") from e\n    except Exception as e:\n        # All other errors - let individual tests handle specific categorization if needed\n        raise TestExecutionError(f\"Test execution error in {self.test_name} test: {str(e)}\") from e\n</code></pre>"},{"location":"api/core/#mmm_eval.core.constants","title":"<code>constants</code>","text":""},{"location":"api/core/#mmm_eval.core.constants-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.constants.ValidationTestConstants","title":"<code>ValidationTestConstants</code>","text":"<p>Constants for the validation tests.</p> Classes\u00b6 <code>PerturbationConstants</code> \u00b6 <p>Constants for the perturbation test.</p>"},{"location":"api/core/#mmm_eval.core.evaluator","title":"<code>evaluator</code>","text":"<p>Main evaluator for MMM frameworks.</p>"},{"location":"api/core/#mmm_eval.core.evaluator-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.evaluator.Evaluator","title":"<code>Evaluator(data: pd.DataFrame, test_names: tuple[str, ...] | None = None)</code>","text":"<p>Main evaluator class for MMM frameworks.</p> <p>This class provides a unified interface for evaluating different MMM frameworks using standardized validation tests.</p> <p>Initialize the evaluator.</p> Source code in <code>mmm_eval/core/evaluator.py</code> <pre><code>def __init__(self, data: pd.DataFrame, test_names: tuple[str, ...] | None = None):\n    \"\"\"Initialize the evaluator.\"\"\"\n    self.validation_orchestrator = ValidationTestOrchestrator()\n    self.data = data\n    self.test_names = (\n        self._get_test_names(test_names) if test_names else self.validation_orchestrator._get_all_test_names()\n    )\n</code></pre> Functions\u00b6 <code>evaluate_framework(framework: str, config: BaseConfig) -&gt; ValidationResults</code> \u00b6 <p>Evaluate an MMM framework using the unified API.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>Name of the MMM framework to evaluate</p> required <code>config</code> <code>BaseConfig</code> <p>Framework-specific configuration</p> required <p>Returns:</p> Type Description <code>ValidationResults</code> <p>ValidationResult object containing evaluation metrics and predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any test name is invalid</p> Source code in <code>mmm_eval/core/evaluator.py</code> <pre><code>def evaluate_framework(self, framework: str, config: BaseConfig) -&gt; ValidationResults:\n    \"\"\"Evaluate an MMM framework using the unified API.\n\n    Args:\n        framework: Name of the MMM framework to evaluate\n        config: Framework-specific configuration\n\n    Returns:\n        ValidationResult object containing evaluation metrics and predictions\n\n    Raises:\n        ValueError: If any test name is invalid\n\n    \"\"\"\n    # Initialize the adapter\n    adapter = get_adapter(framework, config)\n\n    # Run validation tests\n    validation_results = self.validation_orchestrator.validate(\n        adapter=adapter,\n        data=self.data,\n        test_names=self.test_names,\n    )\n\n    return validation_results\n</code></pre>"},{"location":"api/core/#mmm_eval.core.evaluator-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.exceptions-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.exceptions.InvalidTestNameError","title":"<code>InvalidTestNameError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when an invalid test name is provided.</p>"},{"location":"api/core/#mmm_eval.core.exceptions.MetricCalculationError","title":"<code>MetricCalculationError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when metric calculation fails.</p>"},{"location":"api/core/#mmm_eval.core.exceptions.TestExecutionError","title":"<code>TestExecutionError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when test execution fails.</p>"},{"location":"api/core/#mmm_eval.core.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for validation framework errors.</p>"},{"location":"api/core/#mmm_eval.core.run_evaluation","title":"<code>run_evaluation</code>","text":""},{"location":"api/core/#mmm_eval.core.run_evaluation-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.run_evaluation-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.run_evaluation.run_evaluation","title":"<code>run_evaluation(framework: str, data: pd.DataFrame, config: BaseConfig, test_names: tuple[str, ...] | None = None) -&gt; pd.DataFrame</code>","text":"<p>Evaluate an MMM framework.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>The framework to evaluate.</p> required <code>data</code> <code>DataFrame</code> <p>The data to evaluate.</p> required <code>config</code> <code>BaseConfig</code> <p>The config to use for the evaluation.</p> required <code>test_names</code> <code>tuple[str, ...] | None</code> <p>The tests to run. If not provided, all tests will be run.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame containing the evaluation results.</p> Source code in <code>mmm_eval/core/run_evaluation.py</code> <pre><code>def run_evaluation(\n    framework: str,\n    data: pd.DataFrame,\n    config: BaseConfig,\n    test_names: tuple[str, ...] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Evaluate an MMM framework.\n\n    Args:\n        framework: The framework to evaluate.\n        data: The data to evaluate.\n        config: The config to use for the evaluation.\n        test_names: The tests to run. If not provided, all tests will be run.\n\n    Returns:\n        A pandas DataFrame containing the evaluation results.\n\n    \"\"\"\n    # validate + process the input data\n    data = DataPipeline(\n        data=data,\n        date_column=config.date_column,\n        response_column=config.response_column,\n        revenue_column=config.revenue_column,\n        control_columns=config.control_columns,\n        channel_columns=config.channel_columns,\n    ).run()\n\n    # run the evaluation suite\n    results = Evaluator(\n        data=data,\n        test_names=test_names,\n    ).evaluate_framework(framework=framework, config=config)\n\n    return results.to_df()\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_test_orchestrator","title":"<code>validation_test_orchestrator</code>","text":"<p>Test orchestrator for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.validation_test_orchestrator-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_test_orchestrator.ValidationTestOrchestrator","title":"<code>ValidationTestOrchestrator()</code>","text":"<p>Main orchestrator for running validation tests.</p> <p>This class manages the test registry and executes tests in sequence, aggregating their results.</p> <p>Initialize the validator with standard tests pre-registered.</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validator with standard tests pre-registered.\"\"\"\n    self.tests: dict[ValidationTestNames, type[BaseValidationTest]] = {\n        ValidationTestNames.ACCURACY: AccuracyTest,\n        ValidationTestNames.CROSS_VALIDATION: CrossValidationTest,\n        ValidationTestNames.REFRESH_STABILITY: RefreshStabilityTest,\n        ValidationTestNames.PERTURBATION: PerturbationTest,\n    }\n</code></pre> Functions\u00b6 <code>validate(adapter: BaseAdapter, data: pd.DataFrame, test_names: list[ValidationTestNames]) -&gt; ValidationResults</code> \u00b6 <p>Run validation tests on the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <code>test_names</code> <code>list[ValidationTestNames]</code> <p>List of test names to run</p> required <code>adapter</code> <code>BaseAdapter</code> <p>Adapter to use for the test</p> required <p>Returns:</p> Type Description <code>ValidationResults</code> <p>ValidationResults containing all test results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any requested test is not registered</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def validate(\n    self,\n    adapter: BaseAdapter,\n    data: pd.DataFrame,\n    test_names: list[ValidationTestNames],\n) -&gt; ValidationResults:\n    \"\"\"Run validation tests on the model.\n\n    Args:\n        model: Model to validate\n        data: Input data for validation\n        test_names: List of test names to run\n        adapter: Adapter to use for the test\n\n    Returns:\n        ValidationResults containing all test results\n\n    Raises:\n        ValueError: If any requested test is not registered\n\n    \"\"\"\n    # Run tests and collect results\n    results: dict[ValidationTestNames, ValidationTestResult] = {}\n    for test_name in test_names:\n        logger.info(f\"Running test: {test_name}\")\n        test_instance = self.tests[test_name]()\n        test_result = test_instance.run_with_error_handling(adapter, data)\n        results[test_name] = test_result\n\n    return ValidationResults(results)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_test_results","title":"<code>validation_test_results</code>","text":"<p>Result containers for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.validation_test_results-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_test_results.ValidationResults","title":"<code>ValidationResults(test_results: dict[ValidationTestNames, ValidationTestResult])</code>","text":"<p>Container for complete validation results.</p> <p>This class holds the results of all validation tests run, including individual test results and overall summary.</p> <p>Initialize validation results.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <code>dict[ValidationTestNames, ValidationTestResult]</code> <p>Dictionary mapping test names to their results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(self, test_results: dict[ValidationTestNames, ValidationTestResult]):\n    \"\"\"Initialize validation results.\n\n    Args:\n        test_results: Dictionary mapping test names to their results\n\n    \"\"\"\n    self.test_results = test_results\n    self.timestamp = datetime.now()\n</code></pre> Functions\u00b6 <code>all_passed() -&gt; bool</code> \u00b6 <p>Check if all tests passed.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def all_passed(self) -&gt; bool:\n    \"\"\"Check if all tests passed.\"\"\"\n    return all(result.passed for result in self.test_results.values())\n</code></pre> <code>get_test_result(test_name: ValidationTestNames) -&gt; ValidationTestResult</code> \u00b6 <p>Get results for a specific test.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def get_test_result(self, test_name: ValidationTestNames) -&gt; ValidationTestResult:\n    \"\"\"Get results for a specific test.\"\"\"\n    return self.test_results[test_name]\n</code></pre> <code>to_df() -&gt; pd.DataFrame</code> \u00b6 <p>Convert nested test results to a flat DataFrame format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert nested test results to a flat DataFrame format.\"\"\"\n    rows = []\n\n    for result in self.test_results.values():\n        test_name = result.test_name.value\n        passed = result.passed\n        test_scores_dict = result.test_scores.to_dict()\n\n        for metric_key, value in test_scores_dict.items():\n            if isinstance(value, pd.Series):\n                for subkey, subval in value.items():\n                    rows.append(\n                        {\n                            \"test_name\": test_name,\n                            \"metric_name\": f\"{metric_key}:{subkey}\",\n                            \"metric_value\": subval,\n                            \"metric_pass\": passed,\n                        }\n                    )\n            else:\n                rows.append(\n                    {\n                        \"test_name\": test_name,\n                        \"metric_name\": metric_key,\n                        \"metric_value\": value,\n                        \"metric_pass\": passed,\n                    }\n                )\n\n    return pd.DataFrame(rows)\n</code></pre> <code>to_dict() -&gt; dict[str, Any]</code> \u00b6 <p>Convert results to dictionary format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert results to dictionary format.\"\"\"\n    return {\n        ValidationResultAttributeNames.TIMESTAMP.value: self.timestamp.isoformat(),\n        ValidationResultAttributeNames.ALL_PASSED.value: self.all_passed(),\n        ValidationResultAttributeNames.RESULTS.value: {\n            result.test_name.value: result.to_dict() for result in self.test_results.values()\n        },\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_test_results.ValidationTestResult","title":"<code>ValidationTestResult(test_name: ValidationTestNames, passed: bool, metric_names: list[str], test_scores: AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults)</code>","text":"<p>Container for individual test results.</p> <p>This class holds the results of a single validation test, including pass/fail status, metrics, and any error messages.</p> <p>Initialize test results.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>ValidationTestNames</code> <p>Name of the test</p> required <code>passed</code> <code>bool</code> <p>Whether the test passed</p> required <code>metric_names</code> <code>list[str]</code> <p>List of metric names</p> required <code>test_scores</code> <code>AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults</code> <p>Computed metric results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(\n    self,\n    test_name: ValidationTestNames,\n    passed: bool,\n    metric_names: list[str],\n    test_scores: (\n        AccuracyMetricResults\n        | CrossValidationMetricResults\n        | RefreshStabilityMetricResults\n        | PerturbationMetricResults\n    ),\n):\n    \"\"\"Initialize test results.\n\n    Args:\n        test_name: Name of the test\n        passed: Whether the test passed\n        metric_names: List of metric names\n        test_scores: Computed metric results\n\n    \"\"\"\n    self.test_name = test_name\n    self.passed = passed\n    self.metric_names = metric_names\n    self.test_scores = test_scores\n    self.timestamp = datetime.now()\n</code></pre> Functions\u00b6 <code>to_dict() -&gt; dict[str, Any]</code> \u00b6 <p>Convert results to dictionary format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert results to dictionary format.\"\"\"\n    return {\n        ValidationTestAttributeNames.TEST_NAME.value: self.test_name.value,\n        # todo(): Perhaps set as false permanently or dont use if we dont want thresholds\n        ValidationTestAttributeNames.PASSED.value: self.passed,\n        ValidationTestAttributeNames.METRIC_NAMES.value: self.metric_names,\n        ValidationTestAttributeNames.TEST_SCORES.value: self.test_scores.to_dict(),\n        ValidationTestAttributeNames.TIMESTAMP.value: self.timestamp.isoformat(),\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests","title":"<code>validation_tests</code>","text":""},{"location":"api/core/#mmm_eval.core.validation_tests-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_tests.AccuracyTest","title":"<code>AccuracyTest()</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for model accuracy using holdout validation.</p> <p>This test evaluates model performance by splitting data into train/test sets and calculating MAPE and R-squared metrics on the test set.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the accuracy test.</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the accuracy test.\"\"\"\n    # Split data into train/test sets\n    train, test = self._split_data_holdout(data)\n    adapter.fit(train)  # fit() modifies model in-place, returns None\n    predictions = adapter.predict(test)  # predict() on same model instance\n\n    # Calculate metrics\n    test_scores = AccuracyMetricResults.populate_object_with_metrics(\n        actual=test[InputDataframeConstants.RESPONSE_COL],\n        predicted=predictions,\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.ACCURACY,\n        passed=test_scores.check_test_passed(),\n        metric_names=AccuracyMetricNames.metrics_to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests.CrossValidationTest","title":"<code>CrossValidationTest()</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for the cross-validation of the MMM framework.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the cross-validation test using time-series splits.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to validate</p> required <code>adapter</code> <code>BaseAdapter</code> <p>Adapter to use for the test</p> required <code>data</code> <code>DataFrame</code> <p>Input data</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult containing cross-validation metrics</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the cross-validation test using time-series splits.\n\n    Args:\n        model: Model to validate\n        adapter: Adapter to use for the test\n        data: Input data\n\n    Returns:\n        TestResult containing cross-validation metrics\n\n    \"\"\"\n    # Initialize cross-validation splitter\n    cv_splits = self._split_data_time_series_cv(data)\n\n    # Store metrics for each fold\n    fold_metrics = []\n\n    # Run cross-validation\n    for i, (train_idx, test_idx) in enumerate(cv_splits):\n\n        logger.info(f\"Running cross-validation fold {i+1} of {len(cv_splits)}\")\n\n        # Get train/test data\n        train = data.iloc[train_idx]\n        test = data.iloc[test_idx]\n\n        # Get predictions\n        adapter.fit(train)\n        predictions = adapter.predict(test)\n\n        # Add in fold results\n        fold_metrics.append(\n            AccuracyMetricResults.populate_object_with_metrics(\n                actual=test[InputDataframeConstants.RESPONSE_COL],\n                predicted=predictions,\n            )\n        )\n\n    # Calculate mean and std of metrics across folds and create metric results\n    test_scores = CrossValidationMetricResults(\n        mean_mape=calculate_mean_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.MAPE\n        ),\n        std_mape=calculate_std_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.MAPE\n        ),\n        mean_r_squared=calculate_mean_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.R_SQUARED\n        ),\n        std_r_squared=calculate_std_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.R_SQUARED\n        ),\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.CROSS_VALIDATION,\n        passed=test_scores.check_test_passed(),\n        metric_names=CrossValidationMetricNames.metrics_to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests.PerturbationTest","title":"<code>PerturbationTest()</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for the perturbation of the MMM framework.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the perturbation test.</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the perturbation test.\"\"\"\n    # Train model on original data\n    adapter.fit(data)\n    original_rois = adapter.get_channel_roi()\n\n    # Add noise to spend data and retrain\n    noisy_data = self._add_gaussian_noise_to_spend(\n        df=data,\n        spend_cols=adapter.channel_spend_columns,\n    )\n    adapter.fit(noisy_data)\n    noise_rois = adapter.get_channel_roi()\n\n    # calculate the pct change in roi\n    percentage_change = calculate_absolute_percentage_change(\n        baseline_series=original_rois,\n        comparison_series=noise_rois,\n    )\n\n    # Create metric results - roi % change for each channel\n    test_scores = PerturbationMetricResults(\n        percentage_change_for_each_channel=percentage_change,\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.PERTURBATION,\n        passed=test_scores.check_test_passed(),\n        metric_names=PerturbationMetricNames.metrics_to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests.RefreshStabilityTest","title":"<code>RefreshStabilityTest()</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for the stability of the MMM framework.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the stability test.</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the stability test.\"\"\"\n    # Initialize cross-validation splitter\n    cv_splits = self._split_data_time_series_cv(data)\n\n    # Store metrics for each fold\n    fold_metrics = []\n\n    # Run cross-validation\n    for i, (train_idx, refresh_idx) in enumerate(cv_splits):\n\n        logger.info(f\"Running refresh stability test fold {i+1} of {len(cv_splits)}\")\n\n        # Get train/test data\n        # todo(): Can we somehow store these training changes in the adapter for use in time series holdout test\n        current_data = data.iloc[train_idx]\n        # Combine current data with refresh data for retraining\n        refresh_data = pd.concat([current_data, data.iloc[refresh_idx]], ignore_index=True)\n        # Get common dates for roi stability comparison\n        common_start_date, common_end_date = self._get_common_dates(\n            baseline_data=current_data,\n            comparison_data=refresh_data,\n            date_column=adapter.date_column,\n        )\n\n        # Train model and get coefficients\n        adapter.fit(current_data)\n        current_model_rois = adapter.get_channel_roi(\n            start_date=common_start_date,\n            end_date=common_end_date,\n        )\n        adapter.fit(refresh_data)\n        refreshed_model_rois = adapter.get_channel_roi(\n            start_date=common_start_date,\n            end_date=common_end_date,\n        )\n\n        # calculate the pct change in volume\n        percentage_change = calculate_absolute_percentage_change(\n            baseline_series=current_model_rois,\n            comparison_series=refreshed_model_rois,\n        )\n\n        fold_metrics.append(percentage_change)\n\n    # Calculate mean and std of percentage change for each channel across cross validation folds\n    test_scores = RefreshStabilityMetricResults(\n        mean_percentage_change_for_each_channel=calculate_means_for_series_across_cross_validation_folds(\n            fold_metrics\n        ),\n        std_percentage_change_for_each_channel=calculate_stds_for_series_across_cross_validation_folds(\n            fold_metrics\n        ),\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.REFRESH_STABILITY,\n        passed=test_scores.check_test_passed(),\n        metric_names=RefreshStabilityMetricNames.metrics_to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.validation_tests_models","title":"<code>validation_tests_models</code>","text":""},{"location":"api/core/#mmm_eval.core.validation_tests_models-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_tests_models.ValidationResultAttributeNames","title":"<code>ValidationResultAttributeNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the names of the validation result attributes.</p>"},{"location":"api/core/#mmm_eval.core.validation_tests_models.ValidationTestAttributeNames","title":"<code>ValidationTestAttributeNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the names of the validation test attributes.</p>"},{"location":"api/core/#mmm_eval.core.validation_tests_models.ValidationTestNames","title":"<code>ValidationTestNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the names of the validation tests.</p> Functions\u00b6 <code>all_tests() -&gt; list[ValidationTestNames]</code> <code>classmethod</code> \u00b6 <p>Return all validation test names as a list.</p> Source code in <code>mmm_eval/core/validation_tests_models.py</code> <pre><code>@classmethod\ndef all_tests(cls) -&gt; list[\"ValidationTestNames\"]:\n    \"\"\"Return all validation test names as a list.\"\"\"\n    return list(cls)\n</code></pre> <code>all_tests_as_str() -&gt; list[str]</code> <code>classmethod</code> \u00b6 <p>Return all validation test names as a list of strings.</p> Source code in <code>mmm_eval/core/validation_tests_models.py</code> <pre><code>@classmethod\ndef all_tests_as_str(cls) -&gt; list[str]:\n    \"\"\"Return all validation test names as a list of strings.\"\"\"\n    return [test.value for test in cls]\n</code></pre>"},{"location":"api/data/","title":"Data Reference","text":""},{"location":"api/data/#mmm_eval.data","title":"<code>mmm_eval.data</code>","text":"<p>Data loading and processing utilities.</p>"},{"location":"api/data/#mmm_eval.data-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.DataLoader","title":"<code>DataLoader(data_path: str | Path)</code>","text":"<p>Simple data loader for MMM evaluation.</p> <p>Takes a data path and loads the data.</p> <p>Initialize data loader with data path.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str | Path</code> <p>Path to the data file (CSV, Parquet, etc.)</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the data file does not exist.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def __init__(self, data_path: str | Path):\n    \"\"\"Initialize data loader with data path.\n\n    Args:\n        data_path: Path to the data file (CSV, Parquet, etc.)\n\n    Raises:\n        FileNotFoundError: If the data file does not exist.\n\n    \"\"\"\n    self.data_path = Path(data_path)\n\n    if not self.data_path.exists():\n        raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataLoader-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataLoader.load","title":"<code>load() -&gt; pd.DataFrame</code>","text":"<p>Load data from the specified path.</p> <p>Returns     Loaded DataFrame</p> <p>Raises     ValueError: If the file format is not supported.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def load(self) -&gt; pd.DataFrame:\n    \"\"\"Load data from the specified path.\n\n    Returns\n        Loaded DataFrame\n\n    Raises\n        ValueError: If the file format is not supported.\n\n    \"\"\"\n    ext = self.data_path.suffix.lower().lstrip(\".\")\n    if ext not in DataLoaderConstants.ValidDataExtensions.all():\n        raise ValueError(f\"Unsupported file format: {self.data_path.suffix}\")\n\n    if ext == DataLoaderConstants.ValidDataExtensions.CSV:\n        return self._load_csv()\n    elif ext == DataLoaderConstants.ValidDataExtensions.PARQUET:\n        return self._load_parquet()\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataPipeline","title":"<code>DataPipeline(data: pd.DataFrame, control_columns: list[str] | None, channel_columns: list[str], date_column: str, response_column: str, revenue_column: str, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Data pipeline that orchestrates loading, processing, and validation.</p> <p>Provides a simple interface to go from raw data file to validated DataFrame.</p> <p>Initialize data pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the data</p> required <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column</p> required <code>response_column</code> <code>str</code> <p>Name of the response column</p> required <code>revenue_column</code> <code>str</code> <p>Name of the revenue column</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str,\n    response_column: str,\n    revenue_column: str,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize data pipeline.\n\n    Args:\n        data: DataFrame containing the data\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column\n        response_column: Name of the response column\n        revenue_column: Name of the revenue column\n        min_number_observations: Minimum required number of observations\n\n    \"\"\"\n    # Initialize components\n    self.data = data\n    self.processor = DataProcessor(\n        date_column=date_column,\n        response_column=response_column,\n        revenue_column=revenue_column,\n        control_columns=control_columns,\n        channel_columns=channel_columns,\n    )\n    self.validator = DataValidator(\n        control_columns=control_columns,\n        min_number_observations=min_number_observations,\n    )\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataPipeline-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataPipeline.run","title":"<code>run() -&gt; pd.DataFrame</code>","text":"<p>Run the complete data pipeline: process \u2192 validate.</p> <p>Returns     Validated and processed DataFrame</p> <p>Raises     Various exceptions processing or validation steps</p> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"Run the complete data pipeline: process \u2192 validate.\n\n    Returns\n        Validated and processed DataFrame\n\n    Raises\n        Various exceptions processing or validation steps\n\n    \"\"\"\n    processed_df = self.processor.process(self.data)\n\n    self.validator.run_validations(processed_df)\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataProcessor","title":"<code>DataProcessor(control_columns: list[str] | None, channel_columns: list[str], date_column: str = InputDataframeConstants.DATE_COL, response_column: str = InputDataframeConstants.RESPONSE_COL, revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL)</code>","text":"<p>Simple data processor for MMM evaluation.</p> <p>Handles data transformations like datetime casting, column renaming, etc.</p> <p>Initialize data processor.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column to parse and rename</p> <code>DATE_COL</code> <code>response_column</code> <code>str</code> <p>Name of the response column to parse and rename</p> <code>RESPONSE_COL</code> <code>revenue_column</code> <code>str</code> <p>Name of the revenue column to parse and rename</p> <code>MEDIA_CHANNEL_REVENUE_COL</code> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str = InputDataframeConstants.DATE_COL,\n    response_column: str = InputDataframeConstants.RESPONSE_COL,\n    revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,\n):\n    \"\"\"Initialize data processor.\n\n    Args:\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column to parse and rename\n        response_column: Name of the response column to parse and rename\n        revenue_column: Name of the revenue column to parse and rename\n\n    \"\"\"\n    self.date_column = date_column\n    self.response_column = response_column\n    self.revenue_column = revenue_column\n    self.control_columns = control_columns\n    self.channel_columns = channel_columns\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataProcessor-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataProcessor.process","title":"<code>process(df: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Process the DataFrame with configured transformations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>MissingRequiredColumnsError</code> <p>If the required columns are not present.</p> <code>InvalidDateFormatError</code> <p>If the date column cannot be parsed.</p> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process the DataFrame with configured transformations.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Processed DataFrame\n\n    Raises:\n        MissingRequiredColumnsError: If the required columns are not present.\n        InvalidDateFormatError: If the date column cannot be parsed.\n\n    \"\"\"\n    processed_df = df.copy()\n\n    # Validate that all required columns exist\n    self._validate_required_columns_present(\n        df=processed_df,\n        date_column=self.date_column,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n        control_columns=self.control_columns,\n        channel_columns=self.channel_columns,\n    )\n\n    # Parse date columns\n    processed_df = self._parse_date_columns(processed_df, self.date_column)\n\n    # Rename required columns\n    processed_df = self._rename_required_columns(\n        df=processed_df,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n    )\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataValidator","title":"<code>DataValidator(control_columns: list[str] | None, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Validator for MMM data with configurable validation rules.</p> <p>Initialize validator with validation rules.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations for time series CV</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize validator with validation rules.\n\n    Args:\n        control_columns: List of control columns\n        min_number_observations: Minimum required number of observations for time series CV\n\n    \"\"\"\n    self.min_number_observations = min_number_observations\n    self.control_columns = control_columns\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataValidator-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataValidator.run_validations","title":"<code>run_validations(df: pd.DataFrame) -&gt; None</code>","text":"<p>Run all validations on the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>None</code> <p>Validation result with all errors and warnings</p> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def run_validations(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Run all validations on the DataFrame.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Validation result with all errors and warnings\n\n    \"\"\"\n    # Run each validation in order\n    self._validate_not_empty(df)\n    self._validate_schema(df)\n    self._validate_data_size(df)\n\n    if self.control_columns:\n        self._check_control_variables_between_0_and_1(df=df, cols=self.control_columns)\n</code></pre>"},{"location":"api/data/#mmm_eval.data-modules","title":"Modules","text":""},{"location":"api/data/#mmm_eval.data.constants","title":"<code>constants</code>","text":"<p>Defines the constants for the data pipeline.</p>"},{"location":"api/data/#mmm_eval.data.constants-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.constants.DataLoaderConstants","title":"<code>DataLoaderConstants</code>","text":"<p>Constants for the data loader.</p> Classes\u00b6 <code>ValidDataExtensions</code> \u00b6 <p>Valid data extensions.</p> Functions\u00b6 <code>all()</code> <code>classmethod</code> \u00b6 <p>Return list of all supported file extensions.</p> Source code in <code>mmm_eval/data/constants.py</code> <pre><code>@classmethod\ndef all(cls):\n    \"\"\"Return list of all supported file extensions.\"\"\"\n    return [cls.CSV, cls.PARQUET]\n</code></pre>"},{"location":"api/data/#mmm_eval.data.constants.DataPipelineConstants","title":"<code>DataPipelineConstants</code>","text":"<p>Constants for the data pipeline.</p>"},{"location":"api/data/#mmm_eval.data.constants.InputDataframeConstants","title":"<code>InputDataframeConstants</code>","text":"<p>Constants for the dataframe.</p>"},{"location":"api/data/#mmm_eval.data.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for data validation and processing.</p>"},{"location":"api/data/#mmm_eval.data.exceptions-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.exceptions.DataValidationError","title":"<code>DataValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when data validation fails.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.EmptyDataFrameError","title":"<code>EmptyDataFrameError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when DataFrame is empty.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.InvalidDateFormatError","title":"<code>InvalidDateFormatError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when date parsing fails.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.MissingRequiredColumnsError","title":"<code>MissingRequiredColumnsError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when required columns are missing.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for validation errors.</p>"},{"location":"api/data/#mmm_eval.data.loaders","title":"<code>loaders</code>","text":"<p>Data loading utilities for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.loaders-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.loaders.DataLoader","title":"<code>DataLoader(data_path: str | Path)</code>","text":"<p>Simple data loader for MMM evaluation.</p> <p>Takes a data path and loads the data.</p> <p>Initialize data loader with data path.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str | Path</code> <p>Path to the data file (CSV, Parquet, etc.)</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the data file does not exist.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def __init__(self, data_path: str | Path):\n    \"\"\"Initialize data loader with data path.\n\n    Args:\n        data_path: Path to the data file (CSV, Parquet, etc.)\n\n    Raises:\n        FileNotFoundError: If the data file does not exist.\n\n    \"\"\"\n    self.data_path = Path(data_path)\n\n    if not self.data_path.exists():\n        raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n</code></pre> Functions\u00b6 <code>load() -&gt; pd.DataFrame</code> \u00b6 <p>Load data from the specified path.</p> <p>Returns     Loaded DataFrame</p> <p>Raises     ValueError: If the file format is not supported.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def load(self) -&gt; pd.DataFrame:\n    \"\"\"Load data from the specified path.\n\n    Returns\n        Loaded DataFrame\n\n    Raises\n        ValueError: If the file format is not supported.\n\n    \"\"\"\n    ext = self.data_path.suffix.lower().lstrip(\".\")\n    if ext not in DataLoaderConstants.ValidDataExtensions.all():\n        raise ValueError(f\"Unsupported file format: {self.data_path.suffix}\")\n\n    if ext == DataLoaderConstants.ValidDataExtensions.CSV:\n        return self._load_csv()\n    elif ext == DataLoaderConstants.ValidDataExtensions.PARQUET:\n        return self._load_parquet()\n</code></pre>"},{"location":"api/data/#mmm_eval.data.pipeline","title":"<code>pipeline</code>","text":"<p>Data pipeline for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.pipeline-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.pipeline.DataPipeline","title":"<code>DataPipeline(data: pd.DataFrame, control_columns: list[str] | None, channel_columns: list[str], date_column: str, response_column: str, revenue_column: str, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Data pipeline that orchestrates loading, processing, and validation.</p> <p>Provides a simple interface to go from raw data file to validated DataFrame.</p> <p>Initialize data pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the data</p> required <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column</p> required <code>response_column</code> <code>str</code> <p>Name of the response column</p> required <code>revenue_column</code> <code>str</code> <p>Name of the revenue column</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str,\n    response_column: str,\n    revenue_column: str,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize data pipeline.\n\n    Args:\n        data: DataFrame containing the data\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column\n        response_column: Name of the response column\n        revenue_column: Name of the revenue column\n        min_number_observations: Minimum required number of observations\n\n    \"\"\"\n    # Initialize components\n    self.data = data\n    self.processor = DataProcessor(\n        date_column=date_column,\n        response_column=response_column,\n        revenue_column=revenue_column,\n        control_columns=control_columns,\n        channel_columns=channel_columns,\n    )\n    self.validator = DataValidator(\n        control_columns=control_columns,\n        min_number_observations=min_number_observations,\n    )\n</code></pre> Functions\u00b6 <code>run() -&gt; pd.DataFrame</code> \u00b6 <p>Run the complete data pipeline: process \u2192 validate.</p> <p>Returns     Validated and processed DataFrame</p> <p>Raises     Various exceptions processing or validation steps</p> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"Run the complete data pipeline: process \u2192 validate.\n\n    Returns\n        Validated and processed DataFrame\n\n    Raises\n        Various exceptions processing or validation steps\n\n    \"\"\"\n    processed_df = self.processor.process(self.data)\n\n    self.validator.run_validations(processed_df)\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.processor","title":"<code>processor</code>","text":"<p>Data processing utilities for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.processor-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.processor.DataProcessor","title":"<code>DataProcessor(control_columns: list[str] | None, channel_columns: list[str], date_column: str = InputDataframeConstants.DATE_COL, response_column: str = InputDataframeConstants.RESPONSE_COL, revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL)</code>","text":"<p>Simple data processor for MMM evaluation.</p> <p>Handles data transformations like datetime casting, column renaming, etc.</p> <p>Initialize data processor.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column to parse and rename</p> <code>DATE_COL</code> <code>response_column</code> <code>str</code> <p>Name of the response column to parse and rename</p> <code>RESPONSE_COL</code> <code>revenue_column</code> <code>str</code> <p>Name of the revenue column to parse and rename</p> <code>MEDIA_CHANNEL_REVENUE_COL</code> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str = InputDataframeConstants.DATE_COL,\n    response_column: str = InputDataframeConstants.RESPONSE_COL,\n    revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,\n):\n    \"\"\"Initialize data processor.\n\n    Args:\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column to parse and rename\n        response_column: Name of the response column to parse and rename\n        revenue_column: Name of the revenue column to parse and rename\n\n    \"\"\"\n    self.date_column = date_column\n    self.response_column = response_column\n    self.revenue_column = revenue_column\n    self.control_columns = control_columns\n    self.channel_columns = channel_columns\n</code></pre> Functions\u00b6 <code>process(df: pd.DataFrame) -&gt; pd.DataFrame</code> \u00b6 <p>Process the DataFrame with configured transformations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>MissingRequiredColumnsError</code> <p>If the required columns are not present.</p> <code>InvalidDateFormatError</code> <p>If the date column cannot be parsed.</p> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process the DataFrame with configured transformations.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Processed DataFrame\n\n    Raises:\n        MissingRequiredColumnsError: If the required columns are not present.\n        InvalidDateFormatError: If the date column cannot be parsed.\n\n    \"\"\"\n    processed_df = df.copy()\n\n    # Validate that all required columns exist\n    self._validate_required_columns_present(\n        df=processed_df,\n        date_column=self.date_column,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n        control_columns=self.control_columns,\n        channel_columns=self.channel_columns,\n    )\n\n    # Parse date columns\n    processed_df = self._parse_date_columns(processed_df, self.date_column)\n\n    # Rename required columns\n    processed_df = self._rename_required_columns(\n        df=processed_df,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n    )\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.schemas","title":"<code>schemas</code>","text":"<p>Pydantic schemas for MMM data validation.</p>"},{"location":"api/data/#mmm_eval.data.schemas-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.schemas.ValidatedDataSchema","title":"<code>ValidatedDataSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Schema for MMM data validation.</p> <p>Defines the bare minimum columns for MMM evaluation.</p> Classes\u00b6 <code>Config</code> \u00b6 <p>Config for the schema.</p>"},{"location":"api/data/#mmm_eval.data.synth_data_generator","title":"<code>synth_data_generator</code>","text":"<p>Generate synthetic data for testing.</p> <p>Based on: https://www.pymc-marketing.io/en/stable/notebooks/mmm/mmm_example.html</p>"},{"location":"api/data/#mmm_eval.data.synth_data_generator-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.synth_data_generator.generate_data","title":"<code>generate_data()</code>","text":"<p>Generate synthetic MMM data for testing purposes.</p> <p>Returns     DataFrame containing synthetic MMM data with media channels, controls, and response variables</p> Source code in <code>mmm_eval/data/synth_data_generator.py</code> <pre><code>def generate_data():\n    \"\"\"Generate synthetic MMM data for testing purposes.\n\n    Returns\n        DataFrame containing synthetic MMM data with media channels, controls, and response variables\n\n    \"\"\"\n    seed: int = sum(map(ord, \"mmm\"))\n    rng: np.random.Generator = np.random.default_rng(seed=seed)\n\n    # date range\n    min_date = pd.to_datetime(\"2018-04-01\")\n    max_date = pd.to_datetime(\"2021-09-01\")\n\n    df = pd.DataFrame(data={\"date_week\": pd.date_range(start=min_date, end=max_date, freq=\"W-MON\")}).assign(\n        year=lambda x: x[\"date_week\"].dt.year,\n        month=lambda x: x[\"date_week\"].dt.month,\n        dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n    )\n\n    n = df.shape[0]\n\n    # media spend data\n    channel_1 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_1\"] = np.where(channel_1 &gt; 90, channel_1, channel_1 / 2)\n\n    channel_2 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_2\"] = np.where(channel_2 &gt; 80, channel_2, 0)\n\n    # apply geometric adstock transformation\n    alpha1: float = 0.4\n    alpha2: float = 0.2\n\n    df[\"channel_1_adstock\"] = (\n        geometric_adstock(x=df[\"channel_1\"].to_numpy(), alpha=alpha1, l_max=8, normalize=True).eval().flatten()\n    )\n\n    df[\"channel_2_adstock\"] = (\n        geometric_adstock(x=df[\"channel_2\"].to_numpy(), alpha=alpha2, l_max=8, normalize=True).eval().flatten()\n    )\n\n    # apply saturation transformation\n    lam1: float = 4.0\n    lam2: float = 3.0\n\n    df[\"channel_1_adstock_saturated\"] = logistic_saturation(x=df[\"channel_1_adstock\"].to_numpy(), lam=lam1).eval()\n\n    df[\"channel_2_adstock_saturated\"] = logistic_saturation(x=df[\"channel_2_adstock\"].to_numpy(), lam=lam2).eval()\n\n    # trend + seasonal\n    df[\"trend\"] = (np.linspace(start=0.0, stop=50, num=n) + 10) ** (1 / 4) - 1\n\n    df[\"cs\"] = -np.sin(2 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"cc\"] = np.cos(1 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"seasonality\"] = 0.5 * (df[\"cs\"] + df[\"cc\"])\n\n    # controls\n    df[\"event_1\"] = (df[\"date_week\"] == \"2019-05-13\").astype(float)\n    df[\"event_2\"] = (df[\"date_week\"] == \"2020-09-14\").astype(float)\n\n    # generate quantity\n    df[\"intercept\"] = 1000.0  # Base quantity\n    # noise\n    df[\"epsilon\"] = rng.normal(loc=0.0, scale=50.0, size=n)\n\n    # amplitude = 1\n    beta_1 = 400\n    beta_2 = 150\n\n    # Generate price with seasonal fluctuations\n    base_price = 5\n    price_seasonality = 0.03 * (df[\"cs\"] + df[\"cc\"])\n    price_trend = np.linspace(0, 2, n)  # Gradual price increase\n    df[\"price\"] = base_price + price_seasonality + price_trend\n\n    df[\"quantity\"] = (\n        df[\"intercept\"]\n        + df[\"trend\"] * 100\n        + df[\"seasonality\"] * 200\n        + df[\"price\"] * -50\n        + 150 * df[\"event_1\"]\n        + 250 * df[\"event_2\"]\n        + beta_1 * df[\"channel_1_adstock_saturated\"]\n        + beta_2 * df[\"channel_2_adstock_saturated\"]\n        + df[\"epsilon\"]\n    )\n    # Calculate revenue\n    df[\"revenue\"] = df[\"price\"] * df[\"quantity\"]\n\n    columns_to_keep = [\n        \"date_week\",\n        \"quantity\",\n        \"price\",\n        \"revenue\",\n        \"channel_1\",\n        \"channel_2\",\n        \"event_1\",\n        \"event_2\",\n        \"dayofyear\",\n    ]\n\n    df = df[columns_to_keep]\n    return df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.validation","title":"<code>validation</code>","text":"<p>Data validation for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.validation-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.validation.DataValidator","title":"<code>DataValidator(control_columns: list[str] | None, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Validator for MMM data with configurable validation rules.</p> <p>Initialize validator with validation rules.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations for time series CV</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize validator with validation rules.\n\n    Args:\n        control_columns: List of control columns\n        min_number_observations: Minimum required number of observations for time series CV\n\n    \"\"\"\n    self.min_number_observations = min_number_observations\n    self.control_columns = control_columns\n</code></pre> Functions\u00b6 <code>run_validations(df: pd.DataFrame) -&gt; None</code> \u00b6 <p>Run all validations on the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>None</code> <p>Validation result with all errors and warnings</p> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def run_validations(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Run all validations on the DataFrame.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Validation result with all errors and warnings\n\n    \"\"\"\n    # Run each validation in order\n    self._validate_not_empty(df)\n    self._validate_schema(df)\n    self._validate_data_size(df)\n\n    if self.control_columns:\n        self._check_control_variables_between_0_and_1(df=df, cols=self.control_columns)\n</code></pre>"},{"location":"api/metrics/","title":"Metrics Reference","text":""},{"location":"api/metrics/#mmm_eval.metrics","title":"<code>mmm_eval.metrics</code>","text":"<p>Accuracy metrics for MMM evaluation.</p>"},{"location":"api/metrics/#mmm_eval.metrics-functions","title":"Functions","text":""},{"location":"api/metrics/#mmm_eval.metrics.calculate_absolute_percentage_change","title":"<code>calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series</code>","text":"<p>Calculate the absolute percentage change between two series.</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series:\n    \"\"\"Calculate the absolute percentage change between two series.\"\"\"\n    return np.abs((comparison_series - baseline_series) / baseline_series)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_mean_for_singular_values_across_cross_validation_folds","title":"<code>calculate_mean_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the mean of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_mean_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the mean of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Mean value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.mean([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_means_for_series_across_cross_validation_folds","title":"<code>calculate_means_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the mean of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Mean Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_means_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the mean of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Mean Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).mean(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_std_for_singular_values_across_cross_validation_folds","title":"<code>calculate_std_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the standard deviation of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Standard deviation value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_std_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the standard deviation of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Standard deviation value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.std([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_stds_for_series_across_cross_validation_folds","title":"<code>calculate_stds_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the standard deviation of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Standard deviation Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_stds_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the standard deviation of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Standard deviation Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).std(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics-modules","title":"Modules","text":""},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions","title":"<code>accuracy_functions</code>","text":"<p>Accuracy metrics for MMM evaluation.</p>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions-functions","title":"Functions","text":""},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_absolute_percentage_change","title":"<code>calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series</code>","text":"<p>Calculate the absolute percentage change between two series.</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series:\n    \"\"\"Calculate the absolute percentage change between two series.\"\"\"\n    return np.abs((comparison_series - baseline_series) / baseline_series)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_mean_for_singular_values_across_cross_validation_folds","title":"<code>calculate_mean_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the mean of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_mean_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the mean of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Mean value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.mean([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_means_for_series_across_cross_validation_folds","title":"<code>calculate_means_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the mean of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Mean Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_means_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the mean of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Mean Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).mean(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_std_for_singular_values_across_cross_validation_folds","title":"<code>calculate_std_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the standard deviation of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Standard deviation value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_std_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the standard deviation of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Standard deviation value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.std([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_stds_for_series_across_cross_validation_folds","title":"<code>calculate_stds_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the standard deviation of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Standard deviation Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_stds_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the standard deviation of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Standard deviation Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).std(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models","title":"<code>metric_models</code>","text":""},{"location":"api/metrics/#mmm_eval.metrics.metric_models-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.metric_models.AccuracyMetricNames","title":"<code>AccuracyMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the accuracy metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.AccuracyMetricResults","title":"<code>AccuracyMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the accuracy metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    return self.mape &lt;= AccuracyThresholdConstants.MAPE and self.r_squared &gt;= AccuracyThresholdConstants.R_SQUARED\n</code></pre> <code>populate_object_with_metrics(actual: pd.Series, predicted: pd.Series) -&gt; AccuracyMetricResults</code> <code>classmethod</code> \u00b6 <p>Populate the object with the calculated metrics.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>Series</code> <p>The actual values</p> required <code>predicted</code> <code>Series</code> <p>The predicted values</p> required <p>Returns:</p> Type Description <code>AccuracyMetricResults</code> <p>AccuracyMetricResults object with the metrics</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>@classmethod\ndef populate_object_with_metrics(cls, actual: pd.Series, predicted: pd.Series) -&gt; \"AccuracyMetricResults\":\n    \"\"\"Populate the object with the calculated metrics.\n\n    Args:\n        actual: The actual values\n        predicted: The predicted values\n\n    Returns:\n        AccuracyMetricResults object with the metrics\n\n    \"\"\"\n    return cls(\n        mape=mean_absolute_percentage_error(actual, predicted),\n        r_squared=r2_score(actual, predicted),\n    )\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.CrossValidationMetricNames","title":"<code>CrossValidationMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the cross-validation metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.CrossValidationMetricResults","title":"<code>CrossValidationMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the cross-validation metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    return (\n        self.mean_mape &lt;= CrossValidationThresholdConstants.MEAN_MAPE\n        and self.std_mape &lt;= CrossValidationThresholdConstants.STD_MAPE\n        and self.mean_r_squared &gt;= CrossValidationThresholdConstants.MEAN_R_SQUARED\n    )\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.MetricNamesBase","title":"<code>MetricNamesBase</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Base class for metric name enums.</p> Functions\u00b6 <code>metrics_to_list() -&gt; list[str]</code> <code>classmethod</code> \u00b6 <p>Convert the enum to a list of strings.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>@classmethod\ndef metrics_to_list(cls) -&gt; list[str]:\n    \"\"\"Convert the enum to a list of strings.\"\"\"\n    return [member.value for member in cls]\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.MetricResults","title":"<code>MetricResults</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Define the results of the metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    raise NotImplementedError(\"Child classes must implement test_passed()\")\n</code></pre> <code>to_dict() -&gt; dict[str, Any]</code> \u00b6 <p>Convert the class of test results to dictionary format.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the class of test results to dictionary format.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.PerturbationMetricNames","title":"<code>PerturbationMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the perturbation metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.PerturbationMetricResults","title":"<code>PerturbationMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the perturbation metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    return bool(\n        (self.percentage_change_for_each_channel &lt;= PerturbationThresholdConstants.MEAN_PERCENTAGE_CHANGE).all()\n    )\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.RefreshStabilityMetricNames","title":"<code>RefreshStabilityMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the stability metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.RefreshStabilityMetricResults","title":"<code>RefreshStabilityMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the refresh stability metrics.</p> Functions\u00b6 <code>check_test_passed() -&gt; bool</code> \u00b6 <p>Check if the tests passed.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def check_test_passed(self) -&gt; bool:\n    \"\"\"Check if the tests passed.\"\"\"\n    return bool(\n        (\n            self.mean_percentage_change_for_each_channel\n            &lt;= RefreshStabilityThresholdConstants.MEAN_PERCENTAGE_CHANGE\n        ).all()\n    )\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants","title":"<code>threshold_constants</code>","text":""},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.AccuracyThresholdConstants","title":"<code>AccuracyThresholdConstants</code>","text":"<p>Constants for the accuracy threshold.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.CrossValidationThresholdConstants","title":"<code>CrossValidationThresholdConstants</code>","text":"<p>Constants for the cross-validation threshold.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.PerturbationThresholdConstants","title":"<code>PerturbationThresholdConstants</code>","text":"<p>Constants for the perturbation threshold.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.RefreshStabilityThresholdConstants","title":"<code>RefreshStabilityThresholdConstants</code>","text":"<p>Constants for the refresh stability threshold.</p>"},{"location":"development/contributing/","title":"Contributing to mmm-eval","text":"<p>We welcome contributions from the community! This guide will help you get started with contributing to mmm-eval.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ol> <li>Python 3.11+ - Required for development</li> <li>Git - For version control</li> <li>Poetry - For dependency management (recommended)</li> </ol>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository on GitHub</li> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/YOUR_USERNAME/mmm-eval.git\ncd mmm-eval\n</code></pre></p> </li> <li> <p>Set up the development environment:    <pre><code># Install dependencies\npoetry install\n\n# Activate the environment\npoetry shell\n</code></pre></p> </li> <li> <p>Install pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write your code following the coding standards</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> </ul>"},{"location":"development/contributing/#3-test-your-changes","title":"3. Test Your Changes","text":"<pre><code># Run all tests\ntox\n\n# Run specific test categories\npytest tests/unit/\npytest tests/integration/\n\n# Run linting and formatting\nblack mmm_eval tests\nisort mmm_eval tests\nruff check mmm_eval tests\n</code></pre>"},{"location":"development/contributing/#4-commit-your-changes","title":"4. Commit Your Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add new feature description\"\n</code></pre> <p>Follow the conventional commits format: - <code>feat:</code> - New features - <code>fix:</code> - Bug fixes - <code>docs:</code> - Documentation changes - <code>style:</code> - Code style changes - <code>refactor:</code> - Code refactoring - <code>test:</code> - Test changes - <code>chore:</code> - Maintenance tasks</p>"},{"location":"development/contributing/#5-push-and-create-a-pull-request","title":"5. Push and Create a Pull Request","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"development/contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"development/contributing/#python-code-style","title":"Python Code Style","text":"<p>We use several tools to maintain code quality:</p> <ul> <li>Black - Code formatting</li> <li>isort - Import sorting</li> <li>Ruff - Linting and additional checks</li> <li>Pyright - Type checking</li> </ul>"},{"location":"development/contributing/#code-formatting","title":"Code Formatting","text":"<pre><code># Format code\nblack mmm_eval tests\n\n# Sort imports\nisort mmm_eval tests\n\n# Run linting\nruff check mmm_eval tests\nruff check --fix mmm_eval tests\n</code></pre>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>We use type hints throughout the codebase:</p> <pre><code>from typing import List, Optional, Dict, Any\n\ndef process_data(data: List[Dict[str, Any]]) -&gt; Optional[Dict[str, float]]:\n    \"\"\"Process the input data and return results.\"\"\"\n    pass\n</code></pre>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def calculate_mape(actual: List[float], predicted: List[float]) -&gt; float:\n    \"\"\"Calculate Mean Absolute Percentage Error.\n\n    Args:\n        actual: List of actual values\n        predicted: List of predicted values\n\n    Returns:\n        MAPE value as a float\n\n    Raises:\n        ValueError: If inputs are empty or have different lengths\n    \"\"\"\n    pass\n</code></pre>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=mmm_eval\n\n# Run specific test file\npytest tests/test_metrics.py\n\n# Run tests in parallel\npytest -n auto\n</code></pre>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Use descriptive test names</li> <li>Test both success and failure cases</li> <li>Use fixtures for common test data</li> </ul> <p>Example test:</p> <pre><code>import pytest\nfrom mmm_eval.metrics import calculate_mape\n\ndef test_calculate_mape_basic():\n    \"\"\"Test basic MAPE calculation.\"\"\"\n    actual = [100, 200, 300]\n    predicted = [110, 190, 310]\n\n    mape = calculate_mape(actual, predicted)\n\n    assert isinstance(mape, float)\n    assert mape &gt; 0\n\ndef test_calculate_mape_empty_input():\n    \"\"\"Test MAPE calculation with empty input.\"\"\"\n    with pytest.raises(ValueError):\n        calculate_mape([], [])\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#updating-documentation","title":"Updating Documentation","text":"<ol> <li>Update docstrings in the code</li> <li>Update markdown files in the <code>docs/</code> directory</li> <li>Build and test documentation:    <pre><code>mkdocs serve\n</code></pre></li> </ol>"},{"location":"development/contributing/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Keep documentation up to date with code changes</li> <li>Use proper markdown formatting</li> </ul>"},{"location":"development/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Ensure all tests pass</li> <li>Update documentation if needed</li> <li>Add tests for new functionality</li> <li>Follow coding standards</li> <li>Update CHANGELOG.md if adding new features</li> </ol>"},{"location":"development/contributing/#pull-request-template","title":"Pull Request Template","text":"<p>Use the provided pull request template and fill in all sections:</p> <ul> <li>Description - What does this PR do?</li> <li>Type of change - Bug fix, feature, documentation, etc.</li> <li>Testing - How was this tested?</li> <li>Checklist - Ensure all items are completed</li> </ul>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated checks must pass</li> <li>Code review by maintainers</li> <li>Documentation review if needed</li> <li>Merge after approval</li> </ol>"},{"location":"development/contributing/#issue-reporting","title":"Issue Reporting","text":""},{"location":"development/contributing/#before-creating-an-issue","title":"Before Creating an Issue","text":"<ol> <li>Search existing issues to avoid duplicates</li> <li>Check documentation for solutions</li> <li>Try the latest version of mmm-eval</li> </ol>"},{"location":"development/contributing/#issue-template","title":"Issue Template","text":"<p>Use the provided issue template and include:</p> <ul> <li>Description - Clear description of the problem</li> <li>Steps to reproduce - Detailed steps</li> <li>Expected behavior - What should happen</li> <li>Actual behavior - What actually happens</li> <li>Environment - OS, Python version, mmm-eval version</li> <li>Additional context - Any other relevant information</li> </ul>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive environment. Please:</p> <ul> <li>Be respectful and inclusive</li> <li>Use welcoming and inclusive language</li> <li>Be collaborative and constructive</li> <li>Focus on what is best for the community</li> </ul>"},{"location":"development/contributing/#communication","title":"Communication","text":"<ul> <li>GitHub Issues - For bug reports and feature requests</li> <li>GitHub Discussions - For questions and general discussion</li> <li>Pull Requests - For code contributions</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with contributing:</p> <ol> <li>Check the documentation first</li> <li>Search existing issues and discussions</li> <li>Create a new discussion for questions</li> <li>Join our community channels</li> </ol>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in:</p> <ul> <li>README.md - For significant contributions</li> <li>CHANGELOG.md - For all contributions</li> <li>GitHub contributors page</li> </ul> <p>Thank you for contributing to mmm-eval! \ud83c\udf89 </p>"},{"location":"development/setup/","title":"Development Setup","text":"<p>This guide will help you set up a development environment for contributing to mmm-eval.</p>"},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<p>Before setting up the development environment, ensure you have the following installed:</p> <ul> <li>Python 3.11+: The minimum supported Python version</li> <li>Poetry 2.x.x: For dependency management and packaging</li> <li>Git: For version control</li> </ul>"},{"location":"development/setup/#quick-setup","title":"Quick Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>poetry install\n</code></pre></p> </li> <li> <p>Verify the installation:    <pre><code>poetry run mmm-eval --help\n</code></pre></p> </li> </ol>"},{"location":"development/setup/#development-environment","title":"Development Environment","text":""},{"location":"development/setup/#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<p>Poetry automatically creates and manages a virtual environment for the project:</p> <pre><code># Activate the virtual environment\npoetry shell\n\n# Run commands within the environment\npoetry run python -m pytest\n\n# Install additional development dependencies\npoetry add --group dev package-name\n</code></pre>"},{"location":"development/setup/#using-pip-alternative","title":"Using pip (Alternative)","text":"<p>If you prefer using pip directly:</p> <pre><code># Create a virtual environment\npython -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -r requirements-dev.txt  # If available\n</code></pre>"},{"location":"development/setup/#project-structure","title":"Project Structure","text":"<pre><code>mmm-eval/\n\u251c\u2500\u2500 mmm_eval/              # Main package\n\u2502   \u251c\u2500\u2500 adapters/          # Framework adapters\n\u2502   \u251c\u2500\u2500 cli/               # Command-line interface\n\u2502   \u251c\u2500\u2500 core/              # Core evaluation logic\n\u2502   \u251c\u2500\u2500 data/              # Data handling and validation\n\u2502   \u2514\u2500\u2500 metrics/           # Evaluation metrics\n\u251c\u2500\u2500 tests/                 # Test suite\n\u251c\u2500\u2500 docs/                  # Documentation\n\u251c\u2500\u2500 pyproject.toml         # Project configuration\n\u2514\u2500\u2500 README.md             # Project overview\n</code></pre>"},{"location":"development/setup/#code-quality-tools","title":"Code Quality Tools","text":"<p>The project uses several tools to maintain code quality:</p>"},{"location":"development/setup/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks to automatically format and lint your code:</p> <pre><code># Install pre-commit\npoetry add --group dev pre-commit\n\n# Install the git hook scripts\npre-commit install\n\n# Run against all files\npre-commit run --all-files\n</code></pre>"},{"location":"development/setup/#code-formatting","title":"Code Formatting","text":"<p>The project uses Black for code formatting:</p> <pre><code># Format all Python files\npoetry run black .\n\n# Check formatting without making changes\npoetry run black --check .\n</code></pre>"},{"location":"development/setup/#import-sorting","title":"Import Sorting","text":"<p>isort is used to organize imports:</p> <pre><code># Sort imports\npoetry run isort .\n\n# Check import sorting\npoetry run isort --check-only .\n</code></pre>"},{"location":"development/setup/#linting","title":"Linting","text":"<p>Multiple linters are configured:</p> <pre><code># Run all linters\npoetry run ruff check .\npoetry run flake8 .\n\n# Auto-fix issues where possible\npoetry run ruff check --fix .\n</code></pre>"},{"location":"development/setup/#type-checking","title":"Type Checking","text":"<p>Pyright is used for static type checking:</p> <pre><code># Run type checker\npoetry run pyright\n</code></pre>"},{"location":"development/setup/#running-tests","title":"Running Tests","text":""},{"location":"development/setup/#unit-tests","title":"Unit Tests","text":"<pre><code># Run all tests\npoetry run pytest\n\n# Run tests with coverage\npoetry run pytest --cov=mmm_eval\n\n# Run tests in parallel\npoetry run pytest -n auto\n\n# Run specific test file\npoetry run pytest tests/test_core.py\n</code></pre>"},{"location":"development/setup/#integration-tests","title":"Integration Tests","text":"<pre><code># Run integration tests\npoetry run pytest -m integration\n</code></pre>"},{"location":"development/setup/#test-coverage","title":"Test Coverage","text":"<pre><code># Generate coverage report\npoetry run pytest --cov=mmm_eval --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html  # On macOS\n# or\nstart htmlcov/index.html  # On Windows\n</code></pre>"},{"location":"development/setup/#documentation","title":"Documentation","text":""},{"location":"development/setup/#building-documentation","title":"Building Documentation","text":"<pre><code># Install documentation dependencies\npoetry install --with docs\n\n# Build documentation\npoetry run mkdocs build\n\n# Serve documentation locally\npoetry run mkdocs serve\n</code></pre> <p>The documentation will be available at <code>http://localhost:8000</code>.</p>"},{"location":"development/setup/#api-documentation","title":"API Documentation","text":"<p>API documentation is automatically generated from docstrings using mkdocstrings. To update the API docs:</p> <ol> <li>Ensure your code has proper docstrings</li> <li>Build the documentation: <code>poetry run mkdocs build</code></li> <li>The API reference will be updated automatically</li> </ol>"},{"location":"development/setup/#ide-configuration","title":"IDE Configuration","text":""},{"location":"development/setup/#vs-code","title":"VS Code","text":"<p>Recommended VS Code extensions:</p> <ul> <li>Python</li> <li>Pylance</li> <li>Black Formatter</li> <li>isort</li> <li>Ruff</li> </ul> <p>Add to your <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"python.defaultInterpreterPath\": \"./venv/bin/python\",\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.ruffEnabled\": true,\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.organizeImports\": true\n    }\n}\n</code></pre>"},{"location":"development/setup/#pycharm","title":"PyCharm","text":"<ol> <li>Open the project in PyCharm</li> <li>Configure the Python interpreter to use the Poetry virtual environment</li> <li>Enable auto-import organization</li> <li>Configure Black as the code formatter</li> </ol>"},{"location":"development/setup/#common-issues","title":"Common Issues","text":""},{"location":"development/setup/#poetry-installation-issues","title":"Poetry Installation Issues","text":"<p>If you encounter issues with Poetry:</p> <pre><code># Update Poetry to the latest version\npoetry self update\n\n# Clear Poetry cache\npoetry cache clear --all pypi\n</code></pre>"},{"location":"development/setup/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>If you encounter dependency conflicts:</p> <pre><code># Update all dependencies\npoetry update\n\n# Remove and reinstall dependencies\npoetry lock --no-update\npoetry install\n</code></pre>"},{"location":"development/setup/#test-failures","title":"Test Failures","text":"<p>If tests are failing:</p> <ol> <li>Ensure you're using the correct Python version (3.11+)</li> <li>Check that all dependencies are installed: <code>poetry install</code></li> <li>Run tests with verbose output: <code>poetry run pytest -v</code></li> <li>Check for any environment-specific issues</li> </ol>"},{"location":"development/setup/#next-steps","title":"Next Steps","text":"<p>Once your development environment is set up:</p> <ol> <li>Read the Contributing Guide for guidelines</li> <li>Check the Testing Guide for testing practices</li> <li>Look at existing issues and pull requests</li> <li>Start with a small contribution to get familiar with the codebase</li> </ol>"},{"location":"development/setup/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during setup:</p> <ol> <li>Check the GitHub Issues</li> <li>Review the Contributing Guide</li> <li>Ask questions in the project discussions </li> </ol>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>This guide covers testing practices and procedures for the mmm-eval project.</p>"},{"location":"development/testing/#testing-philosophy","title":"Testing Philosophy","text":"<p>We follow these testing principles:</p> <ul> <li>Comprehensive coverage: Aim for high test coverage across all modules</li> <li>Fast feedback: Tests should run quickly to enable rapid development</li> <li>Reliable: Tests should be deterministic and not flaky</li> <li>Maintainable: Tests should be easy to understand and modify</li> <li>Realistic: Tests should reflect real-world usage patterns</li> </ul>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<p>The test suite is organized as follows:</p> <pre><code>tests/\n\u251c\u2500\u2500 unit/                    # Unit tests for individual components\n\u2502   \u251c\u2500\u2500 test_core/          # Core functionality tests\n\u2502   \u251c\u2500\u2500 test_adapters/      # Framework adapter tests\n\u2502   \u251c\u2500\u2500 test_data/          # Data handling tests\n\u2502   \u2514\u2500\u2500 test_metrics/       # Metrics calculation tests\n\u251c\u2500\u2500 integration/            # Integration tests\n\u251c\u2500\u2500 fixtures/               # Test data and fixtures\n\u2514\u2500\u2500 conftest.py            # Pytest configuration and shared fixtures\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npoetry run pytest\n\n# Run tests with verbose output\npoetry run pytest -v\n\n# Run tests with coverage\npoetry run pytest --cov=mmm_eval\n\n# Run tests in parallel\npoetry run pytest -n auto\n</code></pre>"},{"location":"development/testing/#running-specific-test-categories","title":"Running Specific Test Categories","text":"<pre><code># Run only unit tests\npoetry run pytest tests/unit/\n\n# Run only integration tests\npoetry run pytest tests/integration/\n\n# Run tests for a specific module\npoetry run pytest tests/unit/test_core/\n\n# Run tests matching a pattern\npoetry run pytest -k \"test_accuracy\"\n</code></pre>"},{"location":"development/testing/#running-tests-with-markers","title":"Running Tests with Markers","text":"<pre><code># Run integration tests only\npoetry run pytest -m integration\n\n# Run slow tests only\npoetry run pytest -m slow\n\n# Skip slow tests\npoetry run pytest -m \"not slow\"\n</code></pre>"},{"location":"development/testing/#test-types","title":"Test Types","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Unit tests verify individual functions and classes in isolation. They should:</p> <ul> <li>Test one specific behavior or functionality</li> <li>Use mocks for external dependencies</li> <li>Be fast and deterministic</li> <li>Have clear, descriptive names</li> </ul> <p>Example unit test:</p> <pre><code>def test_calculate_mape_returns_correct_value():\n    \"\"\"Test that MAPE calculation returns expected results.\"\"\"\n    actual = [100, 200, 300]\n    predicted = [110, 190, 310]\n\n    result = calculate_mape(actual, predicted)\n\n    expected = 10.0  # 10% average error\n    assert result == pytest.approx(expected, rel=1e-2)\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests verify that multiple components work together correctly. They:</p> <ul> <li>Test the interaction between different modules</li> <li>Use real data and minimal mocking</li> <li>May take longer to run</li> <li>Are marked with the <code>@pytest.mark.integration</code> decorator</li> </ul> <p>Example integration test:</p> <pre><code>@pytest.mark.integration\ndef test_pymc_marketing_evaluation_workflow():\n    \"\"\"Test complete PyMC Marketing evaluation workflow.\"\"\"\n    # Setup test data\n    data = load_test_data()\n\n    # Run evaluation\n    result = evaluate_framework(\n        data=data,\n        framework=\"pymc-marketing\",\n        config=test_config\n    )\n\n    # Verify results\n    assert result.accuracy &gt; 0.8\n    assert result.cross_validation_score &gt; 0.7\n    assert result.refresh_stability &gt; 0.6\n</code></pre>"},{"location":"development/testing/#property-based-tests","title":"Property-Based Tests","text":"<p>For complex calculations, we use property-based testing with Hypothesis:</p> <pre><code>from hypothesis import given, strategies as st\n\n@given(\n    actual=st.lists(st.floats(min_value=0.1, max_value=1000), min_size=1),\n    predicted=st.lists(st.floats(min_value=0.1, max_value=1000), min_size=1)\n)\ndef test_mape_properties(actual, predicted):\n    \"\"\"Test MAPE calculation properties.\"\"\"\n    if len(actual) == len(predicted):\n        mape = calculate_mape(actual, predicted)\n\n        # MAPE should always be non-negative\n        assert mape &gt;= 0\n\n        # MAPE should be 0 when predictions are perfect\n        if actual == predicted:\n            assert mape == 0\n</code></pre>"},{"location":"development/testing/#test-data-and-fixtures","title":"Test Data and Fixtures","text":""},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<p>Pytest fixtures provide reusable test data and setup:</p> <pre><code>@pytest.fixture\ndef sample_mmm_data():\n    \"\"\"Provide sample MMM data for testing.\"\"\"\n    return pd.DataFrame({\n        'date': pd.date_range('2023-01-01', periods=100),\n        'sales': np.random.normal(1000, 100, 100),\n        'tv_spend': np.random.uniform(0, 1000, 100),\n        'radio_spend': np.random.uniform(0, 500, 100),\n        'digital_spend': np.random.uniform(0, 800, 100)\n    })\n\ndef test_data_validation(sample_mmm_data):\n    \"\"\"Test data validation with sample data.\"\"\"\n    validator = DataValidator()\n    result = validator.validate(sample_mmm_data)\n    assert result.is_valid\n</code></pre>"},{"location":"development/testing/#test-data-management","title":"Test Data Management","text":"<ul> <li>Store test data in <code>tests/fixtures/</code></li> <li>Use realistic but synthetic data</li> <li>Keep test data files small and focused</li> <li>Document the structure and purpose of test data</li> </ul>"},{"location":"development/testing/#mocking-and-stubbing","title":"Mocking and Stubbing","text":""},{"location":"development/testing/#when-to-mock","title":"When to Mock","text":"<p>Mock external dependencies to:</p> <ul> <li>Speed up tests</li> <li>Avoid network calls</li> <li>Control test conditions</li> <li>Test error scenarios</li> </ul>"},{"location":"development/testing/#mocking-examples","title":"Mocking Examples","text":"<pre><code>from unittest.mock import Mock, patch\n\ndef test_api_call_with_mock():\n    \"\"\"Test API call with mocked response.\"\"\"\n    with patch('requests.get') as mock_get:\n        mock_get.return_value.json.return_value = {'status': 'success'}\n        mock_get.return_value.status_code = 200\n\n        result = fetch_data_from_api()\n\n        assert result['status'] == 'success'\n        mock_get.assert_called_once()\n</code></pre>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":""},{"location":"development/testing/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Minimum coverage: 80% for all modules</li> <li>Target coverage: 90% for critical modules</li> <li>Critical modules: Core evaluation logic, data validation, metrics calculation</li> </ul>"},{"location":"development/testing/#coverage-reports","title":"Coverage Reports","text":"<pre><code># Generate HTML coverage report\npoetry run pytest --cov=mmm_eval --cov-report=html\n\n# Generate XML coverage report (for CI)\npoetry run pytest --cov=mmm_eval --cov-report=xml\n\n# View coverage summary\npoetry run pytest --cov=mmm_eval --cov-report=term-missing\n</code></pre>"},{"location":"development/testing/#coverage-configuration","title":"Coverage Configuration","text":"<p>Configure coverage in <code>pyproject.toml</code>:</p> <pre><code>[tool.coverage.run]\nsource = [\"mmm_eval\"]\nomit = [\n    \"*/tests/*\",\n    \"*/test_*\",\n    \"*/__pycache__/*\"\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"if self.debug:\",\n    \"if settings.DEBUG\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n    \"if 0:\",\n    \"if __name__ == .__main__.:\",\n    \"class .*\\\\bProtocol\\\\):\",\n    \"@(abc\\\\.)?abstractmethod\"\n]\n</code></pre>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"development/testing/#benchmark-tests","title":"Benchmark Tests","text":"<p>For performance-critical code, use benchmark tests:</p> <pre><code>def test_mape_calculation_performance(benchmark):\n    \"\"\"Benchmark MAPE calculation performance.\"\"\"\n    actual = np.random.normal(1000, 100, 10000)\n    predicted = np.random.normal(1000, 100, 10000)\n\n    result = benchmark(lambda: calculate_mape(actual, predicted))\n\n    assert result &gt; 0\n</code></pre>"},{"location":"development/testing/#memory-usage-tests","title":"Memory Usage Tests","text":"<p>Monitor memory usage in tests:</p> <pre><code>import psutil\nimport os\n\ndef test_memory_usage():\n    \"\"\"Test that operations don't use excessive memory.\"\"\"\n    process = psutil.Process(os.getpid())\n    initial_memory = process.memory_info().rss\n\n    # Run memory-intensive operation\n    result = process_large_dataset()\n\n    final_memory = process.memory_info().rss\n    memory_increase = final_memory - initial_memory\n\n    # Memory increase should be reasonable (&lt; 100MB)\n    assert memory_increase &lt; 100 * 1024 * 1024\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions","title":"GitHub Actions","text":"<p>Tests run automatically on:</p> <ul> <li>Every pull request</li> <li>Every push to main branch</li> <li>Scheduled runs (nightly)</li> </ul>"},{"location":"development/testing/#ci-configuration","title":"CI Configuration","text":"<p>The CI pipeline includes:</p> <ol> <li>Linting: Code style and quality checks</li> <li>Type checking: Static type analysis</li> <li>Unit tests: Fast feedback on basic functionality</li> <li>Integration tests: Verify component interactions</li> <li>Coverage reporting: Track test coverage trends</li> </ol>"},{"location":"development/testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks to catch issues early:</p> <pre><code># Install pre-commit\npoetry add --group dev pre-commit\n\n# Install hooks\npre-commit install\n\n# Run all hooks\npre-commit run --all-files\n</code></pre>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":""},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code># Run with maximum verbosity\npoetry run pytest -vvv\n\n# Show local variables on failures\npoetry run pytest -l\n\n# Stop on first failure\npoetry run pytest -x\n</code></pre>"},{"location":"development/testing/#debugging-with-pdb","title":"Debugging with pdb","text":"<pre><code>def test_debug_example():\n    \"\"\"Example of using pdb for debugging.\"\"\"\n    import pdb; pdb.set_trace()  # Breakpoint\n    result = complex_calculation()\n    assert result &gt; 0\n</code></pre>"},{"location":"development/testing/#test-isolation","title":"Test Isolation","text":"<p>Ensure tests don't interfere with each other:</p> <pre><code>@pytest.fixture(autouse=True)\ndef reset_global_state():\n    \"\"\"Reset global state before each test.\"\"\"\n    # Setup\n    yield\n    # Teardown\n    cleanup_global_state()\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":""},{"location":"development/testing/#test-naming","title":"Test Naming","text":"<ul> <li>Use descriptive test names that explain the expected behavior</li> <li>Follow the pattern: <code>test_[function]_[scenario]_[expected_result]</code></li> <li>Include edge cases and error conditions</li> </ul>"},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<ul> <li>Group related tests in classes</li> <li>Use fixtures for common setup</li> <li>Keep tests focused and single-purpose</li> </ul>"},{"location":"development/testing/#assertions","title":"Assertions","text":"<ul> <li>Use specific assertions (<code>assert result == expected</code>)</li> <li>Avoid complex logic in assertions</li> <li>Use appropriate assertion methods (<code>assertIn</code>, <code>assertRaises</code>, etc.)</li> </ul>"},{"location":"development/testing/#test-data","title":"Test Data","text":"<ul> <li>Use realistic test data</li> <li>Avoid hardcoded magic numbers</li> <li>Document test data assumptions</li> </ul>"},{"location":"development/testing/#documentation","title":"Documentation","text":"<ul> <li>Write clear docstrings for test functions</li> <li>Explain complex test scenarios</li> <li>Document test data sources and assumptions</li> </ul>"},{"location":"development/testing/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"development/testing/#flaky-tests","title":"Flaky Tests","text":"<p>Avoid flaky tests by:</p> <ul> <li>Not relying on timing or external services</li> <li>Using deterministic random seeds</li> <li>Properly mocking external dependencies</li> <li>Avoiding shared state between tests</li> </ul>"},{"location":"development/testing/#slow-tests","title":"Slow Tests","text":"<p>Keep tests fast by:</p> <ul> <li>Using appropriate mocks</li> <li>Minimizing I/O operations</li> <li>Using efficient test data</li> <li>Running tests in parallel when possible</li> </ul>"},{"location":"development/testing/#over-mocking","title":"Over-Mocking","text":"<p>Don't over-mock:</p> <ul> <li>Test the actual behavior, not the implementation</li> <li>Mock only external dependencies</li> <li>Use real objects when possible</li> </ul>"},{"location":"development/testing/#getting-help","title":"Getting Help","text":"<p>If you encounter testing issues:</p> <ol> <li>Check the pytest documentation</li> <li>Review existing tests for examples</li> <li>Ask questions in project discussions</li> <li>Consult the Contributing Guide </li> </ol>"},{"location":"examples/basic-usage/","title":"Basic Usage Examples","text":"<p>This guide provides practical examples of how to use mmm-eval for different scenarios.</p>"},{"location":"examples/basic-usage/#example-1-basic-evaluation","title":"Example 1: Basic Evaluation","text":"<p>The simplest way to run an evaluation:</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing\n</code></pre> <p>This assumes your data has standard column names: - <code>date</code> - Date column - <code>sales</code> - Target variable - <code>tv_spend</code>, <code>digital_spend</code>, <code>print_spend</code> - Media channels</p>"},{"location":"examples/basic-usage/#example-2-custom-column-names","title":"Example 2: Custom Column Names","text":"<p>If your data uses different column names:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --date-column timestamp \\\n  --target-column revenue \\\n  --media-columns television,online,radio \\\n  --control-columns price,holiday\n</code></pre>"},{"location":"examples/basic-usage/#example-3-configuration-file","title":"Example 3: Configuration File","text":"<p>For more complex setups, use a configuration file:</p> <pre><code>{\n  \"data\": {\n    \"date_column\": \"date\",\n    \"target_column\": \"sales\",\n    \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\"],\n    \"date_format\": \"%Y-%m-%d\"\n  },\n  \"tests\": {\n    \"accuracy\": {\n      \"train_test_split\": 0.8,\n      \"random_state\": 42\n    },\n    \"cross_validation\": {\n      \"folds\": 5,\n      \"random_state\": 42\n    }\n  },\n  \"output\": {\n    \"include_plots\": true,\n    \"plot_format\": \"png\"\n  }\n}\n</code></pre> <p>Save as <code>config.json</code> and run:</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json\n</code></pre>"},{"location":"examples/basic-usage/#example-4-specific-tests-only","title":"Example 4: Specific Tests Only","text":"<p>Run only certain tests:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --test-names accuracy,cross_validation\n</code></pre>"},{"location":"examples/basic-usage/#example-5-custom-output-directory","title":"Example 5: Custom Output Directory","text":"<p>Save results to a specific directory:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --output-path ./my_results/ \\\n  --include-plots\n</code></pre>"},{"location":"examples/basic-usage/#example-6-reproducible-results","title":"Example 6: Reproducible Results","text":"<p>Set a random seed for reproducible results:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --random-state 42 \\\n  --train-test-split 0.8\n</code></pre>"},{"location":"examples/basic-usage/#example-7-different-output-formats","title":"Example 7: Different Output Formats","text":"<p>Save results in CSV format:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --output-format csv\n</code></pre>"},{"location":"examples/basic-usage/#example-8-verbose-output","title":"Example 8: Verbose Output","text":"<p>Get detailed information during execution:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --verbose\n</code></pre>"},{"location":"examples/basic-usage/#example-9-environment-variables","title":"Example 9: Environment Variables","text":"<p>Set configuration using environment variables:</p> <pre><code>export MMM_EVAL_DATE_COLUMN=date\nexport MMM_EVAL_TARGET_COLUMN=sales\nexport MMM_EVAL_MEDIA_COLUMNS=tv_spend,digital_spend,print_spend\nexport MMM_EVAL_TRAIN_TEST_SPLIT=0.8\nexport MMM_EVAL_RANDOM_STATE=42\n\nmmm-eval --input-data-path data.csv --framework pymc-marketing\n</code></pre>"},{"location":"examples/basic-usage/#example-10-complete-workflow","title":"Example 10: Complete Workflow","text":"<p>A complete example with all options:</p> <pre><code>mmm-eval \\\n  --input-data-path marketing_data.csv \\\n  --framework pymc-marketing \\\n  --config-path evaluation_config.json \\\n  --date-column date \\\n  --target-column sales \\\n  --media-columns tv_spend,digital_spend,print_spend,radio_spend \\\n  --control-columns price,seasonality,holiday \\\n  --test-names accuracy,cross_validation,refresh_stability,perturbation \\\n  --train-test-split 0.8 \\\n  --cv-folds 5 \\\n  --random-state 42 \\\n  --output-path ./evaluation_results/ \\\n  --output-format json \\\n  --include-plots \\\n  --plot-format png \\\n  --verbose\n</code></pre>"},{"location":"examples/basic-usage/#data-format-examples","title":"Data Format Examples","text":""},{"location":"examples/basic-usage/#basic-csv-structure","title":"Basic CSV Structure","text":"<pre><code>date,sales,tv_spend,digital_spend,print_spend,price\n2023-01-01,1000,5000,2000,1000,10.99\n2023-01-02,1200,5500,2200,1100,10.99\n2023-01-03,1100,5200,2100,1050,11.99\n...\n</code></pre>"},{"location":"examples/basic-usage/#with-control-variables","title":"With Control Variables","text":"<pre><code>date,sales,tv_spend,digital_spend,print_spend,price,seasonality,holiday\n2023-01-01,1000,5000,2000,1000,10.99,0.8,0\n2023-01-02,1200,5500,2200,1100,10.99,0.9,0\n2023-01-03,1100,5200,2100,1050,11.99,0.7,1\n...\n</code></pre>"},{"location":"examples/basic-usage/#expected-output","title":"Expected Output","text":"<p>After running an evaluation, you'll find:</p> <pre><code>evaluation_results/\n\u251c\u2500\u2500 results.json              # Detailed results\n\u251c\u2500\u2500 results_summary.csv       # Summary metrics\n\u2514\u2500\u2500 plots/\n    \u251c\u2500\u2500 accuracy_plot.png     # Accuracy test plots\n    \u251c\u2500\u2500 cross_validation_plot.png\n    \u251c\u2500\u2500 refresh_stability_plot.png\n    \u2514\u2500\u2500 perturbation_plot.png\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data Formats for different data structures</li> <li>Explore Advanced Scenarios for complex use cases</li> <li>Check the Configuration guide for detailed settings </li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>mmm-eval can be customized through configuration files to control data processing, test parameters, and output settings.</p>"},{"location":"getting-started/configuration/#configuration-file-format","title":"Configuration File Format","text":"<p>mmm-eval uses JSON configuration files. You can specify a configuration file using the <code>--config-path</code> option:</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json\n</code></pre>"},{"location":"getting-started/configuration/#configuration-structure","title":"Configuration Structure","text":"<p>A complete configuration file has the following structure:</p> <pre><code>{\n  \"data\": {\n    \"date_column\": \"date\",\n    \"target_column\": \"sales\",\n    \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\"],\n    \"date_format\": \"%Y-%m-%d\",\n    \"validation\": {\n      \"check_missing_values\": true,\n      \"check_negative_values\": true,\n      \"check_date_range\": true\n    }\n  },\n  \"framework\": {\n    \"pymc_marketing\": {\n      \"model_config\": {\n        \"date_column\": \"date\",\n        \"target_column\": \"sales\",\n        \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n        \"control_columns\": [\"price\", \"seasonality\"]\n      }\n    }\n  },\n  \"tests\": {\n    \"accuracy\": {\n      \"train_test_split\": 0.8,\n      \"random_state\": 42,\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    },\n    \"cross_validation\": {\n      \"folds\": 5,\n      \"random_state\": 42,\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    },\n    \"refresh_stability\": {\n      \"refresh_periods\": [0.5, 0.75, 0.9],\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    },\n    \"perturbation\": {\n      \"perturbation_levels\": [0.05, 0.1, 0.15],\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    }\n  },\n  \"output\": {\n    \"format\": \"json\",\n    \"include_plots\": true,\n    \"plot_format\": \"png\",\n    \"save_intermediate_results\": false\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#data-configuration","title":"Data Configuration","text":""},{"location":"getting-started/configuration/#basic-data-settings","title":"Basic Data Settings","text":"<pre><code>{\n  \"data\": {\n    \"date_column\": \"date\",\n    \"target_column\": \"sales\",\n    \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\"]\n  }\n}\n</code></pre> <ul> <li>date_column: Name of the column containing dates</li> <li>target_column: Name of the column containing the target variable</li> <li>media_columns: List of column names for marketing channels</li> <li>control_columns: List of column names for control variables (optional)</li> </ul>"},{"location":"getting-started/configuration/#date-format","title":"Date Format","text":"<p>Specify the date format if your dates aren't in ISO format:</p> <pre><code>{\n  \"data\": {\n    \"date_format\": \"%Y-%m-%d\"\n  }\n}\n</code></pre> <p>Common formats: - <code>%Y-%m-%d</code> - 2023-01-01 - <code>%m/%d/%Y</code> - 01/01/2023 - <code>%d-%m-%Y</code> - 01-01-2023</p>"},{"location":"getting-started/configuration/#data-validation","title":"Data Validation","text":"<p>Control data validation settings:</p> <pre><code>{\n  \"data\": {\n    \"validation\": {\n      \"check_missing_values\": true,\n      \"check_negative_values\": true,\n      \"check_date_range\": true,\n      \"min_date\": \"2020-01-01\",\n      \"max_date\": \"2023-12-31\"\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#framework-configuration","title":"Framework Configuration","text":""},{"location":"getting-started/configuration/#pymc-marketing-settings","title":"PyMC-Marketing Settings","text":"<pre><code>{\n  \"framework\": {\n    \"pymc_marketing\": {\n      \"model_config\": {\n        \"date_column\": \"date\",\n        \"target_column\": \"sales\",\n        \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n        \"control_columns\": [\"price\", \"seasonality\"],\n        \"seasonality\": {\n          \"yearly_seasonality\": 10,\n          \"weekly_seasonality\": 3\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#test-configuration","title":"Test Configuration","text":""},{"location":"getting-started/configuration/#accuracy-test","title":"Accuracy Test","text":"<pre><code>{\n  \"tests\": {\n    \"accuracy\": {\n      \"train_test_split\": 0.8,\n      \"random_state\": 42,\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    }\n  }\n}\n</code></pre> <ul> <li>train_test_split: Proportion of data for training (0.0 to 1.0)</li> <li>random_state: Random seed for reproducibility</li> <li>metrics: List of metrics to calculate</li> </ul>"},{"location":"getting-started/configuration/#cross-validation-test","title":"Cross-Validation Test","text":"<pre><code>{\n  \"tests\": {\n    \"cross_validation\": {\n      \"folds\": 5,\n      \"random_state\": 42,\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    }\n  }\n}\n</code></pre> <ul> <li>folds: Number of cross-validation folds</li> <li>random_state: Random seed for reproducibility</li> <li>metrics: List of metrics to calculate</li> </ul>"},{"location":"getting-started/configuration/#refresh-stability-test","title":"Refresh Stability Test","text":"<pre><code>{\n  \"tests\": {\n    \"refresh_stability\": {\n      \"refresh_periods\": [0.5, 0.75, 0.9],\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    }\n  }\n}\n</code></pre> <ul> <li>refresh_periods: List of proportions for refresh periods</li> <li>metrics: List of metrics to calculate</li> </ul>"},{"location":"getting-started/configuration/#perturbation-test","title":"Perturbation Test","text":"<pre><code>{\n  \"tests\": {\n    \"perturbation\": {\n      \"perturbation_levels\": [0.05, 0.1, 0.15],\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    }\n  }\n}\n</code></pre> <ul> <li>perturbation_levels: List of perturbation levels (as proportions)</li> <li>metrics: List of metrics to calculate</li> </ul>"},{"location":"getting-started/configuration/#output-configuration","title":"Output Configuration","text":"<pre><code>{\n  \"output\": {\n    \"format\": \"json\",\n    \"include_plots\": true,\n    \"plot_format\": \"png\",\n    \"save_intermediate_results\": false\n  }\n}\n</code></pre> <ul> <li>format: Output format (\"json\" or \"csv\")</li> <li>include_plots: Whether to generate plots</li> <li>plot_format: Plot file format (\"png\", \"pdf\", \"svg\")</li> <li>save_intermediate_results: Whether to save intermediate test results</li> </ul>"},{"location":"getting-started/configuration/#available-metrics","title":"Available Metrics","text":"<p>The following metrics are available for all tests:</p> <ul> <li>mape: Mean Absolute Percentage Error</li> <li>rmse: Root Mean Square Error</li> <li>r2: R-squared (coefficient of determination)</li> <li>mae: Mean Absolute Error</li> <li>mse: Mean Square Error</li> </ul>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>You can also use environment variables for configuration:</p> <pre><code>export MMM_EVAL_DATE_COLUMN=date\nexport MMM_EVAL_TARGET_COLUMN=sales\nexport MMM_EVAL_MEDIA_COLUMNS=tv_spend,digital_spend,print_spend\n</code></pre>"},{"location":"getting-started/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>Configuration is applied in the following order (later takes precedence):</p> <ol> <li>Default configuration</li> <li>Environment variables</li> <li>Configuration file</li> <li>Command line arguments</li> </ol>"},{"location":"getting-started/configuration/#example-configurations","title":"Example Configurations","text":""},{"location":"getting-started/configuration/#minimal-configuration","title":"Minimal Configuration","text":"<pre><code>{\n  \"data\": {\n    \"date_column\": \"date\",\n    \"target_column\": \"sales\",\n    \"media_columns\": [\"tv_spend\", \"digital_spend\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>{\n  \"data\": {\n    \"date_column\": \"date\",\n    \"target_column\": \"sales\",\n    \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\"],\n    \"date_format\": \"%Y-%m-%d\",\n    \"validation\": {\n      \"check_missing_values\": true,\n      \"check_negative_values\": true\n    }\n  },\n  \"tests\": {\n    \"accuracy\": {\n      \"train_test_split\": 0.8,\n      \"random_state\": 42\n    },\n    \"cross_validation\": {\n      \"folds\": 5,\n      \"random_state\": 42\n    }\n  },\n  \"output\": {\n    \"include_plots\": true,\n    \"plot_format\": \"png\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data Formats for different data structures</li> <li>Explore Examples for configuration use cases</li> <li>Check the CLI Reference for command-line options </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install mmm-eval on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>mmm-eval requires Python 3.11 or higher. Make sure you have Python installed on your system.</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<p>The recommended way to install mmm-eval is using Poetry:</p> <pre><code># Install from GitHub\npoetry add git+https://github.com/Mutiny-Group/mmm-eval.git\n\n# Or clone and install locally\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npoetry install\n</code></pre> <p>Prerequisite: Poetry 2.x.x or later is required.</p>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>If you prefer using pip directly:</p> <pre><code># Install from GitHub\npip install git+https://github.com/Mutiny-Group/mmm-eval.git\n\n# Or clone and install locally\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>To install from the latest development version:</p> <pre><code>git clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development work, install with all development dependencies:</p> <pre><code>git clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\npoetry install\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>mmm-eval has the following key dependencies:</p> <ul> <li>numpy (&gt;=1.17) - Numerical computing</li> <li>pandas (^2.0.0) - Data manipulation</li> <li>google-meridian (^1.1.0) - Google's Meridian MMM framework</li> <li>pymc-marketing (^0.14.0) - PyMC-based MMM framework</li> <li>scipy (&gt;=1.13.1,&lt;2.0.0) - Scientific computing</li> <li>pytensor (^2.18.0) - Tensor operations</li> <li>pandera (^0.24.0) - Data validation</li> <li>pydantic (^2.5) - Data validation and settings</li> <li>click (^8.1.7) - Command line interface</li> <li>pyarrow (^20.0.0) - Fast data interchange</li> </ul>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify that mmm-eval is working correctly:</p> <pre><code># Check if mmm-eval is installed\npython -c \"import mmm_eval; print(mmm_eval.__version__)\"\n\n# Check CLI availability\nmmm-eval --help\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Import Error: If you get import errors, make sure you're using Python 3.11 or higher.</p> </li> <li> <p>Poetry Installation Issues: If you encounter issues with Poetry:    <pre><code># Update Poetry to the latest version\npoetry self update\n\n# Clear Poetry cache\npoetry cache clear --all pypi\n</code></pre></p> </li> <li> <p>Permission Errors: On some systems, you might need to use <code>pip install --user</code> to install without admin privileges.</p> </li> <li> <p>Version Conflicts: If you encounter dependency conflicts:    <pre><code># With Poetry\npoetry update\n\n# With pip (in a virtual environment)\npython -m venv mmm-eval-env\nsource mmm-eval-env/bin/activate  # On Windows: mmm-eval-env\\Scripts\\activate\npip install git+https://github.com/Mutiny-Group/mmm-eval.git\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter any issues during installation, please:</p> <ol> <li>Check the GitHub Issues for known problems</li> <li>Create a new issue with details about your system and the error message</li> <li>Join our Discussions for community support</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have mmm-eval installed, check out the Quick Start guide to begin using it. </p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you get started with mmm-eval quickly. You'll learn how to run your first evaluation and understand the basic workflow.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ol> <li>mmm-eval installed - See Installation if you haven't installed it yet</li> <li>Your MMM data - A CSV file with your marketing mix model data</li> <li>A supported framework - Currently PyMC-Marketing is supported</li> </ol>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quick-start/#1-prepare-your-data","title":"1. Prepare Your Data","text":"<p>Your data should be in CSV format with columns for:</p> <ul> <li>Date/time period</li> <li>Target variable (e.g., sales, conversions)</li> <li>Revenue variable (for calculating ROI)</li> <li>Marketing spend channels (e.g., TV, digital, print)</li> <li>Other variables (e.g., price, seasonality)</li> </ul> <p>Example data structure:</p> <pre><code>date,sales,revenue,tv_spend,digital_spend,print_spend,price\n2023-01-01,1000,7000,5000,2000,1000,10.99\n2023-01-02,1200,8000,5500,2200,1100,10.99\n...\n</code></pre>"},{"location":"getting-started/quick-start/#2-run-your-first-evaluation","title":"2. Run Your First Evaluation","text":"<p>The simplest way to run an evaluation:</p> <pre><code>mmm-eval --input-data-path your_data.csv --config-path your_config.json --framework pymc-marketing\n</code></pre> <p>This will:</p> <ul> <li>Load your data and config</li> <li>Run the PyMC-Marketing framework</li> <li>Execute all available tests</li> <li>Save results to the current directory</li> </ul>"},{"location":"getting-started/quick-start/#3-view-results","title":"3. View Results","text":"<p>After the evaluation completes, you'll find:</p> <ul> <li><code>mmm_eval_{framework}_{timestamp}.csv</code>, a table of tes results</li> </ul>"},{"location":"getting-started/quick-start/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/quick-start/#run-specific-tests","title":"Run Specific Tests","text":"<p>If you only want to run certain tests:</p> <pre><code>mmm-eval --input-data-path data.csv --config-path config.json --framework pymc-marketing --test-names accuracy cross_validation\n</code></pre> <p>Available tests:</p> <ul> <li><code>accuracy</code> - Model accuracy metrics</li> <li><code>cross_validation</code> - Cross-validation performance</li> <li><code>refresh_stability</code> - Model stability over time</li> <li><code>perturbation</code> - Sensitivity to data changes</li> </ul>"},{"location":"getting-started/quick-start/#custom-output-directory","title":"Custom Output Directory","text":"<p>Specify where to save results:</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --output-path ./my_results/\n</code></pre>"},{"location":"getting-started/quick-start/#understanding-results","title":"Understanding Results","text":""},{"location":"getting-started/quick-start/#key-metrics","title":"Key Metrics","text":"<p>mmm-eval provides several key metrics:</p> <ul> <li>MAPE (Mean Absolute Percentage Error) - Accuracy measure</li> <li>RMSE (Root Mean Square Error) - Error magnitude</li> <li>R-squared - Model fit quality</li> </ul>"},{"location":"getting-started/quick-start/#test-results","title":"Test Results","text":"<p>Each test provides specific insights:</p> <ul> <li>Accuracy Test: How well the model predicts on unseen data</li> <li>Cross-Validation: Model performance across different data splits</li> <li>Refresh Stability: How consistent the model is over time</li> <li>Perturbation: How sensitive the model is to data changes</li> </ul>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first evaluation:</p> <ol> <li>Explore the User Guide for detailed CLI options</li> <li>Check out Examples for more complex scenarios</li> <li>Learn about Data Formats for different data structures</li> <li>Review Metrics to understand the results better</li> </ol>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ul> <li>Check the CLI Reference for all available options</li> <li>Look at Examples for similar use cases</li> <li>Join our Discussions for community support</li> </ul>"},{"location":"user-guide/cli/","title":"Command Line Interface","text":"<p>mmm-eval provides a command-line interface for running MMM evaluations. This guide covers all available options and usage patterns.</p>"},{"location":"user-guide/cli/#basic-usage","title":"Basic Usage","text":"<p>The basic command structure is:</p> <pre><code>mmm-eval [OPTIONS] --input-data-path PATH --framework FRAMEWORK\n</code></pre>"},{"location":"user-guide/cli/#required-arguments","title":"Required Arguments","text":""},{"location":"user-guide/cli/#-input-data-path","title":"--input-data-path","text":"<p>Path to your input data file (CSV format).</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing\n</code></pre>"},{"location":"user-guide/cli/#-framework","title":"--framework","text":"<p>The MMM framework to use for evaluation.</p> <pre><code># Currently supported frameworks\nmmm-eval --input-data-path data.csv --framework pymc-marketing\n</code></pre>"},{"location":"user-guide/cli/#optional-arguments","title":"Optional Arguments","text":""},{"location":"user-guide/cli/#data-configuration","title":"Data Configuration","text":""},{"location":"user-guide/cli/#-config-path","title":"--config-path","text":"<p>Path to a JSON configuration file.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json\n</code></pre>"},{"location":"user-guide/cli/#-date-column","title":"--date-column","text":"<p>Name of the date column in your data.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --date-column date\n</code></pre>"},{"location":"user-guide/cli/#-target-column","title":"--target-column","text":"<p>Name of the target variable column.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --target-column sales\n</code></pre>"},{"location":"user-guide/cli/#-media-columns","title":"--media-columns","text":"<p>Comma-separated list of media channel columns.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --media-columns tv_spend,digital_spend,print_spend\n</code></pre>"},{"location":"user-guide/cli/#-control-columns","title":"--control-columns","text":"<p>Comma-separated list of control variable columns.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --control-columns price,seasonality\n</code></pre>"},{"location":"user-guide/cli/#test-configuration","title":"Test Configuration","text":""},{"location":"user-guide/cli/#-test-names","title":"--test-names","text":"<p>Comma-separated list of tests to run.</p> <pre><code># Run specific tests\nmmm-eval --input-data-path data.csv --framework pymc-marketing --test-names accuracy,cross_validation\n\n# Available tests: accuracy, cross_validation, refresh_stability, perturbation\n</code></pre>"},{"location":"user-guide/cli/#-train-test-split","title":"--train-test-split","text":"<p>Proportion of data to use for training (0.0 to 1.0).</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --train-test-split 0.8\n</code></pre>"},{"location":"user-guide/cli/#-cv-folds","title":"--cv-folds","text":"<p>Number of cross-validation folds.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --cv-folds 5\n</code></pre>"},{"location":"user-guide/cli/#-random-state","title":"--random-state","text":"<p>Random seed for reproducibility.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --random-state 42\n</code></pre>"},{"location":"user-guide/cli/#output-configuration","title":"Output Configuration","text":""},{"location":"user-guide/cli/#-output-path","title":"--output-path","text":"<p>Directory to save results.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --output-path ./results/\n</code></pre>"},{"location":"user-guide/cli/#-output-format","title":"--output-format","text":"<p>Output format for results (json or csv).</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --output-format json\n</code></pre>"},{"location":"user-guide/cli/#-include-plots","title":"--include-plots","text":"<p>Whether to generate plots.</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --include-plots\n</code></pre>"},{"location":"user-guide/cli/#-plot-format","title":"--plot-format","text":"<p>Format for generated plots (png, pdf, svg).</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --plot-format png\n</code></pre>"},{"location":"user-guide/cli/#framework-specific-options","title":"Framework-Specific Options","text":""},{"location":"user-guide/cli/#pymc-marketing-options","title":"PyMC-Marketing Options","text":"<pre><code># Specify seasonality parameters\nmmm-eval --input-data-path data.csv --framework pymc-marketing --yearly-seasonality 10 --weekly-seasonality 3\n</code></pre>"},{"location":"user-guide/cli/#complete-example","title":"Complete Example","text":"<p>Here's a complete example with all options:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --date-column date \\\n  --target-column sales \\\n  --media-columns tv_spend,digital_spend,print_spend \\\n  --control-columns price,seasonality \\\n  --test-names accuracy,cross_validation,refresh_stability \\\n  --train-test-split 0.8 \\\n  --cv-folds 5 \\\n  --random-state 42 \\\n  --output-path ./results/ \\\n  --output-format json \\\n  --include-plots \\\n  --plot-format png\n</code></pre>"},{"location":"user-guide/cli/#environment-variables","title":"Environment Variables","text":"<p>You can also set configuration using environment variables:</p> <pre><code>export MMM_EVAL_DATE_COLUMN=date\nexport MMM_EVAL_TARGET_COLUMN=sales\nexport MMM_EVAL_MEDIA_COLUMNS=tv_spend,digital_spend,print_spend\nexport MMM_EVAL_TRAIN_TEST_SPLIT=0.8\nexport MMM_EVAL_RANDOM_STATE=42\n\nmmm-eval --input-data-path data.csv --framework pymc-marketing\n</code></pre>"},{"location":"user-guide/cli/#help-and-information","title":"Help and Information","text":""},{"location":"user-guide/cli/#-help","title":"--help","text":"<p>Display help information:</p> <pre><code>mmm-eval --help\n</code></pre>"},{"location":"user-guide/cli/#-version","title":"--version","text":"<p>Display version information:</p> <pre><code>mmm-eval --version\n</code></pre>"},{"location":"user-guide/cli/#-verbose","title":"--verbose","text":"<p>Enable verbose output:</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --verbose\n</code></pre>"},{"location":"user-guide/cli/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/cli/#common-errors","title":"Common Errors","text":"<ol> <li>File not found: Ensure the input data file exists and the path is correct</li> <li>Invalid column names: Check that column names match your data file</li> <li>Insufficient data: Ensure you have enough data for the specified train-test split</li> <li>Framework errors: Check that all required dependencies are installed</li> </ol>"},{"location":"user-guide/cli/#debug-mode","title":"Debug Mode","text":"<p>Enable debug mode for detailed error information:</p> <pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --debug\n</code></pre>"},{"location":"user-guide/cli/#output-files","title":"Output Files","text":"<p>After running an evaluation, you'll find the following files in your output directory:</p> <ul> <li><code>results.json</code> - Detailed test results in JSON format</li> <li><code>results_summary.csv</code> - Summary metrics in CSV format</li> <li><code>plots/</code> - Directory containing generated plots</li> <li><code>accuracy_plot.png</code> - Accuracy test visualizations</li> <li><code>cross_validation_plot.png</code> - Cross-validation results</li> <li><code>refresh_stability_plot.png</code> - Refresh stability analysis</li> <li><code>perturbation_plot.png</code> - Perturbation test results</li> </ul>"},{"location":"user-guide/cli/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data Formats for different data structures</li> <li>Explore Examples for practical use cases</li> <li>Check the Configuration guide for advanced settings </li> </ul>"},{"location":"user-guide/data-formats/","title":"Data Formats","text":"<p>mmm-eval expects your data to be in a specific format. This guide explains the required structure and provides examples for preparing your marketing mix modeling data.</p>"},{"location":"user-guide/data-formats/#required-data-format","title":"Required Data Format","text":"<p>mmm-eval accepts CSV files with the following structure:</p>"},{"location":"user-guide/data-formats/#basic-requirements","title":"Basic Requirements","text":"<ul> <li>Date column: Time series data with consistent date format</li> <li>Target column: The variable you want to predict (e.g., sales, conversions)</li> <li>Revenue column: Revenue data for calculating ROI and efficiency metrics</li> <li>Media columns: Marketing channel spend or activity data</li> <li>Control columns (optional): Additional variables that may affect the target</li> </ul>"},{"location":"user-guide/data-formats/#column-types","title":"Column Types","text":""},{"location":"user-guide/data-formats/#date-column","title":"Date Column","text":"<ul> <li>Purpose: Identifies the time period for each observation</li> <li>Format: Date in a consistent format (e.g., YYYY-MM-DD, MM/DD/YYYY)</li> <li>Requirements: </li> <li>Must be in chronological order</li> <li>No missing dates in the series</li> <li>Consistent format throughout</li> </ul>"},{"location":"user-guide/data-formats/#target-column","title":"Target Column","text":"<ul> <li>Purpose: The dependent variable you want to model (e.g., sales, conversions)</li> <li>Format: Numeric values</li> <li>Requirements:</li> <li>No missing values</li> <li>Positive values (for most use cases)</li> <li>Reasonable scale for your business</li> </ul>"},{"location":"user-guide/data-formats/#revenue-column","title":"Revenue Column","text":"<ul> <li>Purpose: Revenue data for calculating ROI and efficiency metrics</li> <li>Format: Numeric values</li> <li>Requirements:</li> <li>No missing values</li> <li>Positive values</li> <li>Same time period as target column</li> <li>Used for ROI calculations and efficiency analysis</li> </ul>"},{"location":"user-guide/data-formats/#media-columns","title":"Media Columns","text":"<ul> <li>Purpose: Marketing channel spend or activity data</li> <li>Format: Numeric values</li> <li>Requirements:</li> <li>No missing values (use 0 for periods with no spend)</li> <li>Non-negative values</li> <li>Consistent units (e.g., all in dollars, all in thousands)</li> </ul>"},{"location":"user-guide/data-formats/#control-columns","title":"Control Columns","text":"<ul> <li>Purpose: Additional variables that may affect the target</li> <li>Format: Numeric or categorical values</li> <li>Examples: Price, seasonality indicators, holiday flags, competitor activity</li> </ul>"},{"location":"user-guide/data-formats/#example-data-structure","title":"Example Data Structure","text":""},{"location":"user-guide/data-formats/#basic-example","title":"Basic Example","text":"<pre><code>date,sales,revenue,tv_spend,digital_spend,print_spend\n2023-01-01,1000,7000,5000,2000,1000\n2023-01-02,1200,8000,5500,2200,1100\n2023-01-03,1100,7500,5200,2100,1050\n2023-01-04,1300,9000,6000,2400,1200\n2023-01-05,1400,9500,6500,2600,1300\n</code></pre>"},{"location":"user-guide/data-formats/#advanced-example-with-controls","title":"Advanced Example with Controls","text":"<pre><code>date,sales,revenue,tv_spend,digital_spend,print_spend,price,seasonality,holiday\n2023-01-01,1000,7000,5000,2000,1000,10.99,0.8,0\n2023-01-02,1200,8000,5500,2200,1100,10.99,0.9,0\n2023-01-03,1100,7500,5200,2100,1050,11.99,0.7,1\n2023-01-04,1300,9000,6000,2400,1200,11.99,0.8,0\n2023-01-05,1400,9500,6500,2600,1300,12.99,0.9,0\n</code></pre>"},{"location":"user-guide/data-formats/#real-world-example","title":"Real-World Example","text":"<pre><code>date,sales,revenue,tv_spend,digital_spend,social_spend,search_spend,email_spend,price,holiday_flag,competitor_promo\n2023-01-01,1250,8750,15000,8000,3000,5000,2000,12.99,0,0\n2023-01-02,1320,9240,16000,8500,3200,5200,2100,12.99,0,0\n2023-01-03,1180,8260,14000,7500,2800,4800,1900,13.99,1,1\n2023-01-04,1450,10150,18000,9500,3800,6000,2400,13.99,0,0\n2023-01-05,1520,10640,19000,10000,4000,6300,2500,14.99,0,0\n</code></pre>"},{"location":"user-guide/data-formats/#data-requirements","title":"Data Requirements","text":""},{"location":"user-guide/data-formats/#minimum-data-requirements","title":"Minimum Data Requirements","text":"<ul> <li>Time period: At least 52 weeks (1 year) of data</li> <li>Frequency: Weekly or daily data (consistent frequency)</li> <li>Observations: Minimum 100 data points recommended</li> <li>Media channels: At least 2-3 channels for meaningful analysis</li> <li>Revenue data: Required for ROI calculations</li> </ul>"},{"location":"user-guide/data-formats/#data-quality-requirements","title":"Data Quality Requirements","text":""},{"location":"user-guide/data-formats/#completeness","title":"Completeness","text":"<ul> <li>No missing values in required columns</li> <li>Complete time series (no gaps in dates)</li> <li>Consistent data collection methodology</li> </ul>"},{"location":"user-guide/data-formats/#consistency","title":"Consistency","text":"<ul> <li>Same units throughout (e.g., all spend in same currency)</li> <li>Consistent date format</li> <li>Consistent naming conventions</li> </ul>"},{"location":"user-guide/data-formats/#reasonableness","title":"Reasonableness","text":"<ul> <li>Values within expected ranges</li> <li>No obvious outliers or errors</li> <li>Logical relationships between variables</li> </ul>"},{"location":"user-guide/data-formats/#configuration-file","title":"Configuration File","text":"<p>mmm-eval uses a configuration file to specify data column mappings and settings:</p>"},{"location":"user-guide/data-formats/#basic-configuration","title":"Basic Configuration","text":"<pre><code>{\n  \"data\": {\n    \"date_column\": \"date\",\n    \"target_column\": \"sales\",\n    \"revenue_column\": \"revenue\",\n    \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\", \"holiday\"]\n  }\n}\n</code></pre>"},{"location":"user-guide/data-formats/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>{\n  \"data\": {\n    \"date_column\": \"date\",\n    \"date_format\": \"%Y-%m-%d\",\n    \"target_column\": \"sales\",\n    \"revenue_column\": \"revenue\",\n    \"media_columns\": [\"tv_spend\", \"digital_spend\", \"social_spend\", \"search_spend\", \"email_spend\"],\n    \"control_columns\": [\"price\", \"holiday_flag\", \"competitor_promo\"],\n    \"validation\": {\n      \"check_missing_values\": true,\n      \"check_negative_values\": true,\n      \"check_date_range\": true,\n      \"min_date\": \"2020-01-01\",\n      \"max_date\": \"2023-12-31\"\n    }\n  },\n  \"tests\": {\n    \"accuracy\": {\n      \"train_test_split\": 0.8\n    },\n    \"cross_validation\": {\n      \"folds\": 5\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/data-formats/#date-formats","title":"Date Formats","text":""},{"location":"user-guide/data-formats/#supported-formats","title":"Supported Formats","text":"<p>mmm-eval supports various date formats:</p> <ul> <li><code>YYYY-MM-DD</code> (ISO format) - Recommended</li> <li><code>MM/DD/YYYY</code></li> <li><code>DD-MM-YYYY</code></li> <li><code>YYYY/MM/DD</code></li> </ul>"},{"location":"user-guide/data-formats/#date-format-specification","title":"Date Format Specification","text":"<p>If your dates aren't in ISO format, specify the format in your configuration:</p> <pre><code>{\n  \"data\": {\n    \"date_format\": \"%m/%d/%Y\"\n  }\n}\n</code></pre>"},{"location":"user-guide/data-formats/#data-validation","title":"Data Validation","text":"<p>mmm-eval performs several validation checks:</p>"},{"location":"user-guide/data-formats/#automatic-validation","title":"Automatic Validation","text":"<ol> <li>Missing values: Checks for missing data in required columns</li> <li>Date consistency: Ensures dates are in chronological order</li> <li>Data types: Verifies numeric columns contain valid numbers</li> <li>Value ranges: Checks for negative values in spend columns</li> <li>Revenue consistency: Ensures revenue data is available and positive</li> </ol>"},{"location":"user-guide/data-formats/#custom-validation","title":"Custom Validation","text":"<p>You can configure additional validation in your config file:</p> <pre><code>{\n  \"data\": {\n    \"validation\": {\n      \"check_missing_values\": true,\n      \"check_negative_values\": true,\n      \"check_date_range\": true,\n      \"min_date\": \"2020-01-01\",\n      \"max_date\": \"2023-12-31\",\n      \"min_observations\": 100,\n      \"required_columns\": [\"date\", \"sales\", \"revenue\", \"tv_spend\"]\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/data-formats/#data-preparation-tips","title":"Data Preparation Tips","text":""},{"location":"user-guide/data-formats/#before-running-mmm-eval","title":"Before Running mmm-eval","text":"<ol> <li>Clean your data:</li> <li>Remove any test or dummy data</li> <li>Handle missing values appropriately</li> <li> <p>Check for and remove outliers if necessary</p> </li> <li> <p>Standardize formats:</p> </li> <li>Ensure consistent date format</li> <li>Use consistent units (e.g., thousands of dollars)</li> <li> <p>Standardize column names</p> </li> <li> <p>Validate relationships:</p> </li> <li>Check that spend and sales have logical relationships</li> <li>Verify that revenue data is consistent with sales</li> <li>Ensure control variables make sense</li> </ol>"},{"location":"user-guide/data-formats/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/data-formats/#missing-values","title":"Missing Values","text":"<pre><code># Problem: Missing values in media columns\ndate,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,\n2023-01-02,1200,8000,,2000\n\n# Solution: Fill with zeros or appropriate values\ndate,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,0\n2023-01-02,1200,8000,0,2000\n</code></pre>"},{"location":"user-guide/data-formats/#inconsistent-date-formats","title":"Inconsistent Date Formats","text":"<pre><code># Problem: Mixed date formats\ndate,sales,revenue\n01/01/2023,1000,7000\n2023-01-02,1200,8000\n\n# Solution: Standardize to one format\ndate,sales,revenue\n2023-01-01,1000,7000\n2023-01-02,1200,8000\n</code></pre>"},{"location":"user-guide/data-formats/#missing-revenue-data","title":"Missing Revenue Data","text":"<pre><code># Problem: No revenue column\ndate,sales,tv_spend,digital_spend\n2023-01-01,1000,5000,2000\n2023-01-02,1200,5500,2200\n\n# Solution: Add revenue column\ndate,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,2000\n2023-01-02,1200,8000,5500,2200\n</code></pre>"},{"location":"user-guide/data-formats/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/data-formats/#data-collection","title":"Data Collection","text":"<ul> <li>Consistent timing: Collect data at the same time each period</li> <li>Complete coverage: Ensure all channels are captured</li> <li>Quality control: Implement data validation at source</li> <li>Documentation: Keep records of any data changes or anomalies</li> </ul>"},{"location":"user-guide/data-formats/#data-storage","title":"Data Storage","text":"<ul> <li>Backup regularly: Keep multiple copies of your data</li> <li>Version control: Track changes to your datasets</li> <li>Metadata: Document data sources, definitions, and assumptions</li> <li>Security: Protect sensitive business data</li> </ul>"},{"location":"user-guide/data-formats/#data-analysis","title":"Data Analysis","text":"<ul> <li>Start simple: Begin with basic models before adding complexity</li> <li>Validate assumptions: Check that your data meets model requirements</li> <li>Monitor quality: Regularly review data for issues</li> <li>Document decisions: Keep records of data preparation choices</li> </ul>"},{"location":"user-guide/data-formats/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/data-formats/#common-error-messages","title":"Common Error Messages","text":"<ul> <li>\"Missing required column\": Ensure all required columns are present</li> <li>\"Invalid date format\": Check your date format specification</li> <li>\"Negative values in spend columns\": Replace negative values with zeros</li> <li>\"Missing revenue data\": Add revenue column to your dataset</li> </ul>"},{"location":"user-guide/data-formats/#getting-help","title":"Getting Help","text":"<p>If you encounter data format issues:</p> <ul> <li>Check the CLI Reference for all available options</li> <li>Review the Examples for similar use cases</li> <li>Join our Discussions for community support </li> </ul>"},{"location":"user-guide/frameworks/","title":"Frameworks","text":"<p>mmm-eval supports multiple Marketing Mix Modeling (MMM) frameworks. This guide explains each supported framework and their features.</p>"},{"location":"user-guide/frameworks/#supported-frameworks","title":"Supported Frameworks","text":""},{"location":"user-guide/frameworks/#pymc-marketing","title":"PyMC-Marketing","text":"<p>Status: \u2705 Fully Supported</p> <p>PyMC-Marketing is a Bayesian MMM framework built on PyMC that provides robust statistical modeling capabilities.</p>"},{"location":"user-guide/frameworks/#features","title":"Features","text":"<ul> <li>Bayesian inference: Probabilistic modeling with uncertainty quantification</li> <li>Flexible modeling: Customizable model structures</li> <li>Seasonality handling: Built-in seasonal components</li> <li>Media saturation: Configurable saturation curves</li> <li>Control variables: Support for external factors</li> </ul>"},{"location":"user-guide/frameworks/#configuration","title":"Configuration","text":"<pre><code>{\n  \"framework\": {\n    \"pymc_marketing\": {\n      \"model_config\": {\n        \"date_column\": \"date\",\n        \"target_column\": \"sales\",\n        \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n        \"control_columns\": [\"price\", \"seasonality\"],\n        \"seasonality\": {\n          \"yearly_seasonality\": 10,\n          \"weekly_seasonality\": 3\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/frameworks/#cli-usage","title":"CLI Usage","text":"<pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing\n</code></pre>"},{"location":"user-guide/frameworks/#advantages","title":"Advantages","text":"<ul> <li>Statistical rigor: Bayesian approach provides uncertainty estimates</li> <li>Flexibility: Highly customizable model structures</li> <li>Interpretability: Clear parameter interpretation</li> <li>Robustness: Handles various data scenarios well</li> </ul>"},{"location":"user-guide/frameworks/#limitations","title":"Limitations","text":"<ul> <li>Computational cost: Can be slower than simpler approaches</li> <li>Complexity: Requires more expertise to configure optimally</li> <li>Data requirements: Needs sufficient data for reliable inference</li> </ul>"},{"location":"user-guide/frameworks/#framework-comparison","title":"Framework Comparison","text":"Feature PyMC-Marketing Google Meridian* Robyn* Type Bayesian Bayesian Bayesian Inference MCMC MCMC MCMC Seasonality \u2705 Built-in \u2705 Built-in \u2705 Built-in Saturation \u2705 Configurable \u2705 Configurable \u2705 Configurable Controls \u2705 Supported \u2705 Supported \u2705 Supported Uncertainty \u2705 Full \u2705 Full \u2705 Full Speed Medium Fast Medium Complexity High Medium Medium <p>*Planned for future releases</p>"},{"location":"user-guide/frameworks/#framework-selection","title":"Framework Selection","text":""},{"location":"user-guide/frameworks/#when-to-use-pymc-marketing","title":"When to Use PyMC-Marketing","text":"<p>Choose PyMC-Marketing when:</p> <ul> <li>You need uncertainty quantification</li> <li>You have sufficient data (100+ observations)</li> <li>You want maximum flexibility in modeling</li> <li>You have expertise in Bayesian modeling</li> <li>You need detailed parameter interpretation</li> </ul> <p>Example use cases: - Strategic planning with uncertainty - Complex market scenarios - Research and development - High-stakes decision making</p>"},{"location":"user-guide/frameworks/#framework-requirements","title":"Framework Requirements","text":""},{"location":"user-guide/frameworks/#data-requirements","title":"Data Requirements","text":"<p>PyMC-Marketing: - Minimum observations: 100 - Recommended observations: 200+ - Time period: 1+ years - Media channels: 2-10 channels - Data quality: High quality, clean data</p>"},{"location":"user-guide/frameworks/#computational-requirements","title":"Computational Requirements","text":"<p>PyMC-Marketing: - Memory: 4GB+ RAM recommended - Processing: Multi-core CPU beneficial - Time: 10-60 minutes depending on data size - Storage: Minimal additional storage needed</p>"},{"location":"user-guide/frameworks/#configuration-options","title":"Configuration Options","text":""},{"location":"user-guide/frameworks/#pymc-marketing-configuration","title":"PyMC-Marketing Configuration","text":""},{"location":"user-guide/frameworks/#basic-configuration","title":"Basic Configuration","text":"<pre><code>{\n  \"framework\": {\n    \"pymc_marketing\": {\n      \"model_config\": {\n        \"date_column\": \"date\",\n        \"target_column\": \"sales\",\n        \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/frameworks/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>{\n  \"framework\": {\n    \"pymc_marketing\": {\n      \"model_config\": {\n        \"date_column\": \"date\",\n        \"target_column\": \"sales\",\n        \"media_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n        \"control_columns\": [\"price\", \"seasonality\", \"holiday\"],\n        \"seasonality\": {\n          \"yearly_seasonality\": 10,\n          \"weekly_seasonality\": 3\n        },\n        \"saturation\": {\n          \"type\": \"hill\",\n          \"parameters\": {\n            \"ec50\": 0.5,\n            \"hill\": 2.0\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/frameworks/#seasonality-configuration","title":"Seasonality Configuration","text":"<pre><code>{\n  \"seasonality\": {\n    \"yearly_seasonality\": 10,    // Number of yearly seasonality terms\n    \"weekly_seasonality\": 3,     // Number of weekly seasonality terms\n    \"yearly_seasonality_prior\": {\n      \"sigma\": 0.1\n    },\n    \"weekly_seasonality_prior\": {\n      \"sigma\": 0.1\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/frameworks/#saturation-configuration","title":"Saturation Configuration","text":"<pre><code>{\n  \"saturation\": {\n    \"type\": \"hill\",              // \"hill\" or \"s-curve\"\n    \"parameters\": {\n      \"ec50\": 0.5,              // Half-saturation point\n      \"hill\": 2.0               // Hill coefficient\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/frameworks/#framework-specific-cli-options","title":"Framework-Specific CLI Options","text":""},{"location":"user-guide/frameworks/#pymc-marketing-options","title":"PyMC-Marketing Options","text":"<pre><code># Basic usage\nmmm-eval --input-data-path data.csv --framework pymc-marketing\n\n# With seasonality parameters\nmmm-eval --input-data-path data.csv --framework pymc-marketing \\\n  --yearly-seasonality 10 --weekly-seasonality 3\n\n# With custom configuration\nmmm-eval --input-data-path data.csv --framework pymc-marketing \\\n  --config-path pymc_config.json\n</code></pre>"},{"location":"user-guide/frameworks/#performance-considerations","title":"Performance Considerations","text":""},{"location":"user-guide/frameworks/#pymc-marketing-performance","title":"PyMC-Marketing Performance","text":"<p>Factors affecting performance: - Data size: Larger datasets take longer to process - Model complexity: More parameters increase computation time - Hardware: CPU cores and memory affect speed - Convergence: Some models may require more iterations</p> <p>Optimization tips: - Use appropriate data size (not too small, not too large) - Simplify model when possible - Use multiple CPU cores - Monitor convergence diagnostics</p>"},{"location":"user-guide/frameworks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/frameworks/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/frameworks/#pymc-marketing-issues","title":"PyMC-Marketing Issues","text":"<p>Slow convergence: - Increase number of samples - Adjust prior distributions - Check data quality - Simplify model structure</p> <p>Memory errors: - Reduce data size - Use fewer parameters - Increase system memory - Use data sampling</p> <p>Poor performance: - Check data quality - Verify model specification - Review convergence diagnostics - Consider simpler model</p>"},{"location":"user-guide/frameworks/#getting-help","title":"Getting Help","text":"<p>For framework-specific issues:</p> <ol> <li>Check documentation: Review framework-specific guides</li> <li>Review examples: Look at example configurations</li> <li>Check data: Ensure data meets requirements</li> <li>Community support: Use GitHub discussions for help</li> </ol>"},{"location":"user-guide/frameworks/#future-frameworks","title":"Future Frameworks","text":""},{"location":"user-guide/frameworks/#planned-support","title":"Planned Support","text":"<p>Google Meridian: - Google's open-source MMM framework - Fast Bayesian inference - Production-ready implementation</p> <p>Robyn: - Meta's MMM framework - Automated model selection - Built-in validation</p>"},{"location":"user-guide/frameworks/#framework-requests","title":"Framework Requests","text":"<p>To request support for additional frameworks:</p> <ol> <li>Create an issue on GitHub</li> <li>Provide details about the framework</li> <li>Include use cases and requirements</li> <li>Contribute if possible</li> </ol>"},{"location":"user-guide/frameworks/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Configuration for framework setup</li> <li>Check Examples for framework usage</li> <li>Review Tests for validation approaches </li> </ul>"},{"location":"user-guide/metrics/","title":"Metrics","text":"<p>mmm-eval provides a comprehensive set of metrics to evaluate MMM performance. This guide explains each metric and how to interpret the results.</p>"},{"location":"user-guide/metrics/#overview","title":"Overview","text":"<p>mmm-eval calculates several key metrics across different validation tests:</p> <ul> <li>Accuracy Metrics: How well the model predicts on unseen data</li> <li>Stability Metrics: How consistent the model is over time</li> <li>Robustness Metrics: How sensitive the model is to data changes</li> </ul>"},{"location":"user-guide/metrics/#accuracy-metrics","title":"Accuracy Metrics","text":""},{"location":"user-guide/metrics/#mape-mean-absolute-percentage-error","title":"MAPE (Mean Absolute Percentage Error)","text":"<p>Formula: MAPE = (100% / n) * \u03a3|(y_i - \u0177_i) / y_i|</p> <p>Interpretation:  - Lower values indicate better accuracy - Expressed as a percentage - Sensitive to scale of target variable</p> <p>Example: MAPE of 15% means predictions are off by 15% on average</p>"},{"location":"user-guide/metrics/#rmse-root-mean-square-error","title":"RMSE (Root Mean Square Error)","text":"<p>Formula: RMSE = \u221a(\u03a3(y_i - \u0177_i)\u00b2 / n)</p> <p>Interpretation: - Lower values indicate better accuracy - Expressed in same units as target variable - Penalizes large errors more heavily than small ones</p> <p>Example: RMSE of 100 means predictions deviate by 100 units on average</p>"},{"location":"user-guide/metrics/#r-squared-coefficient-of-determination","title":"R-squared (Coefficient of Determination)","text":"<p>Formula: R\u00b2 = 1 - (\u03a3(y_i - \u0177_i)\u00b2 / \u03a3(y_i - \u0233)\u00b2)</p> <p>Interpretation: - Range: 0 to 1 (or 0% to 100%) - Higher values indicate better fit - Represents proportion of variance explained by the model</p> <p>Example: R\u00b2 of 0.85 means the model explains 85% of the variance</p>"},{"location":"user-guide/metrics/#mae-mean-absolute-error","title":"MAE (Mean Absolute Error)","text":"<p>Formula: MAE = \u03a3|y_i - \u0177_i| / n</p> <p>Interpretation: - Lower values indicate better accuracy - Expressed in same units as target variable - Less sensitive to outliers than RMSE</p> <p>Example: MAE of 80 means predictions deviate by 80 units on average</p>"},{"location":"user-guide/metrics/#mse-mean-square-error","title":"MSE (Mean Square Error)","text":"<p>Formula: MSE = \u03a3(y_i - \u0177_i)\u00b2 / n</p> <p>Interpretation: - Lower values indicate better accuracy - Expressed in squared units of target variable - Heavily penalizes large errors</p> <p>Example: MSE of 10,000 means average squared error is 10,000</p>"},{"location":"user-guide/metrics/#stability-metrics","title":"Stability Metrics","text":""},{"location":"user-guide/metrics/#refresh-stability","title":"Refresh Stability","text":"<p>Measures how consistent model performance is when retrained on different time periods.</p> <p>Calculation: 1. Train model on different proportions of data (e.g., 50%, 75%, 90%) 2. Calculate metrics for each refresh period 3. Measure variation in performance across periods</p> <p>Interpretation: - Lower variation indicates more stable model - High variation suggests model is sensitive to training data</p>"},{"location":"user-guide/metrics/#cross-validation-stability","title":"Cross-Validation Stability","text":"<p>Measures consistency across different data splits.</p> <p>Calculation: 1. Perform k-fold cross-validation 2. Calculate metrics for each fold 3. Measure variation in performance across folds</p> <p>Interpretation: - Lower variation indicates more stable model - High variation suggests overfitting or data issues</p>"},{"location":"user-guide/metrics/#robustness-metrics","title":"Robustness Metrics","text":""},{"location":"user-guide/metrics/#perturbation-sensitivity","title":"Perturbation Sensitivity","text":"<p>Measures how sensitive the model is to small changes in the data.</p> <p>Calculation: 1. Add small random perturbations to input data 2. Retrain model and calculate metrics 3. Measure change in performance</p> <p>Interpretation: - Lower sensitivity indicates more robust model - High sensitivity suggests model may not generalize well</p>"},{"location":"user-guide/metrics/#metric-comparison","title":"Metric Comparison","text":""},{"location":"user-guide/metrics/#when-to-use-each-metric","title":"When to Use Each Metric","text":"Metric Best For Considerations MAPE Relative accuracy Sensitive to scale, good for comparison RMSE Overall accuracy Penalizes large errors heavily R\u00b2 Model fit quality May be misleading with non-linear relationships MAE Robust accuracy Less sensitive to outliers MSE Mathematical optimization Harder to interpret"},{"location":"user-guide/metrics/#metric-ranges-and-benchmarks","title":"Metric Ranges and Benchmarks","text":""},{"location":"user-guide/metrics/#mape-benchmarks","title":"MAPE Benchmarks","text":"<ul> <li>Excellent: &lt; 10%</li> <li>Good: 10% - 20%</li> <li>Fair: 20% - 30%</li> <li>Poor: &gt; 30%</li> </ul>"},{"location":"user-guide/metrics/#r2-benchmarks","title":"R\u00b2 Benchmarks","text":"<ul> <li>Excellent: &gt; 0.9</li> <li>Good: 0.8 - 0.9</li> <li>Fair: 0.6 - 0.8</li> <li>Poor: &lt; 0.6</li> </ul> <p>Note: These benchmarks are general guidelines. Industry-specific benchmarks may vary.</p>"},{"location":"user-guide/metrics/#interpreting-results","title":"Interpreting Results","text":""},{"location":"user-guide/metrics/#single-model-evaluation","title":"Single Model Evaluation","text":"<p>When evaluating a single model:</p> <ol> <li>Check accuracy metrics: Are predictions reasonably accurate?</li> <li>Assess stability: Is performance consistent across different scenarios?</li> <li>Evaluate robustness: How sensitive is the model to data changes?</li> </ol>"},{"location":"user-guide/metrics/#model-comparison","title":"Model Comparison","text":"<p>When comparing multiple models:</p> <ol> <li>Compare accuracy: Which model has better predictive performance?</li> <li>Compare stability: Which model is more consistent?</li> <li>Compare robustness: Which model is less sensitive to changes?</li> </ol>"},{"location":"user-guide/metrics/#example-results","title":"Example Results","text":"<pre><code>{\n  \"accuracy\": {\n    \"mape\": 12.5,\n    \"rmse\": 150.2,\n    \"r2\": 0.87,\n    \"mae\": 120.8\n  },\n  \"cross_validation\": {\n    \"mape_mean\": 13.2,\n    \"mape_std\": 1.8,\n    \"r2_mean\": 0.85,\n    \"r2_std\": 0.03\n  },\n  \"refresh_stability\": {\n    \"mape_variation\": 2.1,\n    \"r2_variation\": 0.05\n  }\n}\n</code></pre> <p>Interpretation: - Good accuracy (MAPE 12.5%, R\u00b2 0.87) - Stable cross-validation (low standard deviations) - Good refresh stability (low variation)</p>"},{"location":"user-guide/metrics/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/metrics/#metric-selection","title":"Metric Selection","text":"<ol> <li>Use multiple metrics: Don't rely on a single metric</li> <li>Consider business context: Choose metrics relevant to your goals</li> <li>Account for scale: Use relative metrics for comparison across scales</li> </ol>"},{"location":"user-guide/metrics/#result-interpretation","title":"Result Interpretation","text":"<ol> <li>Set realistic expectations: MMM accuracy varies by industry</li> <li>Consider data quality: Poor data leads to poor metrics</li> <li>Validate assumptions: Ensure metrics align with business needs</li> </ol>"},{"location":"user-guide/metrics/#continuous-monitoring","title":"Continuous Monitoring","text":"<ol> <li>Track metrics over time: Monitor performance degradation</li> <li>Set up alerts: Flag when metrics fall below thresholds</li> <li>Regular re-evaluation: Periodically reassess model performance</li> </ol>"},{"location":"user-guide/metrics/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Tests to understand how metrics are calculated</li> <li>Check Examples for practical metric interpretation</li> <li>Review Configuration for customizing metric calculation </li> </ul>"},{"location":"user-guide/tests/","title":"Tests","text":"<p>mmm-eval provides a comprehensive suite of validation tests to evaluate MMM performance. This guide explains each test and how to interpret the results.</p>"},{"location":"user-guide/tests/#overview","title":"Overview","text":"<p>mmm-eval includes four main types of validation tests:</p> <ol> <li>Accuracy Test - Evaluates predictive performance</li> <li>Cross-Validation Test - Assesses model stability</li> <li>Refresh Stability Test - Tests temporal consistency</li> <li>Perturbation Test - Evaluates robustness</li> </ol>"},{"location":"user-guide/tests/#accuracy-test","title":"Accuracy Test","text":""},{"location":"user-guide/tests/#purpose","title":"Purpose","text":"<p>The accuracy test evaluates how well the model predicts on unseen data.</p>"},{"location":"user-guide/tests/#methodology","title":"Methodology","text":"<ol> <li>Data Split: Divides data into training and test sets</li> <li>Model Training: Trains the model on the training set</li> <li>Prediction: Makes predictions on the test set</li> <li>Evaluation: Calculates accuracy metrics</li> </ol>"},{"location":"user-guide/tests/#configuration","title":"Configuration","text":"<pre><code>{\n  \"tests\": {\n    \"accuracy\": {\n      \"train_test_split\": 0.8,\n      \"random_state\": 42,\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/tests/#interpretation","title":"Interpretation","text":"<ul> <li>Good accuracy: Low MAPE/RMSE, high R\u00b2</li> <li>Poor accuracy: High MAPE/RMSE, low R\u00b2</li> <li>Overfitting: Good training performance, poor test performance</li> </ul>"},{"location":"user-guide/tests/#cross-validation-test","title":"Cross-Validation Test","text":""},{"location":"user-guide/tests/#purpose_1","title":"Purpose","text":"<p>The cross-validation test assesses model stability across different data splits.</p>"},{"location":"user-guide/tests/#methodology_1","title":"Methodology","text":"<ol> <li>K-Fold Split: Divides data into k equal parts</li> <li>Iterative Training: Trains k models, each using k-1 folds</li> <li>Performance Assessment: Evaluates each model on the held-out fold</li> <li>Stability Analysis: Measures variation in performance across folds</li> </ol>"},{"location":"user-guide/tests/#configuration_1","title":"Configuration","text":"<pre><code>{\n  \"tests\": {\n    \"cross_validation\": {\n      \"folds\": 5,\n      \"random_state\": 42,\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/tests/#interpretation_1","title":"Interpretation","text":"<ul> <li>Stable model: Low standard deviation across folds</li> <li>Unstable model: High standard deviation across folds</li> <li>Overfitting: High variation suggests poor generalization</li> </ul>"},{"location":"user-guide/tests/#refresh-stability-test","title":"Refresh Stability Test","text":""},{"location":"user-guide/tests/#purpose_2","title":"Purpose","text":"<p>The refresh stability test evaluates how consistent model performance is over time.</p>"},{"location":"user-guide/tests/#methodology_2","title":"Methodology","text":"<ol> <li>Progressive Training: Trains models on increasing proportions of data</li> <li>Performance Tracking: Measures performance at each refresh point</li> <li>Stability Assessment: Analyzes variation in performance over time</li> </ol>"},{"location":"user-guide/tests/#configuration_2","title":"Configuration","text":"<pre><code>{\n  \"tests\": {\n    \"refresh_stability\": {\n      \"refresh_periods\": [0.5, 0.75, 0.9],\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/tests/#interpretation_2","title":"Interpretation","text":"<ul> <li>Stable model: Consistent performance across refresh periods</li> <li>Unstable model: Performance varies significantly over time</li> <li>Improving model: Performance improves with more data</li> </ul>"},{"location":"user-guide/tests/#perturbation-test","title":"Perturbation Test","text":""},{"location":"user-guide/tests/#purpose_3","title":"Purpose","text":"<p>The perturbation test evaluates how sensitive the model is to small changes in the data.</p>"},{"location":"user-guide/tests/#methodology_3","title":"Methodology","text":"<ol> <li>Data Perturbation: Adds small random noise to input data</li> <li>Model Retraining: Retrains model on perturbed data</li> <li>Performance Comparison: Compares performance with original model</li> <li>Sensitivity Analysis: Measures change in performance</li> </ol>"},{"location":"user-guide/tests/#configuration_3","title":"Configuration","text":"<pre><code>{\n  \"tests\": {\n    \"perturbation\": {\n      \"perturbation_levels\": [0.05, 0.1, 0.15],\n      \"metrics\": [\"mape\", \"rmse\", \"r2\", \"mae\"]\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/tests/#interpretation_3","title":"Interpretation","text":"<ul> <li>Robust model: Performance changes little with perturbations</li> <li>Sensitive model: Performance degrades significantly with perturbations</li> <li>Overfitting: High sensitivity suggests poor generalization</li> </ul>"},{"location":"user-guide/tests/#test-selection","title":"Test Selection","text":""},{"location":"user-guide/tests/#when-to-use-each-test","title":"When to Use Each Test","text":"Test Best For When to Use Accuracy Basic evaluation Initial model assessment Cross-Validation Stability assessment Model comparison Refresh Stability Temporal analysis Long-term planning Perturbation Robustness evaluation Production readiness"},{"location":"user-guide/tests/#recommended-test-combinations","title":"Recommended Test Combinations","text":""},{"location":"user-guide/tests/#basic-evaluation","title":"Basic Evaluation","text":"<pre><code>mmm-eval --test-names accuracy\n</code></pre>"},{"location":"user-guide/tests/#comprehensive-evaluation","title":"Comprehensive Evaluation","text":"<pre><code>mmm-eval --test-names accuracy,cross_validation,refresh_stability,perturbation\n</code></pre>"},{"location":"user-guide/tests/#production-readiness","title":"Production Readiness","text":"<pre><code>mmm-eval --test-names cross_validation,perturbation\n</code></pre>"},{"location":"user-guide/tests/#interpreting-test-results","title":"Interpreting Test Results","text":""},{"location":"user-guide/tests/#single-test-results","title":"Single Test Results","text":"<p>Each test provides specific insights:</p> <ul> <li>Accuracy Test: Overall predictive performance</li> <li>Cross-Validation: Model stability and generalization</li> <li>Refresh Stability: Temporal consistency</li> <li>Perturbation: Robustness to data changes</li> </ul>"},{"location":"user-guide/tests/#combined-results","title":"Combined Results","text":"<p>When all tests are run together:</p> <ol> <li>Check accuracy: Is the model predicting well?</li> <li>Assess stability: Is performance consistent?</li> <li>Evaluate robustness: Is the model reliable?</li> </ol>"},{"location":"user-guide/tests/#example-results","title":"Example Results","text":"<pre><code>{\n  \"accuracy\": {\n    \"mape\": 12.5,\n    \"rmse\": 150.2,\n    \"r2\": 0.87\n  },\n  \"cross_validation\": {\n    \"mape_mean\": 13.2,\n    \"mape_std\": 1.8,\n    \"r2_mean\": 0.85,\n    \"r2_std\": 0.03\n  },\n  \"refresh_stability\": {\n    \"mape_variation\": 2.1,\n    \"r2_variation\": 0.05\n  },\n  \"perturbation\": {\n    \"mape_sensitivity\": 0.15,\n    \"r2_sensitivity\": 0.02\n  }\n}\n</code></pre> <p>Interpretation: - Good accuracy (MAPE 12.5%, R\u00b2 0.87) - Stable cross-validation (low std dev) - Good refresh stability (low variation) - Robust to perturbations (low sensitivity)</p>"},{"location":"user-guide/tests/#test-configuration","title":"Test Configuration","text":""},{"location":"user-guide/tests/#customizing-test-parameters","title":"Customizing Test Parameters","text":"<p>You can customize test parameters through configuration:</p> <pre><code>{\n  \"tests\": {\n    \"accuracy\": {\n      \"train_test_split\": 0.8,\n      \"random_state\": 42\n    },\n    \"cross_validation\": {\n      \"folds\": 10,\n      \"random_state\": 42\n    },\n    \"refresh_stability\": {\n      \"refresh_periods\": [0.3, 0.5, 0.7, 0.9]\n    },\n    \"perturbation\": {\n      \"perturbation_levels\": [0.01, 0.05, 0.1]\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/tests/#environment-variables","title":"Environment Variables","text":"<p>You can also set test parameters via environment variables:</p> <pre><code>export MMM_EVAL_TRAIN_TEST_SPLIT=0.8\nexport MMM_EVAL_CV_FOLDS=5\nexport MMM_EVAL_RANDOM_STATE=42\n</code></pre>"},{"location":"user-guide/tests/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/tests/#test-selection_1","title":"Test Selection","text":"<ol> <li>Start with accuracy: Always run accuracy test first</li> <li>Add stability tests: Include cross-validation for model comparison</li> <li>Consider temporal aspects: Use refresh stability for time series</li> <li>Test robustness: Include perturbation for production models</li> </ol>"},{"location":"user-guide/tests/#result-interpretation","title":"Result Interpretation","text":"<ol> <li>Set benchmarks: Define acceptable performance thresholds</li> <li>Compare models: Use same tests for fair comparison</li> <li>Consider context: Industry-specific benchmarks may apply</li> <li>Monitor trends: Track performance over time</li> </ol>"},{"location":"user-guide/tests/#continuous-testing","title":"Continuous Testing","text":"<ol> <li>Automate testing: Include tests in CI/CD pipeline</li> <li>Regular evaluation: Periodically reassess model performance</li> <li>Alert on degradation: Set up monitoring for performance drops</li> </ol>"},{"location":"user-guide/tests/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Metrics to understand test outputs</li> <li>Check Examples for practical test usage</li> <li>Review Configuration for test customization </li> </ul>"}]}