{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to mmm-eval","text":"<p>A comprehensive evaluation framework for Marketing Mix Modeling (MMM) frameworks.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get started with mmm-eval in just a few steps:</p>"},{"location":"#1-install-mmm-eval","title":"1. Install mmm-eval","text":"<pre><code>pip install mmm-eval\n</code></pre>"},{"location":"#2-prepare-your-data","title":"2. Prepare your data","text":"<p>Your data should include: - Date column - Target variable (e.g., sales, conversions) - Media spend columns - Revenue column (for ROI calculations)</p> <p>Example data structure: <pre><code>date,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,2000\n2023-01-02,1200,8000,5500,2200\n</code></pre></p>"},{"location":"#3-create-a-configuration","title":"3. Create a configuration","text":"<p>For PyMC-Marketing: <pre><code>from pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\nfrom mmm_eval.configs import PyMCConfig\n\nmodel = MMM(\n    date_column=\"date\",\n    channel_columns=[\"tv_spend\", \"digital_spend\"],\n    adstock=GeometricAdstock(l_max=4),\n    saturation=LogisticSaturation()\n)\n\nconfig = PyMCConfig.from_model_object(\n    model_object=model,\n    revenue_column=\"revenue\"\n)\n</code></pre></p>"},{"location":"#4-run-evaluation","title":"4. Run evaluation","text":"<pre><code>benjammmin --input-data-path data.csv --config-path config.json --output-path ./output --framework pymc-marketing\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Get up and running with mmm-eval in minutes.</li> <li>Quick Start - Learn the basics with a hands-on example.</li> <li>Configuration - Configure your MMM frameworks.</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>CLI Reference - Learn how to use mmm-eval effectively.</li> <li>Data Requirements - Understand data format and requirements.</li> <li>Frameworks - Supported MMM frameworks.</li> <li>Tests - Available validation tests.</li> <li>Metrics - Understanding evaluation metrics.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Basic Usage - Practical examples and use cases.</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Contributing - How to contribute to mmm-eval.</li> <li>Setup - Development environment setup.</li> <li>Testing - Testing practices and procedures.</li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#multi-framework-support","title":"Multi-Framework Support","text":"<ul> <li>PyMC-Marketing: Bayesian MMM framework using PyMC</li> <li>Google Meridian: Google's MMM framework</li> <li>Extensible: Easy to add new frameworks</li> </ul>"},{"location":"#comprehensive-testing","title":"Comprehensive Testing","text":"<ul> <li>Accuracy Tests: MAPE, RMSE, R-squared metrics</li> <li>Cross-Validation: Time series cross-validation</li> <li>Refresh Stability: Model stability over time</li> <li>Performance Tests: Computational efficiency metrics</li> </ul>"},{"location":"#standardized-evaluation","title":"Standardized Evaluation","text":"<ul> <li>Consistent metrics across frameworks</li> <li>Reproducible results</li> <li>Industry-standard validation approaches</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#getting-started_1","title":"Getting Started","text":"<p>Get up and running with mmm-eval in minutes.</p>"},{"location":"#user-guide_1","title":"User Guide","text":"<p>Learn how to use mmm-eval effectively.</p>"},{"location":"#examples_1","title":"Examples","text":"<p>Practical examples and use cases.</p>"},{"location":"#development_1","title":"Development","text":"<p>Contribute to mmm-eval development. </p>"},{"location":"about/license/","title":"License","text":"<p>This project is licensed under the Apache License, Version 2.0.</p>"},{"location":"about/license/#apache-license-20","title":"Apache License 2.0","text":"<p>Copyright 2025 mmm-eval Contributors</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"about/license/#key-features-of-apache-20","title":"Key Features of Apache 2.0","text":"<p>The Apache License 2.0 provides:</p> <ul> <li>Patent protection: Contributors grant patent licenses for their contributions</li> <li>Commercial use: Allows commercial use, modification, and distribution</li> <li>Patent termination: Patent licenses terminate if you file patent litigation</li> <li>Attribution: Requires preservation of copyright notices and license text</li> <li>Contributor protection: Protects contributors from liability</li> </ul>"},{"location":"about/license/#contributing","title":"Contributing","text":"<p>By contributing to this project, you agree that your contributions will be licensed under the Apache License, Version 2.0.</p>"},{"location":"about/license/#full-license-text","title":"Full License Text","text":"<p>For the complete license text, see the LICENSE file in the root of this repository. </p>"},{"location":"api/adapters/","title":"Adapters Reference","text":""},{"location":"api/adapters/#mmm_eval.adapters","title":"<code>mmm_eval.adapters</code>","text":"<p>Adapters for different MMM frameworks.</p>"},{"location":"api/adapters/#mmm_eval.adapters-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter","title":"<code>MeridianAdapter(config: MeridianConfig)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for Google Meridian MMM framework.</p> <p>Initialize the Meridian adapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MeridianConfig</code> <p>MeridianConfig object</p> required Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def __init__(self, config: MeridianConfig):\n    \"\"\"Initialize the Meridian adapter.\n\n    Args:\n        config: MeridianConfig object\n\n    \"\"\"\n    self.config = config\n    self.input_data_builder_schema = config.input_data_builder_config\n\n    self.date_column = config.date_column\n    self.channel_spend_columns = self.input_data_builder_schema.channel_spend_columns\n\n    # Initialize stateful attributes to None/False\n    self._reset_state()\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter-attributes","title":"Attributes","text":""},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter.media_channels","title":"<code>media_channels: list[str]</code>  <code>property</code>","text":"<p>Return the channel names used by this adapter.</p> <p>For Meridian, this returns the human-readable channel names from the config.</p> <p>Returns     List of channel names</p>"},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter.primary_media_regressor_columns","title":"<code>primary_media_regressor_columns: list[str]</code>  <code>property</code>","text":"<p>Return the primary media regressor columns that should be perturbed in tests.</p> <p>For Meridian, this depends on the configuration: - If channel_reach_columns is provided: returns empty list (not supported in perturbation tests) - If channel_impressions_columns is provided: returns channel_impressions_columns - Otherwise: returns channel_spend_columns</p> <p>Returns     List of column names that are used as primary media regressors in the model</p>"},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter.primary_media_regressor_type","title":"<code>primary_media_regressor_type: PrimaryMediaRegressor</code>  <code>property</code>","text":"<p>Return the type of primary media regressors used by the model.</p> <p>For Meridian, this is determined by the configuration: - If channel_reach_columns is provided: returns PrimaryMediaRegressor.REACH_AND_FREQUENCY - If channel_impressions_columns is provided: returns PrimaryMediaRegressor.IMPRESSIONS - Otherwise: returns PrimaryMediaRegressor.SPEND</p> <p>Returns     PrimaryMediaRegressor enum value</p>"},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter-functions","title":"Functions","text":""},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter.fit","title":"<code>fit(data: pd.DataFrame, max_train_date: pd.Timestamp | None = None) -&gt; None</code>","text":"<p>Fit the Meridian model to data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Training data</p> required <code>max_train_date</code> <code>Timestamp | None</code> <p>Optional maximum training date for holdout validation</p> <code>None</code> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def fit(self, data: pd.DataFrame, max_train_date: pd.Timestamp | None = None) -&gt; None:\n    \"\"\"Fit the Meridian model to data.\n\n    Args:\n        data: Training data\n        max_train_date: Optional maximum training date for holdout validation\n\n    \"\"\"\n    # Reset state to ensure clean start when new data is provided\n    self._reset_state()\n\n    # build Meridian data object\n    self.training_data = construct_meridian_data_object(data, self.config)\n    self.max_train_date = max_train_date\n\n    model_spec_kwargs = dict(self.config.model_spec_config)\n\n    # if max train date is provided, construct a mask that is True for all dates before max_train_date\n    if self.max_train_date:\n        self.holdout_mask = construct_holdout_mask(self.max_train_date, self.training_data.kpi.time)\n        # model expects a 2D array of shape (n_geos, n_times) so have to duplicate the values across each geo\n        model_spec_kwargs[\"holdout_id\"] = np.repeat(\n            self.holdout_mask[None, :], repeats=len(self.training_data.kpi.geo), axis=0\n        )\n\n    # Create and fit the Meridian model\n    model_spec = ModelSpec(**model_spec_kwargs)\n    self.model = Meridian(\n        input_data=self.training_data,  # type: ignore\n        model_spec=model_spec,\n    )\n    self.trace = self.model.sample_posterior(**dict(self.config.sample_posterior_config))\n\n    # used to compute channel contributions for ROI calculations\n    self.analyzer = Analyzer(self.model)\n    self.is_fitted = True\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter.fit_and_predict","title":"<code>fit_and_predict(train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray</code>","text":"<p>Fit the Meridian model and make predictions given new input data.</p> <p>The full dataset must be passed to <code>fit()</code>, since making out-of-sample predictions is only possible by way of specifying a holdout mask when sampling from the posterior.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>DataFrame</code> <p>Training data</p> required <code>test</code> <code>DataFrame</code> <p>Test data</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values for the test period</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def fit_and_predict(self, train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Fit the Meridian model and make predictions given new input data.\n\n    The full dataset must be passed to `fit()`, since making out-of-sample predictions\n    is only possible by way of specifying a holdout mask when sampling from the\n    posterior.\n\n    Args:\n        train: Training data\n        test: Test data\n\n    Returns:\n        Predicted values for the test period\n\n    \"\"\"\n    train_and_test = pd.concat([train, test])\n    max_train_date = train[self.date_column].squeeze().max()\n    self.fit(train_and_test, max_train_date=max_train_date)\n    return self.predict()\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter.get_channel_names","title":"<code>get_channel_names() -&gt; list[str]</code>","text":"<p>Get the channel names that would be used as the index in get_channel_roi results.</p> <p>For Meridian, this returns the media_channels which are the human-readable channel names used in the ROI results.</p> <p>Returns     List of channel names</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def get_channel_names(self) -&gt; list[str]:\n    \"\"\"Get the channel names that would be used as the index in get_channel_roi results.\n\n    For Meridian, this returns the media_channels which are the human-readable\n    channel names used in the ROI results.\n\n    Returns\n        List of channel names\n\n    \"\"\"\n    return self.media_channels\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter.get_channel_roi","title":"<code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code>","text":"<p>Get channel ROI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Get channel ROI estimates.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted or self.training_data is None or self.analyzer is None:\n        raise RuntimeError(\"Model must be fit before computing ROI\")\n\n    # restrict ROI calculation to a particular period if start/end date args are\n    # passed\n    training_date_index = pd.to_datetime(self.training_data.kpi.time)\n    roi_date_index = training_date_index.copy()\n    if start_date:\n        roi_date_index = roi_date_index[roi_date_index &gt;= start_date]\n    if end_date:\n        roi_date_index = roi_date_index[roi_date_index &lt; end_date]\n\n    selected_times = [bool(date) for date in training_date_index.isin(roi_date_index)]\n\n    # analyzer.roi() returns a tensor of shape (n_chains, n_draws, n_channels)\n    rois_per_channel = np.mean(self.analyzer.roi(selected_times=selected_times), axis=(0, 1))\n\n    rois = {}\n    for channel, roi in zip(self.media_channels, rois_per_channel, strict=False):\n        rois[channel] = float(roi)\n    return pd.Series(rois)\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.MeridianAdapter.predict","title":"<code>predict(data: pd.DataFrame | None = None) -&gt; np.ndarray</code>","text":"<p>Make predictions using the fitted model.</p> <p>This returns predictions for the entirety of the dataset passed to fit() unless <code>max_train_date</code> is specified when calling fit(); in that case it only returns predictions for the time periods indicated by the holdout mask.</p> <p>Note: Meridian doesn't require input data for prediction - it uses the fitted model state, so the <code>data</code> argument will be ignored if passed.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | None</code> <p>Ignored - Meridian uses the fitted model state for predictions.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If model is not fitted</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def predict(self, data: pd.DataFrame | None = None) -&gt; np.ndarray:\n    \"\"\"Make predictions using the fitted model.\n\n    This returns predictions for the entirety of the dataset passed to fit() unless\n    `max_train_date` is specified when calling fit(); in that case it only returns\n    predictions for the time periods indicated by the holdout mask.\n\n    Note: Meridian doesn't require input data for prediction - it uses the fitted\n    model state, so the `data` argument will be ignored if passed.\n\n    Args:\n        data: Ignored - Meridian uses the fitted model state for predictions.\n\n    Returns:\n        Predicted values\n\n    Raises:\n        RuntimeError: If model is not fitted\n\n    \"\"\"\n    if not self.is_fitted or self.analyzer is None:\n        raise RuntimeError(\"Model must be fit before prediction\")\n\n    # shape (n_chains, n_draws, n_times)\n    preds_tensor = self.analyzer.expected_outcome(aggregate_geos=True, aggregate_times=False)\n    posterior_mean = np.mean(preds_tensor, axis=(0, 1))\n\n    # if holdout mask is provided, use it to mask the predictions to restrict only to the\n    # holdout period\n    if self.holdout_mask is not None:\n        posterior_mean = posterior_mean[self.holdout_mask]\n\n    return posterior_mean\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter","title":"<code>PyMCAdapter(config: PyMCConfig)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Initialize the PyMCAdapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PyMCConfig</code> <p>PyMCConfig object</p> required Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def __init__(self, config: PyMCConfig):\n    \"\"\"Initialize the PyMCAdapter.\n\n    Args:\n        config: PyMCConfig object\n\n    \"\"\"\n    self.model_kwargs = config.pymc_model_config_dict\n    self.fit_kwargs = config.fit_config_dict\n    self.predict_kwargs = config.predict_config_dict\n    self.date_column = config.date_column\n    self.channel_spend_columns = config.channel_columns\n    self.control_columns = config.control_columns\n    self.model = None\n    self.trace = None\n    self._channel_roi_df = None\n    self.is_fitted = False\n\n    # Store original values to reset on subsequent fit calls\n    self._original_channel_spend_columns = config.channel_columns.copy()\n    self._original_model_kwargs = config.pymc_model_config_dict.copy()\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter-attributes","title":"Attributes","text":""},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.media_channels","title":"<code>media_channels: list[str]</code>  <code>property</code>","text":"<p>Return the channel names used by this adapter.</p> <p>For PyMC, this returns the channel_spend_columns which are used as the channel names in ROI results.</p> <p>Returns     List of channel names</p>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.primary_media_regressor_columns","title":"<code>primary_media_regressor_columns: list[str]</code>  <code>property</code>","text":"<p>Return the primary media regressor columns that should be perturbed in tests.</p> <p>For PyMC, this is always the <code>channel_spend_columns</code> since the PyMC adapter uses spend as the primary regressor in the model.</p> <p>Returns     List of channel spend column names</p>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.primary_media_regressor_type","title":"<code>primary_media_regressor_type: PrimaryMediaRegressor</code>  <code>property</code>","text":"<p>Return the type of primary media regressors used by this adapter.</p> <p>For PyMC, this is always SPEND since the PyMC adapter uses spend as the primary regressor.</p> <p>Returns     PrimaryMediaRegressor.SPEND</p>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter-functions","title":"Functions","text":""},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.fit","title":"<code>fit(data: pd.DataFrame) -&gt; None</code>","text":"<p>Fit the model and compute ROIs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the training data adhering to the PyMCInputDataSchema.</p> required Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the model and compute ROIs.\n\n    Args:\n        data: DataFrame containing the training data adhering to the PyMCInputDataSchema.\n\n    \"\"\"\n    # Reset to original values at the start of each fit call\n    self.channel_spend_columns = self._original_channel_spend_columns.copy()\n    self.model_kwargs = self._original_model_kwargs.copy()\n\n    # Identify channel spend columns that sum to zero and remove them from modelling.\n    # We cannot reliably make any prediction based on these channels when making\n    # predictions on new data.\n    channel_spend_data = data[self.channel_spend_columns]\n    zero_spend_channels = channel_spend_data.columns[channel_spend_data.sum() == 0].tolist()\n\n    if zero_spend_channels:\n        logger.info(f\"Dropping channels with zero spend: {zero_spend_channels}\")\n        # Remove zero-spend channels from the list passed to the MMM constructor\n        self.channel_spend_columns = [col for col in self.channel_spend_columns if col not in zero_spend_channels]\n        # also update the model config field to reflect the new channel spend columns\n        self.model_kwargs[\"channel_columns\"] = self.channel_spend_columns\n\n        # Check for vector priors that might cause shape mismatches\n        _check_vector_priors_when_dropping_channels(self.model_kwargs[\"model_config\"], zero_spend_channels)\n\n        data = data.drop(columns=zero_spend_channels)\n\n    X = data.drop(columns=[InputDataframeConstants.RESPONSE_COL, InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL])\n    y = data[InputDataframeConstants.RESPONSE_COL]\n\n    self.model = MMM(**self.model_kwargs)\n    self.trace = self.model.fit(X=X, y=y, **self.fit_kwargs)\n\n    self._channel_roi_df = self._compute_channel_contributions(data)\n    self.is_fitted = True\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.fit_and_predict","title":"<code>fit_and_predict(train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray</code>","text":"<p>Fit on training data and make predictions on test data.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>DataFrame</code> <p>training dataset</p> required <code>test</code> <code>DataFrame</code> <p>test dataset</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>model predictions.</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def fit_and_predict(self, train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Fit on training data and make predictions on test data.\n\n    Arguments:\n        train: training dataset\n        test: test dataset\n\n    Returns:\n        model predictions.\n\n    \"\"\"\n    self.fit(train)\n    return self.predict(test)\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.get_channel_names","title":"<code>get_channel_names() -&gt; list[str]</code>","text":"<p>Get the channel names that would be used as the index in get_channel_roi results.</p> <p>For PyMC, this returns the <code>channel_spend_columns</code> which are used as the index in the ROI results.</p> <p>Returns     List of channel names</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def get_channel_names(self) -&gt; list[str]:\n    \"\"\"Get the channel names that would be used as the index in get_channel_roi results.\n\n    For PyMC, this returns the `channel_spend_columns` which are used as the index\n    in the ROI results.\n\n    Returns\n        List of channel names\n\n    \"\"\"\n    return self.channel_spend_columns\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.get_channel_roi","title":"<code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code>","text":"<p>Return the ROIs for each channel, optionally within a given date range.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Return the ROIs for each channel, optionally within a given date range.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted or self._channel_roi_df is None:\n        raise RuntimeError(\"Model must be fit before computing ROI.\")\n\n    _validate_start_end_dates(start_date, end_date, pd.DatetimeIndex(self._channel_roi_df.index))\n\n    # Filter the contribution DataFrame by date range\n    date_range_df = self._channel_roi_df.loc[start_date:end_date]\n\n    if date_range_df.empty:\n        raise ValueError(f\"No data found for date range {start_date} to {end_date}\")\n\n    return pd.Series(self._calculate_rois(date_range_df))\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.PyMCAdapter.predict","title":"<code>predict(data: pd.DataFrame | None = None) -&gt; np.ndarray</code>","text":"<p>Predict the response variable for new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | None</code> <p>Input data for prediction. This parameter is required for PyMC predictions and cannot be None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If model is not fitted</p> <code>ValueError</code> <p>If data is None (PyMC requires data for prediction)</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def predict(self, data: pd.DataFrame | None = None) -&gt; np.ndarray:\n    \"\"\"Predict the response variable for new data.\n\n    Args:\n        data: Input data for prediction. This parameter is required for PyMC\n            predictions and cannot be None.\n\n    Returns:\n        Predicted values\n\n    Raises:\n        RuntimeError: If model is not fitted\n        ValueError: If data is None (PyMC requires data for prediction)\n\n    \"\"\"\n    if not self.is_fitted or self.model is None:\n        raise RuntimeError(\"Model must be fit before prediction.\")\n\n    if data is None:\n        raise ValueError(\"PyMC adapter requires data for prediction\")\n\n    if InputDataframeConstants.RESPONSE_COL in data.columns:\n        data = data.drop(columns=[InputDataframeConstants.RESPONSE_COL])\n    predictions = predictions = self.model.predict(\n        data, extend_idata=False, include_last_observations=True, **self.predict_kwargs\n    )\n    return predictions\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters-functions","title":"Functions","text":""},{"location":"api/adapters/#mmm_eval.adapters.get_adapter","title":"<code>get_adapter(framework: str, config: PyMCConfig | MeridianConfig)</code>","text":"<p>Get an adapter instance for the specified framework.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>Name of the MMM framework</p> required <code>config</code> <code>PyMCConfig | MeridianConfig</code> <p>Framework-specific configuration</p> required <p>Returns:</p> Type Description <p>Adapter instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If framework is not supported</p> Source code in <code>mmm_eval/adapters/__init__.py</code> <pre><code>def get_adapter(framework: str, config: PyMCConfig | MeridianConfig):\n    \"\"\"Get an adapter instance for the specified framework.\n\n    Args:\n        framework: Name of the MMM framework\n        config: Framework-specific configuration\n\n    Returns:\n        Adapter instance\n\n    Raises:\n        ValueError: If framework is not supported\n\n    \"\"\"\n    if framework not in ADAPTER_REGISTRY:\n        raise ValueError(f\"Unsupported framework: {framework}. Available: {list(ADAPTER_REGISTRY.keys())}\")\n\n    adapter_class = ADAPTER_REGISTRY[framework]\n    return adapter_class(config)\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters-modules","title":"Modules","text":""},{"location":"api/adapters/#mmm_eval.adapters.base","title":"<code>base</code>","text":"<p>Base adapter class for MMM frameworks.</p>"},{"location":"api/adapters/#mmm_eval.adapters.base-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.base.BaseAdapter","title":"<code>BaseAdapter(config: dict[str, Any] | None = None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for MMM framework adapters.</p> <p>Initialize the base adapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Configuration dictionary</p> <code>None</code> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"Initialize the base adapter.\n\n    Args:\n        config: Configuration dictionary\n\n    \"\"\"\n    self.config = config or {}\n    self.is_fitted = False\n    self.channel_spend_columns: list[str] = []\n    self.date_column: str\n</code></pre> Attributes\u00b6 <code>media_channels: list[str]</code> <code>abstractmethod</code> <code>property</code> \u00b6 <p>Return the channel names used by this adapter.</p> <p>This property provides a consistent way to get channel names across different adapters. For most frameworks, this will be human-readable channel names, but for PyMC it may be the column names themselves.</p> <p>Returns     List of channel names used by this adapter</p> <code>primary_media_regressor_columns: list[str]</code> <code>abstractmethod</code> <code>property</code> \u00b6 <p>Return the primary media regressor columns that should be perturbed in tests.</p> <p>This property returns the columns that are actually used as regressors in the model. For most frameworks, this will be the spend columns, but for e.g. Meridian it could be impressions or reach/frequency columns depending on the configuration.</p> <p>Returns     List of column names that are used as primary media regressors in the model</p> <code>primary_media_regressor_type: PrimaryMediaRegressor</code> <code>abstractmethod</code> <code>property</code> \u00b6 <p>Return the type of primary media regressors used by this adapter.</p> <p>This property indicates what type of regressors are used as primary inputs to the model, which determines what should be perturbed in tests.</p> <p>Returns     PrimaryMediaRegressor enum value indicating the type of primary media regressors</p> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> <code>abstractmethod</code> \u00b6 <p>Fit the model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Training data</p> required Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the model to the data.\n\n    Args:\n        data: Training data\n\n    \"\"\"\n    pass\n</code></pre> <code>fit_and_predict(train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray</code> <code>abstractmethod</code> \u00b6 <p>Fit on training data and make predictions on test data.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>DataFrame</code> <p>dataset to train model on</p> required <code>test</code> <code>DataFrame</code> <p>dataset to make predictions using</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values.</p> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef fit_and_predict(self, train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Fit on training data and make predictions on test data.\n\n    Args:\n        train: dataset to train model on\n        test: dataset to make predictions using\n\n    Returns:\n        Predicted values.\n\n    \"\"\"\n    pass\n</code></pre> <code>get_channel_names() -&gt; list[str]</code> <code>abstractmethod</code> \u00b6 <p>Get the channel names that would be used as the index in channel ROI results.</p> <p>This method provides a consistent way to get channel names across different adapters without needing to call get_channel_roi() (which requires the model to be fitted).</p> <p>Returns     List of channel names that would be used as the index in get_channel_roi results</p> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef get_channel_names(self) -&gt; list[str]:  # pyright: ignore[reportReturnType]\n    \"\"\"Get the channel names that would be used as the index in channel ROI results.\n\n    This method provides a consistent way to get channel names across different adapters\n    without needing to call get_channel_roi() (which requires the model to be fitted).\n\n    Returns\n        List of channel names that would be used as the index in get_channel_roi results\n\n    \"\"\"\n    pass\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> <code>abstractmethod</code> \u00b6 <p>Get channel ROI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Get channel ROI estimates.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    pass\n</code></pre> <code>predict(data: pd.DataFrame | None = None) -&gt; np.ndarray</code> <code>abstractmethod</code> \u00b6 <p>Make predictions on new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | None</code> <p>Input data for prediction. Behavior varies by adapter: - Some adapters (e.g., PyMC) require this parameter and will raise   an error if None is passed - Other adapters (e.g., Meridian) ignore this parameter and use   the fitted model state instead</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> Source code in <code>mmm_eval/adapters/base.py</code> <pre><code>@abstractmethod\ndef predict(self, data: pd.DataFrame | None = None) -&gt; np.ndarray:\n    \"\"\"Make predictions on new data.\n\n    Args:\n        data: Input data for prediction. Behavior varies by adapter:\n            - Some adapters (e.g., PyMC) require this parameter and will raise\n              an error if None is passed\n            - Other adapters (e.g., Meridian) ignore this parameter and use\n              the fitted model state instead\n\n    Returns:\n        Predicted values\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.base.PrimaryMediaRegressor","title":"<code>PrimaryMediaRegressor</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for primary media regressor types used in MMM frameworks.</p>"},{"location":"api/adapters/#mmm_eval.adapters.meridian","title":"<code>meridian</code>","text":"<p>Google Meridian adapter for MMM evaluation.</p>"},{"location":"api/adapters/#mmm_eval.adapters.meridian-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.meridian.MeridianAdapter","title":"<code>MeridianAdapter(config: MeridianConfig)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for Google Meridian MMM framework.</p> <p>Initialize the Meridian adapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MeridianConfig</code> <p>MeridianConfig object</p> required Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def __init__(self, config: MeridianConfig):\n    \"\"\"Initialize the Meridian adapter.\n\n    Args:\n        config: MeridianConfig object\n\n    \"\"\"\n    self.config = config\n    self.input_data_builder_schema = config.input_data_builder_config\n\n    self.date_column = config.date_column\n    self.channel_spend_columns = self.input_data_builder_schema.channel_spend_columns\n\n    # Initialize stateful attributes to None/False\n    self._reset_state()\n</code></pre> Attributes\u00b6 <code>media_channels: list[str]</code> <code>property</code> \u00b6 <p>Return the channel names used by this adapter.</p> <p>For Meridian, this returns the human-readable channel names from the config.</p> <p>Returns     List of channel names</p> <code>primary_media_regressor_columns: list[str]</code> <code>property</code> \u00b6 <p>Return the primary media regressor columns that should be perturbed in tests.</p> <p>For Meridian, this depends on the configuration: - If channel_reach_columns is provided: returns empty list (not supported in perturbation tests) - If channel_impressions_columns is provided: returns channel_impressions_columns - Otherwise: returns channel_spend_columns</p> <p>Returns     List of column names that are used as primary media regressors in the model</p> <code>primary_media_regressor_type: PrimaryMediaRegressor</code> <code>property</code> \u00b6 <p>Return the type of primary media regressors used by the model.</p> <p>For Meridian, this is determined by the configuration: - If channel_reach_columns is provided: returns PrimaryMediaRegressor.REACH_AND_FREQUENCY - If channel_impressions_columns is provided: returns PrimaryMediaRegressor.IMPRESSIONS - Otherwise: returns PrimaryMediaRegressor.SPEND</p> <p>Returns     PrimaryMediaRegressor enum value</p> Functions\u00b6 <code>fit(data: pd.DataFrame, max_train_date: pd.Timestamp | None = None) -&gt; None</code> \u00b6 <p>Fit the Meridian model to data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Training data</p> required <code>max_train_date</code> <code>Timestamp | None</code> <p>Optional maximum training date for holdout validation</p> <code>None</code> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def fit(self, data: pd.DataFrame, max_train_date: pd.Timestamp | None = None) -&gt; None:\n    \"\"\"Fit the Meridian model to data.\n\n    Args:\n        data: Training data\n        max_train_date: Optional maximum training date for holdout validation\n\n    \"\"\"\n    # Reset state to ensure clean start when new data is provided\n    self._reset_state()\n\n    # build Meridian data object\n    self.training_data = construct_meridian_data_object(data, self.config)\n    self.max_train_date = max_train_date\n\n    model_spec_kwargs = dict(self.config.model_spec_config)\n\n    # if max train date is provided, construct a mask that is True for all dates before max_train_date\n    if self.max_train_date:\n        self.holdout_mask = construct_holdout_mask(self.max_train_date, self.training_data.kpi.time)\n        # model expects a 2D array of shape (n_geos, n_times) so have to duplicate the values across each geo\n        model_spec_kwargs[\"holdout_id\"] = np.repeat(\n            self.holdout_mask[None, :], repeats=len(self.training_data.kpi.geo), axis=0\n        )\n\n    # Create and fit the Meridian model\n    model_spec = ModelSpec(**model_spec_kwargs)\n    self.model = Meridian(\n        input_data=self.training_data,  # type: ignore\n        model_spec=model_spec,\n    )\n    self.trace = self.model.sample_posterior(**dict(self.config.sample_posterior_config))\n\n    # used to compute channel contributions for ROI calculations\n    self.analyzer = Analyzer(self.model)\n    self.is_fitted = True\n</code></pre> <code>fit_and_predict(train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray</code> \u00b6 <p>Fit the Meridian model and make predictions given new input data.</p> <p>The full dataset must be passed to <code>fit()</code>, since making out-of-sample predictions is only possible by way of specifying a holdout mask when sampling from the posterior.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>DataFrame</code> <p>Training data</p> required <code>test</code> <code>DataFrame</code> <p>Test data</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values for the test period</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def fit_and_predict(self, train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Fit the Meridian model and make predictions given new input data.\n\n    The full dataset must be passed to `fit()`, since making out-of-sample predictions\n    is only possible by way of specifying a holdout mask when sampling from the\n    posterior.\n\n    Args:\n        train: Training data\n        test: Test data\n\n    Returns:\n        Predicted values for the test period\n\n    \"\"\"\n    train_and_test = pd.concat([train, test])\n    max_train_date = train[self.date_column].squeeze().max()\n    self.fit(train_and_test, max_train_date=max_train_date)\n    return self.predict()\n</code></pre> <code>get_channel_names() -&gt; list[str]</code> \u00b6 <p>Get the channel names that would be used as the index in get_channel_roi results.</p> <p>For Meridian, this returns the media_channels which are the human-readable channel names used in the ROI results.</p> <p>Returns     List of channel names</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def get_channel_names(self) -&gt; list[str]:\n    \"\"\"Get the channel names that would be used as the index in get_channel_roi results.\n\n    For Meridian, this returns the media_channels which are the human-readable\n    channel names used in the ROI results.\n\n    Returns\n        List of channel names\n\n    \"\"\"\n    return self.media_channels\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> \u00b6 <p>Get channel ROI estimates.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Get channel ROI estimates.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted or self.training_data is None or self.analyzer is None:\n        raise RuntimeError(\"Model must be fit before computing ROI\")\n\n    # restrict ROI calculation to a particular period if start/end date args are\n    # passed\n    training_date_index = pd.to_datetime(self.training_data.kpi.time)\n    roi_date_index = training_date_index.copy()\n    if start_date:\n        roi_date_index = roi_date_index[roi_date_index &gt;= start_date]\n    if end_date:\n        roi_date_index = roi_date_index[roi_date_index &lt; end_date]\n\n    selected_times = [bool(date) for date in training_date_index.isin(roi_date_index)]\n\n    # analyzer.roi() returns a tensor of shape (n_chains, n_draws, n_channels)\n    rois_per_channel = np.mean(self.analyzer.roi(selected_times=selected_times), axis=(0, 1))\n\n    rois = {}\n    for channel, roi in zip(self.media_channels, rois_per_channel, strict=False):\n        rois[channel] = float(roi)\n    return pd.Series(rois)\n</code></pre> <code>predict(data: pd.DataFrame | None = None) -&gt; np.ndarray</code> \u00b6 <p>Make predictions using the fitted model.</p> <p>This returns predictions for the entirety of the dataset passed to fit() unless <code>max_train_date</code> is specified when calling fit(); in that case it only returns predictions for the time periods indicated by the holdout mask.</p> <p>Note: Meridian doesn't require input data for prediction - it uses the fitted model state, so the <code>data</code> argument will be ignored if passed.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | None</code> <p>Ignored - Meridian uses the fitted model state for predictions.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If model is not fitted</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def predict(self, data: pd.DataFrame | None = None) -&gt; np.ndarray:\n    \"\"\"Make predictions using the fitted model.\n\n    This returns predictions for the entirety of the dataset passed to fit() unless\n    `max_train_date` is specified when calling fit(); in that case it only returns\n    predictions for the time periods indicated by the holdout mask.\n\n    Note: Meridian doesn't require input data for prediction - it uses the fitted\n    model state, so the `data` argument will be ignored if passed.\n\n    Args:\n        data: Ignored - Meridian uses the fitted model state for predictions.\n\n    Returns:\n        Predicted values\n\n    Raises:\n        RuntimeError: If model is not fitted\n\n    \"\"\"\n    if not self.is_fitted or self.analyzer is None:\n        raise RuntimeError(\"Model must be fit before prediction\")\n\n    # shape (n_chains, n_draws, n_times)\n    preds_tensor = self.analyzer.expected_outcome(aggregate_geos=True, aggregate_times=False)\n    posterior_mean = np.mean(preds_tensor, axis=(0, 1))\n\n    # if holdout mask is provided, use it to mask the predictions to restrict only to the\n    # holdout period\n    if self.holdout_mask is not None:\n        posterior_mean = posterior_mean[self.holdout_mask]\n\n    return posterior_mean\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.meridian-functions","title":"Functions","text":""},{"location":"api/adapters/#mmm_eval.adapters.meridian.construct_holdout_mask","title":"<code>construct_holdout_mask(max_train_date: pd.Timestamp, time_index: np.ndarray) -&gt; np.ndarray</code>","text":"<p>Construct a boolean mask for holdout period identification.</p> <p>This function creates a boolean mask that identifies which time periods fall into the holdout/test period (after the maximum training date). The mask can be used to separate training and test data or to filter predictions to only the holdout period.</p> <p>Parameters:</p> Name Type Description Default <code>max_train_date</code> <code>Timestamp</code> <p>The maximum date to be considered part of the training period. All dates after this will be marked as holdout/test data.</p> required <code>time_index</code> <code>ndarray</code> <p>Array-like object containing the time index to be masked. Can be any format that pandas.to_datetime can convert.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A boolean array of the same length as time_index, where True indicates</p> <code>ndarray</code> <p>the time period is in the holdout/test set (after max_train_date).</p> Example <p>time_index = pd.date_range('2023-01-01', '2023-12-31', freq='D') max_train_date = pd.Timestamp('2023-06-30') mask = construct_holdout_mask(max_train_date, time_index)</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def construct_holdout_mask(max_train_date: pd.Timestamp, time_index: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Construct a boolean mask for holdout period identification.\n\n    This function creates a boolean mask that identifies which time periods fall into\n    the holdout/test period (after the maximum training date). The mask can be used\n    to separate training and test data or to filter predictions to only the holdout period.\n\n    Args:\n        max_train_date: The maximum date to be considered part of the training period.\n            All dates after this will be marked as holdout/test data.\n        time_index: Array-like object containing the time index to be masked.\n            Can be any format that pandas.to_datetime can convert.\n\n    Returns:\n        A boolean array of the same length as time_index, where True indicates\n        the time period is in the holdout/test set (after max_train_date).\n\n    Example:\n        &gt;&gt;&gt; time_index = pd.date_range('2023-01-01', '2023-12-31', freq='D')\n        &gt;&gt;&gt; max_train_date = pd.Timestamp('2023-06-30')\n        &gt;&gt;&gt; mask = construct_holdout_mask(max_train_date, time_index)\n        &gt;&gt;&gt; # mask will be True for dates from 2023-07-01 onwards\n\n    \"\"\"\n    full_index = pd.to_datetime(time_index)\n    test_index = full_index[full_index &gt; max_train_date]\n\n    return full_index.isin(test_index)\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.meridian.construct_holdout_mask--mask-will-be-true-for-dates-from-2023-07-01-onwards","title":"mask will be True for dates from 2023-07-01 onwards","text":""},{"location":"api/adapters/#mmm_eval.adapters.meridian.construct_meridian_data_object","title":"<code>construct_meridian_data_object(df: pd.DataFrame, config: MeridianConfig) -&gt; pd.DataFrame</code>","text":"<p>Construct a Meridian data object from a pandas DataFrame.</p> <p>This function transforms a standard DataFrame into the format required by the Meridian framework. It handles the conversion of revenue to revenue_per_kpi, and configures the data builder with various components including KPI, population, controls, media channels (with different types: reach/frequency, impressions, or spend-only), organic media, and non-media treatments.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the raw data with columns for dates, response, revenue, media spend, and other variables as specified in the config.</p> required <code>config</code> <code>MeridianConfig</code> <p>MeridianConfig object containing the configuration for data transformation including column mappings and feature specifications.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Meridian data object built from the input DataFrame according to the config.</p> Note <p>The function modifies the input DataFrame by: - Converting revenue to revenue_per_kpi (revenue / response) - Dropping the original revenue column - Adding various media and control components based on config - Validating that media channels have sufficient variation (non-zero spend)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a media channel has zero spend across all time periods and geos,        as this would cause Meridian to fail or produce unreliable results.</p> Source code in <code>mmm_eval/adapters/meridian.py</code> <pre><code>def construct_meridian_data_object(df: pd.DataFrame, config: MeridianConfig) -&gt; pd.DataFrame:\n    \"\"\"Construct a Meridian data object from a pandas DataFrame.\n\n    This function transforms a standard DataFrame into the format required by the Meridian\n    framework. It handles the conversion of revenue to revenue_per_kpi, and configures\n    the data builder with various components including KPI, population, controls,\n    media channels (with different types: reach/frequency, impressions, or spend-only),\n    organic media, and non-media treatments.\n\n    Args:\n        df: Input DataFrame containing the raw data with columns for dates, response,\n            revenue, media spend, and other variables as specified in the config.\n        config: MeridianConfig object containing the configuration for data transformation\n            including column mappings and feature specifications.\n\n    Returns:\n        A Meridian data object built from the input DataFrame according to the config.\n\n    Note:\n        The function modifies the input DataFrame by:\n        - Converting revenue to revenue_per_kpi (revenue / response)\n        - Dropping the original revenue column\n        - Adding various media and control components based on config\n        - Validating that media channels have sufficient variation (non-zero spend)\n\n    Raises:\n        ValueError: If a media channel has zero spend across all time periods and geos,\n                   as this would cause Meridian to fail or produce unreliable results.\n\n    \"\"\"\n    df = df.copy()\n\n    # Validate media channels have sufficient variation\n    _validate_media_channels(df, config)\n\n    # convert from \"revenue\" to \"revenue_per_kpi\"\n    df[REVENUE_PER_KPI_COL] = (\n        df[InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL] / df[InputDataframeConstants.RESPONSE_COL]\n    )\n    df = df.drop(columns=InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL)\n\n    input_data_builder_schema = config.input_data_builder_config\n\n    # KPI, population, and control variables\n    builder = (\n        data_builder.DataFrameInputDataBuilder(kpi_type=\"non_revenue\")\n        .with_kpi(df, time_col=config.date_column, kpi_col=InputDataframeConstants.RESPONSE_COL)\n        .with_revenue_per_kpi(df, time_col=config.date_column, revenue_per_kpi_col=REVENUE_PER_KPI_COL)\n    )\n    # population, if provided, needs to be called \"population\" in the DF\n    if \"population\" in df.columns:\n        builder = builder.with_population(df)\n\n    # controls (non-intervenable, e.g. macroeconomics)\n    if input_data_builder_schema.control_columns:\n        builder = builder.with_controls(\n            df, time_col=config.date_column, control_cols=input_data_builder_schema.control_columns\n        )\n\n    # paid media\n    builder = _add_media_to_data_builder(df, builder, input_data_builder_schema, config.date_column)\n\n    # organic media\n    if input_data_builder_schema.organic_media_columns:\n        builder = builder.with_organic_media(\n            df,\n            organic_media_cols=input_data_builder_schema.organic_media_columns,\n            organic_media_channels=input_data_builder_schema.organic_media_channels,\n            media_time_col=config.date_column,\n        )\n\n    # non-media treatments (anything that is \"intervenable\", e.g. pricing/promotions)\n    if input_data_builder_schema.non_media_treatment_columns:\n        builder = builder.with_non_media_treatments(\n            df,\n            non_media_treatment_cols=input_data_builder_schema.non_media_treatment_columns,\n            time_col=config.date_column,\n        )\n\n    return builder.build()\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.pymc","title":"<code>pymc</code>","text":"<p>PyMC MMM framework adapter.</p> <p>N.B. we expect control variables to be scaled to [-1, 1] using maxabs scaling BEFORE being passed to the PyMCAdapter.</p>"},{"location":"api/adapters/#mmm_eval.adapters.pymc-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.pymc.PyMCAdapter","title":"<code>PyMCAdapter(config: PyMCConfig)</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Initialize the PyMCAdapter.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PyMCConfig</code> <p>PyMCConfig object</p> required Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def __init__(self, config: PyMCConfig):\n    \"\"\"Initialize the PyMCAdapter.\n\n    Args:\n        config: PyMCConfig object\n\n    \"\"\"\n    self.model_kwargs = config.pymc_model_config_dict\n    self.fit_kwargs = config.fit_config_dict\n    self.predict_kwargs = config.predict_config_dict\n    self.date_column = config.date_column\n    self.channel_spend_columns = config.channel_columns\n    self.control_columns = config.control_columns\n    self.model = None\n    self.trace = None\n    self._channel_roi_df = None\n    self.is_fitted = False\n\n    # Store original values to reset on subsequent fit calls\n    self._original_channel_spend_columns = config.channel_columns.copy()\n    self._original_model_kwargs = config.pymc_model_config_dict.copy()\n</code></pre> Attributes\u00b6 <code>media_channels: list[str]</code> <code>property</code> \u00b6 <p>Return the channel names used by this adapter.</p> <p>For PyMC, this returns the channel_spend_columns which are used as the channel names in ROI results.</p> <p>Returns     List of channel names</p> <code>primary_media_regressor_columns: list[str]</code> <code>property</code> \u00b6 <p>Return the primary media regressor columns that should be perturbed in tests.</p> <p>For PyMC, this is always the <code>channel_spend_columns</code> since the PyMC adapter uses spend as the primary regressor in the model.</p> <p>Returns     List of channel spend column names</p> <code>primary_media_regressor_type: PrimaryMediaRegressor</code> <code>property</code> \u00b6 <p>Return the type of primary media regressors used by this adapter.</p> <p>For PyMC, this is always SPEND since the PyMC adapter uses spend as the primary regressor.</p> <p>Returns     PrimaryMediaRegressor.SPEND</p> Functions\u00b6 <code>fit(data: pd.DataFrame) -&gt; None</code> \u00b6 <p>Fit the model and compute ROIs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the training data adhering to the PyMCInputDataSchema.</p> required Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def fit(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Fit the model and compute ROIs.\n\n    Args:\n        data: DataFrame containing the training data adhering to the PyMCInputDataSchema.\n\n    \"\"\"\n    # Reset to original values at the start of each fit call\n    self.channel_spend_columns = self._original_channel_spend_columns.copy()\n    self.model_kwargs = self._original_model_kwargs.copy()\n\n    # Identify channel spend columns that sum to zero and remove them from modelling.\n    # We cannot reliably make any prediction based on these channels when making\n    # predictions on new data.\n    channel_spend_data = data[self.channel_spend_columns]\n    zero_spend_channels = channel_spend_data.columns[channel_spend_data.sum() == 0].tolist()\n\n    if zero_spend_channels:\n        logger.info(f\"Dropping channels with zero spend: {zero_spend_channels}\")\n        # Remove zero-spend channels from the list passed to the MMM constructor\n        self.channel_spend_columns = [col for col in self.channel_spend_columns if col not in zero_spend_channels]\n        # also update the model config field to reflect the new channel spend columns\n        self.model_kwargs[\"channel_columns\"] = self.channel_spend_columns\n\n        # Check for vector priors that might cause shape mismatches\n        _check_vector_priors_when_dropping_channels(self.model_kwargs[\"model_config\"], zero_spend_channels)\n\n        data = data.drop(columns=zero_spend_channels)\n\n    X = data.drop(columns=[InputDataframeConstants.RESPONSE_COL, InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL])\n    y = data[InputDataframeConstants.RESPONSE_COL]\n\n    self.model = MMM(**self.model_kwargs)\n    self.trace = self.model.fit(X=X, y=y, **self.fit_kwargs)\n\n    self._channel_roi_df = self._compute_channel_contributions(data)\n    self.is_fitted = True\n</code></pre> <code>fit_and_predict(train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray</code> \u00b6 <p>Fit on training data and make predictions on test data.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>DataFrame</code> <p>training dataset</p> required <code>test</code> <code>DataFrame</code> <p>test dataset</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>model predictions.</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def fit_and_predict(self, train: pd.DataFrame, test: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Fit on training data and make predictions on test data.\n\n    Arguments:\n        train: training dataset\n        test: test dataset\n\n    Returns:\n        model predictions.\n\n    \"\"\"\n    self.fit(train)\n    return self.predict(test)\n</code></pre> <code>get_channel_names() -&gt; list[str]</code> \u00b6 <p>Get the channel names that would be used as the index in get_channel_roi results.</p> <p>For PyMC, this returns the <code>channel_spend_columns</code> which are used as the index in the ROI results.</p> <p>Returns     List of channel names</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def get_channel_names(self) -&gt; list[str]:\n    \"\"\"Get the channel names that would be used as the index in get_channel_roi results.\n\n    For PyMC, this returns the `channel_spend_columns` which are used as the index\n    in the ROI results.\n\n    Returns\n        List of channel names\n\n    \"\"\"\n    return self.channel_spend_columns\n</code></pre> <code>get_channel_roi(start_date: pd.Timestamp | None = None, end_date: pd.Timestamp | None = None) -&gt; pd.Series</code> \u00b6 <p>Return the ROIs for each channel, optionally within a given date range.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp | None</code> <p>Optional start date for ROI calculation</p> <code>None</code> <code>end_date</code> <code>Timestamp | None</code> <p>Optional end date for ROI calculation</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>Series containing ROI estimates for each channel</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def get_channel_roi(\n    self,\n    start_date: pd.Timestamp | None = None,\n    end_date: pd.Timestamp | None = None,\n) -&gt; pd.Series:\n    \"\"\"Return the ROIs for each channel, optionally within a given date range.\n\n    Args:\n        start_date: Optional start date for ROI calculation\n        end_date: Optional end date for ROI calculation\n\n    Returns:\n        Series containing ROI estimates for each channel\n\n    \"\"\"\n    if not self.is_fitted or self._channel_roi_df is None:\n        raise RuntimeError(\"Model must be fit before computing ROI.\")\n\n    _validate_start_end_dates(start_date, end_date, pd.DatetimeIndex(self._channel_roi_df.index))\n\n    # Filter the contribution DataFrame by date range\n    date_range_df = self._channel_roi_df.loc[start_date:end_date]\n\n    if date_range_df.empty:\n        raise ValueError(f\"No data found for date range {start_date} to {end_date}\")\n\n    return pd.Series(self._calculate_rois(date_range_df))\n</code></pre> <code>predict(data: pd.DataFrame | None = None) -&gt; np.ndarray</code> \u00b6 <p>Predict the response variable for new data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | None</code> <p>Input data for prediction. This parameter is required for PyMC predictions and cannot be None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If model is not fitted</p> <code>ValueError</code> <p>If data is None (PyMC requires data for prediction)</p> Source code in <code>mmm_eval/adapters/pymc.py</code> <pre><code>def predict(self, data: pd.DataFrame | None = None) -&gt; np.ndarray:\n    \"\"\"Predict the response variable for new data.\n\n    Args:\n        data: Input data for prediction. This parameter is required for PyMC\n            predictions and cannot be None.\n\n    Returns:\n        Predicted values\n\n    Raises:\n        RuntimeError: If model is not fitted\n        ValueError: If data is None (PyMC requires data for prediction)\n\n    \"\"\"\n    if not self.is_fitted or self.model is None:\n        raise RuntimeError(\"Model must be fit before prediction.\")\n\n    if data is None:\n        raise ValueError(\"PyMC adapter requires data for prediction\")\n\n    if InputDataframeConstants.RESPONSE_COL in data.columns:\n        data = data.drop(columns=[InputDataframeConstants.RESPONSE_COL])\n    predictions = predictions = self.model.predict(\n        data, extend_idata=False, include_last_observations=True, **self.predict_kwargs\n    )\n    return predictions\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.schemas","title":"<code>schemas</code>","text":""},{"location":"api/adapters/#mmm_eval.adapters.schemas-classes","title":"Classes","text":""},{"location":"api/adapters/#mmm_eval.adapters.schemas.MeridianInputDataBuilderSchema","title":"<code>MeridianInputDataBuilderSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for Meridian input data builder configuration.</p> <p>These arguments are passed to the DataFrameInputDataBuilder class for constructing a data object to be fed into the Meridian model.</p> <p>See here for how to determine whether to consider a particular feature a non-media treatment or a control: https://developers.google.com/meridian/docs/advanced-modeling/organic-and-non-media-variables?hl=en</p> Functions\u00b6 <code>validate_media_channels(v)</code> \u00b6 <p>Validate media columns are not empty.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Media columns value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If media columns is empty</p> Source code in <code>mmm_eval/adapters/schemas.py</code> <pre><code>@field_validator(\"media_channels\")\ndef validate_media_channels(cls, v):\n    \"\"\"Validate media columns are not empty.\n\n    Args:\n        v: Media columns value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If media columns is empty\n\n    \"\"\"\n    if v is not None and not v:\n        raise ValueError(\"media_channels must not be empty\")\n    return v\n</code></pre> <code>validate_organic_media_fields(v, info)</code> \u00b6 <p>Validate that exactly zero or two of organic_media_columns and organic_media_channels are provided.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>The value being validated</p> required <code>info</code> <p>Validation info containing the field name</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If exactly zero or two of the fields are not provided</p> Source code in <code>mmm_eval/adapters/schemas.py</code> <pre><code>@field_validator(\"organic_media_columns\", \"organic_media_channels\", mode=\"after\")\ndef validate_organic_media_fields(cls, v, info):\n    \"\"\"Validate that exactly zero or two of organic_media_columns and organic_media_channels are provided.\n\n    Args:\n        v: The value being validated\n        info: Validation info containing the field name\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If exactly zero or two of the fields are not provided\n\n    \"\"\"\n    # Get the current values of both fields\n    organic_columns = getattr(info.data, \"organic_media_columns\", None)\n    organic_channels = getattr(info.data, \"organic_media_channels\", None)\n\n    # Count how many are provided (not None and not empty)\n    provided_count = sum(1 for field in [organic_columns, organic_channels] if field is not None and len(field) &gt; 0)\n\n    if provided_count not in [0, 2]:\n        raise ValueError(\"Exactly zero or two of organic_media_columns and organic_media_channels must be provided\")\n\n    return v\n</code></pre> <code>validate_reach_frequency_columns(v, info)</code> \u00b6 <p>Validate that exactly zero or two of channel_reach_columns and channel_frequency_columns are provided.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>The value being validated</p> required <code>info</code> <p>Validation info containing the field name</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If exactly zero or two of the fields are not provided</p> Source code in <code>mmm_eval/adapters/schemas.py</code> <pre><code>@field_validator(\"channel_reach_columns\", \"channel_frequency_columns\", mode=\"after\")\ndef validate_reach_frequency_columns(cls, v, info):\n    \"\"\"Validate that exactly zero or two of channel_reach_columns and channel_frequency_columns are provided.\n\n    Args:\n        v: The value being validated\n        info: Validation info containing the field name\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If exactly zero or two of the fields are not provided\n\n    \"\"\"\n    # Get the current values of both fields\n    reach_columns = getattr(info.data, \"channel_reach_columns\", None)\n    frequency_columns = getattr(info.data, \"channel_frequency_columns\", None)\n\n    # Count how many are provided (not None and not empty)\n    provided_count = sum(1 for field in [reach_columns, frequency_columns] if field is not None and len(field) &gt; 0)\n\n    if provided_count not in [0, 2]:\n        raise ValueError(\n            \"Exactly zero or two of channel_reach_columns and channel_frequency_columns must be provided\"\n        )\n\n    return v\n</code></pre> <code>validate_reach_impressions_mutual_exclusion()</code> \u00b6 <p>Validate that channel_reach_columns and channel_impressions_columns are not both provided.</p> <p>Returns     Self</p> <p>Raises     ValueError: If both fields are provided</p> Source code in <code>mmm_eval/adapters/schemas.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_reach_impressions_mutual_exclusion(self):\n    \"\"\"Validate that channel_reach_columns and channel_impressions_columns are not both provided.\n\n    Returns\n        Self\n\n    Raises\n        ValueError: If both fields are provided\n\n    \"\"\"\n    reach_columns = self.channel_reach_columns\n    impressions_columns = self.channel_impressions_columns\n\n    if (\n        reach_columns is not None\n        and len(reach_columns) &gt; 0\n        and impressions_columns is not None\n        and len(impressions_columns) &gt; 0\n    ):\n        raise ValueError(\"channel_reach_columns and channel_impressions_columns cannot both be provided\")\n\n    return self\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.schemas.MeridianModelSpecSchema","title":"<code>MeridianModelSpecSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for Meridian ModelSpec configuration.</p>"},{"location":"api/adapters/#mmm_eval.adapters.schemas.MeridianSamplePosteriorSchema","title":"<code>MeridianSamplePosteriorSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for Meridian sample_posterior configuration.</p> <p>These arguments are passed to the Meridian model's sample_posterior() method.</p> Attributes\u00b6 <code>fit_config_dict_without_non_provided_fields: dict[str, Any]</code> <code>property</code> \u00b6 <p>Return only non-None values.</p> <p>Returns     Dictionary of non-None values</p>"},{"location":"api/adapters/#mmm_eval.adapters.schemas.MeridianStringConfigSchema","title":"<code>MeridianStringConfigSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for Meridian evaluation config dictionary.</p>"},{"location":"api/adapters/#mmm_eval.adapters.schemas.PyMCFitSchema","title":"<code>PyMCFitSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for PyMC Fit Configuration.</p> <p>Defaults are all set to None so that the user can provide only the values they want to change. If a user does not provide a value, we will let the latest PYMC defaults be used in model instantiation.</p> Attributes\u00b6 <code>fit_config_dict_without_non_provided_fields: dict[str, Any]</code> <code>property</code> \u00b6 <p>Return only non-None values.</p> <p>These are the values that are provided by the user.    We don't want to include the default values as they should be set by the latest PYMC</p> <p>Returns     Dictionary of non-None values</p>"},{"location":"api/adapters/#mmm_eval.adapters.schemas.PyMCModelSchema","title":"<code>PyMCModelSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for PyMC Config Dictionary.</p> Functions\u00b6 <code>validate_adstock(v)</code> \u00b6 <p>Validate adstock component.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Adstock value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If adstock is not a valid type</p> Source code in <code>mmm_eval/adapters/schemas.py</code> <pre><code>@field_validator(\"adstock\")\ndef validate_adstock(cls, v):\n    \"\"\"Validate adstock component.\n\n    Args:\n        v: Adstock value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If adstock is not a valid type\n\n    \"\"\"\n    if v is not None:\n        assert isinstance(v, AdstockTransformation)\n    return v\n</code></pre> <code>validate_channel_columns(v)</code> \u00b6 <p>Validate channel columns are not empty.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Channel columns value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If channel columns is empty</p> Source code in <code>mmm_eval/adapters/schemas.py</code> <pre><code>@field_validator(\"channel_columns\")\ndef validate_channel_columns(cls, v):\n    \"\"\"Validate channel columns are not empty.\n\n    Args:\n        v: Channel columns value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If channel columns is empty\n\n    \"\"\"\n    if v is not None and not v:\n        raise ValueError(\"channel_columns must not be empty\")\n    return v\n</code></pre> <code>validate_saturation(v)</code> \u00b6 <p>Validate saturation component.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Saturation value</p> required <p>Returns:</p> Type Description <p>Validated value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If saturation is not a valid type</p> Source code in <code>mmm_eval/adapters/schemas.py</code> <pre><code>@field_validator(\"saturation\")\ndef validate_saturation(cls, v):\n    \"\"\"Validate saturation component.\n\n    Args:\n        v: Saturation value\n\n    Returns:\n        Validated value\n\n    Raises:\n        ValueError: If saturation is not a valid type\n\n    \"\"\"\n    if v is not None:\n        assert isinstance(v, SaturationTransformation)\n    return v\n</code></pre>"},{"location":"api/adapters/#mmm_eval.adapters.schemas.PyMCStringConfigSchema","title":"<code>PyMCStringConfigSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for PyMC Evaluation Config Dictionary.</p>"},{"location":"api/cli/","title":"CLI reference","text":""},{"location":"api/cli/#mmm_eval.cli","title":"<code>mmm_eval.cli</code>","text":"<p>CLI module for MMM evaluation.</p>"},{"location":"api/cli/#mmm_eval.cli-modules","title":"Modules","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate","title":"<code>evaluate</code>","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate-classes","title":"Classes","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate-functions","title":"Functions","text":""},{"location":"api/cli/#mmm_eval.cli.evaluate.main","title":"<code>main(config_path: str, input_data_path: str, test_names: tuple[str, ...], framework: str, output_path: str, verbose: bool)</code>","text":"<p>Evaluate MMM frameworks using the unified API.</p> Source code in <code>mmm_eval/cli/evaluate.py</code> <pre><code>@click.command()\n@click.option(\n    \"--framework\",\n    type=click.Choice(list(ADAPTER_REGISTRY.keys())),\n    required=True,\n    help=\"Open source MMM framework to evaluate\",\n)\n@click.option(\n    \"--input-data-path\",\n    type=str,\n    required=True,\n    help=\"Path to input data file. Supported formats: CSV, Parquet\",\n)\n@click.option(\n    \"--output-path\",\n    type=str,\n    required=True,\n    help=\"Directory to save evaluation results as a CSV file with name 'mmm_eval_&lt;framework&gt;_&lt;timestamp&gt;.csv'\",\n)\n@click.option(\n    \"--config-path\",\n    type=str,\n    required=True,\n    help=\"Path to framework-specific JSON config file\",\n)\n@click.option(\n    \"--test-names\",\n    type=click.Choice(ValidationTestNames.all_tests_as_str()),\n    multiple=True,\n    default=tuple(ValidationTestNames.all_tests_as_str()),\n    help=(\n        \"Test names to run. Can specify multiple tests as space-separated values \"\n        \"(e.g. --test-names accuracy cross_validation) or by repeating the flag \"\n        \"(e.g. --test-names accuracy --test-names cross_validation). \"\n        \"Defaults to all tests if not specified.\"\n    ),\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Enable verbose logging\",\n)\ndef main(\n    config_path: str,\n    input_data_path: str,\n    test_names: tuple[str, ...],\n    framework: str,\n    output_path: str,\n    verbose: bool,\n):\n    \"\"\"Evaluate MMM frameworks using the unified API.\"\"\"\n    # logging\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logging.basicConfig(level=log_level)\n\n    logger.info(\"Loading config...\")\n    config = get_config(framework, config_path)\n\n    logger.info(\"Loading input data...\")\n    data = DataLoader(input_data_path).load()\n\n    # Run evaluation\n    logger.info(f\"Running evaluation suite for {framework} framework...\")\n    results = run_evaluation(framework, data, config, test_names)\n\n    # Save results\n    if results.empty:\n        logger.warning(\"Results df empty, nothing to save.\")\n    else:\n        save_results(results, framework, output_path)\n</code></pre>"},{"location":"api/cli/#mmm_eval.cli.evaluate-modules","title":"Modules","text":""},{"location":"api/core/","title":"Core API Reference","text":""},{"location":"api/core/#mmm_eval.core","title":"<code>mmm_eval.core</code>","text":"<p>Core validation functionality for MMM frameworks.</p>"},{"location":"api/core/#mmm_eval.core-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.BaseValidationTest","title":"<code>BaseValidationTest(date_column: str)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for validation tests.</p> <p>All validation tests must inherit from this class and implement the required methods to provide a unified testing interface.</p> <p>Initialize the validation test.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self, date_column: str):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.date_column = date_column\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.BaseValidationTest-attributes","title":"Attributes","text":""},{"location":"api/core/#mmm_eval.core.BaseValidationTest.test_name","title":"<code>test_name: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the name of the test.</p> <p>Returns     Test name (e.g., 'accuracy', 'stability')</p>"},{"location":"api/core/#mmm_eval.core.BaseValidationTest-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.BaseValidationTest.run","title":"<code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code>  <code>abstractmethod</code>","text":"<p>Run the validation test.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>@abstractmethod\ndef run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#mmm_eval.core.BaseValidationTest.run_with_error_handling","title":"<code>run_with_error_handling(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code>","text":"<p>Run the validation test with error handling.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> <p>Raises:</p> Type Description <code>MetricCalculationError</code> <p>If metric calculation fails</p> <code>TestExecutionError</code> <p>If test execution fails</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def run_with_error_handling(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test with error handling.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    Raises:\n        MetricCalculationError: If metric calculation fails\n        TestExecutionError: If test execution fails\n\n    \"\"\"\n    try:\n        return self.run(adapter, data)\n    except ZeroDivisionError as e:\n        # This is clearly a mathematical calculation issue\n        raise MetricCalculationError(f\"Metric calculation error in {self.test_name} test: {str(e)}\") from e\n    except Exception as e:\n        # All other errors - let individual tests handle specific categorization if needed\n        raise TestExecutionError(f\"Test execution error in {self.test_name} test: {str(e)}\") from e\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults","title":"<code>ValidationResults(test_results: dict[ValidationTestNames, ValidationTestResult])</code>","text":"<p>Container for complete validation results.</p> <p>This class holds the results of all validation tests run, including individual test results and overall summary.</p> <p>Initialize validation results.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <code>dict[ValidationTestNames, ValidationTestResult]</code> <p>Dictionary mapping test names to their results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(self, test_results: dict[ValidationTestNames, ValidationTestResult]):\n    \"\"\"Initialize validation results.\n\n    Args:\n        test_results: Dictionary mapping test names to their results\n\n    \"\"\"\n    self.test_results = test_results\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.ValidationResults.get_test_result","title":"<code>get_test_result(test_name: ValidationTestNames) -&gt; ValidationTestResult</code>","text":"<p>Get results for a specific test.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def get_test_result(self, test_name: ValidationTestNames) -&gt; ValidationTestResult:\n    \"\"\"Get results for a specific test.\"\"\"\n    return self.test_results[test_name]\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationResults.to_df","title":"<code>to_df() -&gt; pd.DataFrame</code>","text":"<p>Convert validation results to a flat DataFrame format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert validation results to a flat DataFrame format.\"\"\"\n    return pd.concat(\n        [self.get_test_result(test_name).to_df() for test_name in self.test_results.keys()],\n        ignore_index=True,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestOrchestrator","title":"<code>ValidationTestOrchestrator()</code>","text":"<p>Main orchestrator for running validation tests.</p> <p>This class manages the test registry and executes tests in sequence, aggregating their results.</p> <p>Initialize the validator with standard tests pre-registered.</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validator with standard tests pre-registered.\"\"\"\n    self.tests: dict[ValidationTestNames, type[BaseValidationTest]] = {\n        ValidationTestNames.ACCURACY: AccuracyTest,\n        ValidationTestNames.CROSS_VALIDATION: CrossValidationTest,\n        ValidationTestNames.REFRESH_STABILITY: RefreshStabilityTest,\n        ValidationTestNames.PERTURBATION: PerturbationTest,\n    }\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestOrchestrator-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.ValidationTestOrchestrator.validate","title":"<code>validate(adapter: BaseAdapter, data: pd.DataFrame, test_names: list[ValidationTestNames]) -&gt; ValidationResults</code>","text":"<p>Run validation tests on the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <code>test_names</code> <code>list[ValidationTestNames]</code> <p>List of test names to run</p> required <code>adapter</code> <code>BaseAdapter</code> <p>Adapter to use for the test</p> required <p>Returns:</p> Type Description <code>ValidationResults</code> <p>ValidationResults containing all test results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any requested test is not registered</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def validate(\n    self,\n    adapter: BaseAdapter,\n    data: pd.DataFrame,\n    test_names: list[ValidationTestNames],\n) -&gt; ValidationResults:\n    \"\"\"Run validation tests on the model.\n\n    Args:\n        model: Model to validate\n        data: Input data for validation\n        test_names: List of test names to run\n        adapter: Adapter to use for the test\n\n    Returns:\n        ValidationResults containing all test results\n\n    Raises:\n        ValueError: If any requested test is not registered\n\n    \"\"\"\n    # Run tests and collect results\n    results: dict[ValidationTestNames, ValidationTestResult] = {}\n    for test_name in test_names:\n        logger.info(f\"Running test: {test_name}\")\n        test_instance = self.tests[test_name](adapter.date_column)\n        test_result = test_instance.run_with_error_handling(adapter, data)\n        results[test_name] = test_result\n\n    return ValidationResults(results)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestResult","title":"<code>ValidationTestResult(test_name: ValidationTestNames, metric_names: list[str], test_scores: AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults)</code>","text":"<p>Container for individual test results.</p> <p>This class holds the results of a single validation test, including pass/fail status, metrics, and any error messages.</p> <p>Initialize test results.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>ValidationTestNames</code> <p>Name of the test</p> required <code>metric_names</code> <code>list[str]</code> <p>List of metric names</p> required <code>test_scores</code> <code>AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults</code> <p>Computed metric results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(\n    self,\n    test_name: ValidationTestNames,\n    metric_names: list[str],\n    test_scores: (\n        AccuracyMetricResults\n        | CrossValidationMetricResults\n        | RefreshStabilityMetricResults\n        | PerturbationMetricResults\n    ),\n):\n    \"\"\"Initialize test results.\n\n    Args:\n        test_name: Name of the test\n        metric_names: List of metric names\n        test_scores: Computed metric results\n\n    \"\"\"\n    self.test_name = test_name\n    self.metric_names = metric_names\n    self.test_scores = test_scores\n    self.timestamp = datetime.now()\n</code></pre>"},{"location":"api/core/#mmm_eval.core.ValidationTestResult-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.ValidationTestResult.to_df","title":"<code>to_df() -&gt; pd.DataFrame</code>","text":"<p>Convert test results to a flat DataFrame format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert test results to a flat DataFrame format.\"\"\"\n    test_scores_df = self.test_scores.to_df()\n    test_scores_df[ValidationTestAttributeNames.TEST_NAME.value] = self.test_name.value\n    test_scores_df[ValidationTestAttributeNames.TIMESTAMP.value] = self.timestamp.isoformat()\n    return test_scores_df\n</code></pre>"},{"location":"api/core/#mmm_eval.core-modules","title":"Modules","text":""},{"location":"api/core/#mmm_eval.core.base_validation_test","title":"<code>base_validation_test</code>","text":"<p>Abstract base classes for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.base_validation_test-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.base_validation_test.BaseValidationTest","title":"<code>BaseValidationTest(date_column: str)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for validation tests.</p> <p>All validation tests must inherit from this class and implement the required methods to provide a unified testing interface.</p> <p>Initialize the validation test.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self, date_column: str):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.date_column = date_column\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: str</code> <code>abstractmethod</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> <p>Returns     Test name (e.g., 'accuracy', 'stability')</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> <code>abstractmethod</code> \u00b6 <p>Run the validation test.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>@abstractmethod\ndef run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    \"\"\"\n    pass\n</code></pre> <code>run_with_error_handling(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the validation test with error handling.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>BaseAdapter</code> <p>The adapter to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult object containing test results</p> <p>Raises:</p> Type Description <code>MetricCalculationError</code> <p>If metric calculation fails</p> <code>TestExecutionError</code> <p>If test execution fails</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def run_with_error_handling(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; \"ValidationTestResult\":\n    \"\"\"Run the validation test with error handling.\n\n    Args:\n        adapter: The adapter to validate\n        data: Input data for validation\n\n    Returns:\n        TestResult object containing test results\n\n    Raises:\n        MetricCalculationError: If metric calculation fails\n        TestExecutionError: If test execution fails\n\n    \"\"\"\n    try:\n        return self.run(adapter, data)\n    except ZeroDivisionError as e:\n        # This is clearly a mathematical calculation issue\n        raise MetricCalculationError(f\"Metric calculation error in {self.test_name} test: {str(e)}\") from e\n    except Exception as e:\n        # All other errors - let individual tests handle specific categorization if needed\n        raise TestExecutionError(f\"Test execution error in {self.test_name} test: {str(e)}\") from e\n</code></pre>"},{"location":"api/core/#mmm_eval.core.base_validation_test-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.base_validation_test.split_timeseries_cv","title":"<code>split_timeseries_cv(data: pd.DataFrame, n_splits: PositiveInt, test_size: PositiveInt, date_column: str) -&gt; Generator[tuple[np.ndarray, np.ndarray], None, None]</code>","text":"<p>Produce train/test masks for rolling CV, split globally based on date.</p> <p>This simulates regular refreshes and utilises the last <code>test_size</code> data points for testing in the first fold, using all prior data for training. For a dataset with T dates, the subsequen test folds follow the pattern [T-4, T], [T-8, T-4], ...</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>dataframe of MMM data to be split</p> required <code>n_splits</code> <code>PositiveInt</code> <p>number of unique folds to generate</p> required <code>test_size</code> <code>PositiveInt</code> <p>the number of observations in each testing fold</p> required <code>date_column</code> <code>str</code> <p>the name of the date column in the dataframe to split by</p> required <p>Yields:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>integer masks corresponding training and test set indices.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def split_timeseries_cv(\n    data: pd.DataFrame, n_splits: PositiveInt, test_size: PositiveInt, date_column: str\n) -&gt; Generator[tuple[np.ndarray, np.ndarray], None, None]:\n    \"\"\"Produce train/test masks for rolling CV, split globally based on date.\n\n    This simulates regular refreshes and utilises the last `test_size` data points for\n    testing in the first fold, using all prior data for training. For a dataset with\n    T dates, the subsequen test folds follow the pattern [T-4, T], [T-8, T-4], ...\n\n    Arguments:\n        data: dataframe of MMM data to be split\n        n_splits: number of unique folds to generate\n        test_size: the number of observations in each testing fold\n        date_column: the name of the date column in the dataframe to split by\n\n    Yields:\n        integer masks corresponding training and test set indices.\n\n    \"\"\"\n    sorted_dates = sorted(data[date_column].unique())\n    n_dates = len(sorted_dates)\n\n    # assuming the minimum training set size allowable is equal to `test_size`, ensure there's\n    # enough data temporally to do the splits\n    n_required_dates = test_size * (n_splits + 1)\n    if n_dates &lt; n_required_dates:\n        raise ValueError(\n            \"Insufficient timeseries data provided for splitting. In order to \"\n            f\"perform {n_splits} splits with test_size={test_size}, at least \"\n            f\"{n_required_dates} unique dates are required, but only {n_dates} \"\n            f\"dates are available.\"\n        )\n\n    for i in range(n_splits):\n        test_end = n_dates - i * test_size\n        test_start = n_dates - (i + 1) * test_size\n        test_dates = sorted_dates[test_start:test_end]\n        train_dates = sorted_dates[:test_start]\n\n        train_mask = data[date_column].isin(train_dates)\n        test_mask = data[date_column].isin(test_dates)\n        yield train_mask, test_mask\n</code></pre>"},{"location":"api/core/#mmm_eval.core.base_validation_test.split_timeseries_data","title":"<code>split_timeseries_data(data: pd.DataFrame, test_proportion: PositiveFloat, date_column: str) -&gt; tuple[np.ndarray, np.ndarray]</code>","text":"<p>Split data globally based on date.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>timeseries data to split, possibly with another index like geography</p> required <code>test_proportion</code> <code>PositiveFloat</code> <p>proportion of test data, must be in (0, 1)</p> required <code>date_column</code> <code>str</code> <p>name of the date column</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>boolean masks for training and test data respectively</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def split_timeseries_data(\n    data: pd.DataFrame, test_proportion: PositiveFloat, date_column: str\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Split data globally based on date.\n\n    Arguments:\n        data: timeseries data to split, possibly with another index like geography\n        test_proportion: proportion of test data, must be in (0, 1)\n        date_column: name of the date column\n\n    Returns:\n        boolean masks for training and test data respectively\n\n    \"\"\"\n    if test_proportion &lt;= 0 or test_proportion &gt;= 1:\n        raise ValueError(\"`test_proportion` must be in the range (0, 1)\")\n\n    sorted_dates = sorted(data[date_column].unique())\n    # rounding eliminates possibility of floating point precision issues\n    split_idx = int(round(len(sorted_dates) * (1 - test_proportion)))\n    cutoff = sorted_dates[split_idx]\n\n    train_mask = data[date_column] &lt; cutoff\n    test_mask = data[date_column] &gt;= cutoff\n\n    return train_mask, test_mask\n</code></pre>"},{"location":"api/core/#mmm_eval.core.constants","title":"<code>constants</code>","text":""},{"location":"api/core/#mmm_eval.core.constants-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.constants.ValidationTestConstants","title":"<code>ValidationTestConstants</code>","text":"<p>Constants for the validation tests.</p> Classes\u00b6 <code>PerturbationConstants</code> \u00b6 <p>Constants for the perturbation test.</p>"},{"location":"api/core/#mmm_eval.core.evaluator","title":"<code>evaluator</code>","text":"<p>Main evaluator for MMM frameworks.</p>"},{"location":"api/core/#mmm_eval.core.evaluator-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.evaluator.Evaluator","title":"<code>Evaluator(data: pd.DataFrame, test_names: tuple[str, ...] | None = None)</code>","text":"<p>Main evaluator class for MMM frameworks.</p> <p>This class provides a unified interface for evaluating different MMM frameworks using standardized validation tests.</p> <p>Initialize the evaluator.</p> Source code in <code>mmm_eval/core/evaluator.py</code> <pre><code>def __init__(self, data: pd.DataFrame, test_names: tuple[str, ...] | None = None):\n    \"\"\"Initialize the evaluator.\"\"\"\n    self.validation_orchestrator = ValidationTestOrchestrator()\n    self.data = data\n    self.test_names = (\n        self._get_test_names(test_names) if test_names else self.validation_orchestrator._get_all_test_names()\n    )\n</code></pre> Functions\u00b6 <code>evaluate_framework(framework: str, config: BaseConfig) -&gt; ValidationResults</code> \u00b6 <p>Evaluate an MMM framework using the unified API.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>Name of the MMM framework to evaluate</p> required <code>config</code> <code>BaseConfig</code> <p>Framework-specific configuration</p> required <p>Returns:</p> Type Description <code>ValidationResults</code> <p>ValidationResult object containing evaluation metrics and predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any test name is invalid</p> Source code in <code>mmm_eval/core/evaluator.py</code> <pre><code>def evaluate_framework(self, framework: str, config: BaseConfig) -&gt; ValidationResults:\n    \"\"\"Evaluate an MMM framework using the unified API.\n\n    Args:\n        framework: Name of the MMM framework to evaluate\n        config: Framework-specific configuration\n\n    Returns:\n        ValidationResult object containing evaluation metrics and predictions\n\n    Raises:\n        ValueError: If any test name is invalid\n\n    \"\"\"\n    # Initialize the adapter\n    adapter = get_adapter(framework, config)\n\n    # Run validation tests\n    validation_results = self.validation_orchestrator.validate(\n        adapter=adapter,\n        data=self.data,\n        test_names=self.test_names,\n    )\n\n    return validation_results\n</code></pre>"},{"location":"api/core/#mmm_eval.core.evaluator-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.exceptions-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.exceptions.InvalidTestNameError","title":"<code>InvalidTestNameError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when an invalid test name is provided.</p>"},{"location":"api/core/#mmm_eval.core.exceptions.MetricCalculationError","title":"<code>MetricCalculationError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when metric calculation fails.</p>"},{"location":"api/core/#mmm_eval.core.exceptions.TestExecutionError","title":"<code>TestExecutionError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when test execution fails.</p>"},{"location":"api/core/#mmm_eval.core.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for validation framework errors.</p>"},{"location":"api/core/#mmm_eval.core.run_evaluation","title":"<code>run_evaluation</code>","text":""},{"location":"api/core/#mmm_eval.core.run_evaluation-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.run_evaluation-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.run_evaluation.run_evaluation","title":"<code>run_evaluation(framework: str, data: pd.DataFrame, config: BaseConfig, test_names: tuple[str, ...] | None = None) -&gt; pd.DataFrame</code>","text":"<p>Evaluate an MMM framework.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>The framework to evaluate.</p> required <code>data</code> <code>DataFrame</code> <p>The data to evaluate.</p> required <code>config</code> <code>BaseConfig</code> <p>The config to use for the evaluation.</p> required <code>test_names</code> <code>tuple[str, ...] | None</code> <p>The tests to run. If not provided, all tests will be run.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame containing the evaluation results.</p> Source code in <code>mmm_eval/core/run_evaluation.py</code> <pre><code>def run_evaluation(\n    framework: str,\n    data: pd.DataFrame,\n    config: BaseConfig,\n    test_names: tuple[str, ...] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Evaluate an MMM framework.\n\n    Args:\n        framework: The framework to evaluate.\n        data: The data to evaluate.\n        config: The config to use for the evaluation.\n        test_names: The tests to run. If not provided, all tests will be run.\n\n    Returns:\n        A pandas DataFrame containing the evaluation results.\n\n    \"\"\"\n    # validate + process the input data\n    data = DataPipeline(\n        data=data,\n        framework=framework,\n        date_column=config.date_column,\n        response_column=config.response_column,\n        revenue_column=config.revenue_column,\n        control_columns=config.control_columns,\n        channel_columns=config.channel_columns,\n    ).run()\n\n    # run the evaluation suite\n    results = Evaluator(\n        data=data,\n        test_names=test_names,\n    ).evaluate_framework(framework=framework, config=config)\n\n    return results.to_df()\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_test_orchestrator","title":"<code>validation_test_orchestrator</code>","text":"<p>Test orchestrator for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.validation_test_orchestrator-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_test_orchestrator.ValidationTestOrchestrator","title":"<code>ValidationTestOrchestrator()</code>","text":"<p>Main orchestrator for running validation tests.</p> <p>This class manages the test registry and executes tests in sequence, aggregating their results.</p> <p>Initialize the validator with standard tests pre-registered.</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the validator with standard tests pre-registered.\"\"\"\n    self.tests: dict[ValidationTestNames, type[BaseValidationTest]] = {\n        ValidationTestNames.ACCURACY: AccuracyTest,\n        ValidationTestNames.CROSS_VALIDATION: CrossValidationTest,\n        ValidationTestNames.REFRESH_STABILITY: RefreshStabilityTest,\n        ValidationTestNames.PERTURBATION: PerturbationTest,\n    }\n</code></pre> Functions\u00b6 <code>validate(adapter: BaseAdapter, data: pd.DataFrame, test_names: list[ValidationTestNames]) -&gt; ValidationResults</code> \u00b6 <p>Run validation tests on the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to validate</p> required <code>data</code> <code>DataFrame</code> <p>Input data for validation</p> required <code>test_names</code> <code>list[ValidationTestNames]</code> <p>List of test names to run</p> required <code>adapter</code> <code>BaseAdapter</code> <p>Adapter to use for the test</p> required <p>Returns:</p> Type Description <code>ValidationResults</code> <p>ValidationResults containing all test results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any requested test is not registered</p> Source code in <code>mmm_eval/core/validation_test_orchestrator.py</code> <pre><code>def validate(\n    self,\n    adapter: BaseAdapter,\n    data: pd.DataFrame,\n    test_names: list[ValidationTestNames],\n) -&gt; ValidationResults:\n    \"\"\"Run validation tests on the model.\n\n    Args:\n        model: Model to validate\n        data: Input data for validation\n        test_names: List of test names to run\n        adapter: Adapter to use for the test\n\n    Returns:\n        ValidationResults containing all test results\n\n    Raises:\n        ValueError: If any requested test is not registered\n\n    \"\"\"\n    # Run tests and collect results\n    results: dict[ValidationTestNames, ValidationTestResult] = {}\n    for test_name in test_names:\n        logger.info(f\"Running test: {test_name}\")\n        test_instance = self.tests[test_name](adapter.date_column)\n        test_result = test_instance.run_with_error_handling(adapter, data)\n        results[test_name] = test_result\n\n    return ValidationResults(results)\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_test_results","title":"<code>validation_test_results</code>","text":"<p>Result containers for MMM validation framework.</p>"},{"location":"api/core/#mmm_eval.core.validation_test_results-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_test_results.ValidationResults","title":"<code>ValidationResults(test_results: dict[ValidationTestNames, ValidationTestResult])</code>","text":"<p>Container for complete validation results.</p> <p>This class holds the results of all validation tests run, including individual test results and overall summary.</p> <p>Initialize validation results.</p> <p>Parameters:</p> Name Type Description Default <code>test_results</code> <code>dict[ValidationTestNames, ValidationTestResult]</code> <p>Dictionary mapping test names to their results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(self, test_results: dict[ValidationTestNames, ValidationTestResult]):\n    \"\"\"Initialize validation results.\n\n    Args:\n        test_results: Dictionary mapping test names to their results\n\n    \"\"\"\n    self.test_results = test_results\n</code></pre> Functions\u00b6 <code>get_test_result(test_name: ValidationTestNames) -&gt; ValidationTestResult</code> \u00b6 <p>Get results for a specific test.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def get_test_result(self, test_name: ValidationTestNames) -&gt; ValidationTestResult:\n    \"\"\"Get results for a specific test.\"\"\"\n    return self.test_results[test_name]\n</code></pre> <code>to_df() -&gt; pd.DataFrame</code> \u00b6 <p>Convert validation results to a flat DataFrame format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert validation results to a flat DataFrame format.\"\"\"\n    return pd.concat(\n        [self.get_test_result(test_name).to_df() for test_name in self.test_results.keys()],\n        ignore_index=True,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_test_results.ValidationTestResult","title":"<code>ValidationTestResult(test_name: ValidationTestNames, metric_names: list[str], test_scores: AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults)</code>","text":"<p>Container for individual test results.</p> <p>This class holds the results of a single validation test, including pass/fail status, metrics, and any error messages.</p> <p>Initialize test results.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>ValidationTestNames</code> <p>Name of the test</p> required <code>metric_names</code> <code>list[str]</code> <p>List of metric names</p> required <code>test_scores</code> <code>AccuracyMetricResults | CrossValidationMetricResults | RefreshStabilityMetricResults | PerturbationMetricResults</code> <p>Computed metric results</p> required Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def __init__(\n    self,\n    test_name: ValidationTestNames,\n    metric_names: list[str],\n    test_scores: (\n        AccuracyMetricResults\n        | CrossValidationMetricResults\n        | RefreshStabilityMetricResults\n        | PerturbationMetricResults\n    ),\n):\n    \"\"\"Initialize test results.\n\n    Args:\n        test_name: Name of the test\n        metric_names: List of metric names\n        test_scores: Computed metric results\n\n    \"\"\"\n    self.test_name = test_name\n    self.metric_names = metric_names\n    self.test_scores = test_scores\n    self.timestamp = datetime.now()\n</code></pre> Functions\u00b6 <code>to_df() -&gt; pd.DataFrame</code> \u00b6 <p>Convert test results to a flat DataFrame format.</p> Source code in <code>mmm_eval/core/validation_test_results.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert test results to a flat DataFrame format.\"\"\"\n    test_scores_df = self.test_scores.to_df()\n    test_scores_df[ValidationTestAttributeNames.TEST_NAME.value] = self.test_name.value\n    test_scores_df[ValidationTestAttributeNames.TIMESTAMP.value] = self.timestamp.isoformat()\n    return test_scores_df\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests","title":"<code>validation_tests</code>","text":""},{"location":"api/core/#mmm_eval.core.validation_tests-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_tests.AccuracyTest","title":"<code>AccuracyTest(date_column: str)</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for model accuracy using holdout validation.</p> <p>This test evaluates model performance by splitting data into train/test sets and calculating MAPE and R-squared metrics on the test set.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self, date_column: str):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.date_column = date_column\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the accuracy test.</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the accuracy test.\"\"\"\n    # Split data into train/test sets\n    train, test = self._split_data_holdout(data)\n    predictions = adapter.fit_and_predict(train, test)\n    actual = test.groupby(self.date_column)[InputDataframeConstants.RESPONSE_COL].sum()\n    assert len(actual) == len(predictions), \"Actual and predicted lengths must match\"\n\n    # Calculate metrics\n    test_scores = AccuracyMetricResults.populate_object_with_metrics(\n        actual=pd.Series(actual),  # Ensure it's a Series\n        predicted=pd.Series(predictions, index=actual.index),\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.ACCURACY,\n        metric_names=AccuracyMetricNames.to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests.CrossValidationTest","title":"<code>CrossValidationTest(date_column: str)</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for the cross-validation of the MMM framework.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self, date_column: str):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.date_column = date_column\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the cross-validation test using time-series splits.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to validate</p> required <code>adapter</code> <code>BaseAdapter</code> <p>Adapter to use for the test</p> required <code>data</code> <code>DataFrame</code> <p>Input data</p> required <p>Returns:</p> Type Description <code>ValidationTestResult</code> <p>TestResult containing cross-validation metrics</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the cross-validation test using time-series splits.\n\n    Args:\n        model: Model to validate\n        adapter: Adapter to use for the test\n        data: Input data\n\n    Returns:\n        TestResult containing cross-validation metrics\n\n    \"\"\"\n    # Initialize cross-validation splitter\n    cv_splits = self._split_data_time_series_cv(data)\n\n    # Store metrics for each fold\n    fold_metrics = []\n\n    # Run cross-validation\n    for i, (train_idx, test_idx) in enumerate(cv_splits):\n\n        logger.info(f\"Running cross-validation fold {i+1} of {len(cv_splits)}\")\n\n        # Get train/test data\n        train = data.loc[train_idx]\n        test = data.loc[test_idx]\n\n        # Get predictions\n        predictions = adapter.fit_and_predict(train, test)\n        actual = test.groupby(self.date_column)[InputDataframeConstants.RESPONSE_COL].sum()\n        assert len(actual) == len(predictions), \"Actual and predicted lengths must match\"\n\n        # Add in fold results\n        fold_metrics.append(\n            AccuracyMetricResults.populate_object_with_metrics(\n                actual=pd.Series(actual),  # Ensure it's a Series\n                predicted=pd.Series(predictions, index=actual.index),\n            )\n        )\n\n    # Calculate mean and std of metrics across folds and create metric results\n    test_scores = CrossValidationMetricResults(\n        mean_mape=calculate_mean_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.MAPE\n        ),\n        std_mape=calculate_std_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.MAPE\n        ),\n        mean_r_squared=calculate_mean_for_singular_values_across_cross_validation_folds(\n            fold_metrics, AccuracyMetricNames.R_SQUARED\n        ),\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.CROSS_VALIDATION,\n        metric_names=CrossValidationMetricNames.to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests.PerturbationTest","title":"<code>PerturbationTest(date_column: str)</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for the perturbation of the MMM framework.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self, date_column: str):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.date_column = date_column\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the perturbation test.</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the perturbation test.\"\"\"\n    # Train model on original data\n    adapter.fit(data)\n    original_rois = adapter.get_channel_roi()\n\n    # TODO: support perturbation of reach and frequency regressors\n    if adapter.primary_media_regressor_type == PrimaryMediaRegressor.REACH_AND_FREQUENCY:\n        logger.warning(\n            \"Perturbation test skipped: Reach and frequency regressor type not supported for perturbation.\"\n        )\n        # Return NaN results for each channel indicating the test was not run\n        channel_names = adapter.get_channel_names()\n        test_scores = PerturbationMetricResults(\n            percentage_change_for_each_channel=pd.Series(np.nan, index=channel_names),\n        )\n        return ValidationTestResult(\n            test_name=ValidationTestNames.PERTURBATION,\n            metric_names=PerturbationMetricNames.to_list(),\n            test_scores=test_scores,\n        )\n\n    # Add noise to primary regressor data and retrain\n    noisy_data = self._add_gaussian_noise_to_primary_regressors(\n        df=data,\n        regressor_cols=adapter.primary_media_regressor_columns,\n    )\n    adapter.fit(noisy_data)\n    noise_rois = adapter.get_channel_roi()\n\n    # calculate the pct change in roi\n    percentage_change = calculate_absolute_percentage_change(\n        baseline_series=original_rois,\n        comparison_series=noise_rois,\n    )\n\n    # Create metric results - roi % change for each channel\n    test_scores = PerturbationMetricResults(\n        percentage_change_for_each_channel=percentage_change,\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.PERTURBATION,\n        metric_names=PerturbationMetricNames.to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests.RefreshStabilityTest","title":"<code>RefreshStabilityTest(date_column: str)</code>","text":"<p>               Bases: <code>BaseValidationTest</code></p> <p>Validation test for the stability of the MMM framework.</p> Source code in <code>mmm_eval/core/base_validation_test.py</code> <pre><code>def __init__(self, date_column: str):\n    \"\"\"Initialize the validation test.\"\"\"\n    self.date_column = date_column\n    self.rng = np.random.default_rng(ValidationTestConstants.RANDOM_STATE)\n</code></pre> Attributes\u00b6 <code>test_name: ValidationTestNames</code> <code>property</code> \u00b6 <p>Return the name of the test.</p> Functions\u00b6 <code>run(adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult</code> \u00b6 <p>Run the stability test.</p> Source code in <code>mmm_eval/core/validation_tests.py</code> <pre><code>def run(self, adapter: BaseAdapter, data: pd.DataFrame) -&gt; ValidationTestResult:\n    \"\"\"Run the stability test.\"\"\"\n    # Initialize cross-validation splitter\n    cv_splits = self._split_data_time_series_cv(data)\n\n    # Store metrics for each fold\n    fold_metrics = []\n\n    # Run cross-validation\n    for i, (train_idx, refresh_idx) in enumerate(cv_splits):\n\n        logger.info(f\"Running refresh stability test fold {i+1} of {len(cv_splits)}\")\n\n        # Get train/test data\n        # todo(): Can we somehow store these training changes in the adapter for use in time series holdout test\n        current_data = data.loc[train_idx]\n        # Combine current data with refresh data for retraining\n        refresh_data = pd.concat([current_data, data.loc[refresh_idx]], ignore_index=True)\n        # Get common dates for roi stability comparison\n        common_start_date, common_end_date = self._get_common_dates(\n            baseline_data=current_data,\n            comparison_data=refresh_data,\n            date_column=adapter.date_column,\n        )\n\n        # Train model and get coefficients\n        adapter.fit(current_data)\n        current_model_rois = adapter.get_channel_roi(\n            start_date=common_start_date,\n            end_date=common_end_date,\n        )\n        adapter.fit(refresh_data)\n        refreshed_model_rois = adapter.get_channel_roi(\n            start_date=common_start_date,\n            end_date=common_end_date,\n        )\n\n        # calculate the pct change in volume\n        percentage_change = calculate_absolute_percentage_change(\n            baseline_series=current_model_rois,\n            comparison_series=refreshed_model_rois,\n        )\n\n        fold_metrics.append(percentage_change)\n\n    # Calculate mean and std of percentage change for each channel across cross validation folds\n    test_scores = RefreshStabilityMetricResults(\n        mean_percentage_change_for_each_channel=calculate_means_for_series_across_cross_validation_folds(\n            fold_metrics\n        ),\n        std_percentage_change_for_each_channel=calculate_stds_for_series_across_cross_validation_folds(\n            fold_metrics\n        ),\n    )\n\n    logger.info(f\"Saving the test results for {self.test_name} test\")\n\n    return ValidationTestResult(\n        test_name=ValidationTestNames.REFRESH_STABILITY,\n        metric_names=RefreshStabilityMetricNames.to_list(),\n        test_scores=test_scores,\n    )\n</code></pre>"},{"location":"api/core/#mmm_eval.core.validation_tests-functions","title":"Functions","text":""},{"location":"api/core/#mmm_eval.core.validation_tests_models","title":"<code>validation_tests_models</code>","text":""},{"location":"api/core/#mmm_eval.core.validation_tests_models-classes","title":"Classes","text":""},{"location":"api/core/#mmm_eval.core.validation_tests_models.ValidationResultAttributeNames","title":"<code>ValidationResultAttributeNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the names of the validation result attributes.</p>"},{"location":"api/core/#mmm_eval.core.validation_tests_models.ValidationTestAttributeNames","title":"<code>ValidationTestAttributeNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the names of the validation test attributes.</p>"},{"location":"api/core/#mmm_eval.core.validation_tests_models.ValidationTestNames","title":"<code>ValidationTestNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the names of the validation tests.</p> Functions\u00b6 <code>all_tests() -&gt; list[ValidationTestNames]</code> <code>classmethod</code> \u00b6 <p>Return all validation test names as a list.</p> Source code in <code>mmm_eval/core/validation_tests_models.py</code> <pre><code>@classmethod\ndef all_tests(cls) -&gt; list[\"ValidationTestNames\"]:\n    \"\"\"Return all validation test names as a list.\"\"\"\n    return list(cls)\n</code></pre> <code>all_tests_as_str() -&gt; list[str]</code> <code>classmethod</code> \u00b6 <p>Return all validation test names as a list of strings.</p> Source code in <code>mmm_eval/core/validation_tests_models.py</code> <pre><code>@classmethod\ndef all_tests_as_str(cls) -&gt; list[str]:\n    \"\"\"Return all validation test names as a list of strings.\"\"\"\n    return [test.value for test in cls]\n</code></pre>"},{"location":"api/data/","title":"Data Reference","text":""},{"location":"api/data/#mmm_eval.data","title":"<code>mmm_eval.data</code>","text":"<p>Data loading and processing utilities.</p>"},{"location":"api/data/#mmm_eval.data-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.DataLoader","title":"<code>DataLoader(data_path: str | Path)</code>","text":"<p>Simple data loader for MMM evaluation.</p> <p>Takes a data path and loads the data.</p> <p>Initialize data loader with data path.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str | Path</code> <p>Path to the data file (CSV, Parquet, etc.)</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the data file does not exist.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def __init__(self, data_path: str | Path):\n    \"\"\"Initialize data loader with data path.\n\n    Args:\n        data_path: Path to the data file (CSV, Parquet, etc.)\n\n    Raises:\n        FileNotFoundError: If the data file does not exist.\n\n    \"\"\"\n    self.data_path = Path(data_path)\n\n    if not self.data_path.exists():\n        raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataLoader-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataLoader.load","title":"<code>load() -&gt; pd.DataFrame</code>","text":"<p>Load data from the specified path.</p> <p>Returns     Loaded DataFrame</p> <p>Raises     ValueError: If the file format is not supported.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def load(self) -&gt; pd.DataFrame:\n    \"\"\"Load data from the specified path.\n\n    Returns\n        Loaded DataFrame\n\n    Raises\n        ValueError: If the file format is not supported.\n\n    \"\"\"\n    ext = self.data_path.suffix.lower().lstrip(\".\")\n    if ext not in DataLoaderConstants.ValidDataExtensions.all():\n        raise ValueError(f\"Unsupported file format: {self.data_path.suffix}\")\n\n    if ext == DataLoaderConstants.ValidDataExtensions.CSV:\n        return self._load_csv()\n    elif ext == DataLoaderConstants.ValidDataExtensions.PARQUET:\n        return self._load_parquet()\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataPipeline","title":"<code>DataPipeline(data: pd.DataFrame, framework: str, control_columns: list[str] | None, channel_columns: list[str], date_column: str, response_column: str, revenue_column: str, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Data pipeline that orchestrates loading, processing, and validation.</p> <p>Provides a simple interface to go from raw data file to validated DataFrame.</p> <p>Initialize data pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the data</p> required <code>framework</code> <code>str</code> <p>name of supported framework</p> required <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column</p> required <code>response_column</code> <code>str</code> <p>Name of the response column</p> required <code>revenue_column</code> <code>str</code> <p>Name of the revenue column</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    framework: str,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str,\n    response_column: str,\n    revenue_column: str,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize data pipeline.\n\n    Args:\n        data: DataFrame containing the data\n        framework: name of supported framework\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column\n        response_column: Name of the response column\n        revenue_column: Name of the revenue column\n        min_number_observations: Minimum required number of observations\n\n    \"\"\"\n    # Initialize components\n    self.data = data\n    self.processor = DataProcessor(\n        date_column=date_column,\n        response_column=response_column,\n        revenue_column=revenue_column,\n        control_columns=control_columns,\n        channel_columns=channel_columns,\n    )\n    self.validator = DataValidator(\n        framework=framework,\n        date_column=date_column,\n        response_column=InputDataframeConstants.RESPONSE_COL,\n        revenue_column=InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,\n        control_columns=control_columns,\n        min_number_observations=min_number_observations,\n    )\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataPipeline-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataPipeline.run","title":"<code>run() -&gt; pd.DataFrame</code>","text":"<p>Run the complete data pipeline: process \u2192 validate.</p> <p>Returns     Validated and processed DataFrame</p> <p>Raises     Various exceptions processing or validation steps</p> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"Run the complete data pipeline: process \u2192 validate.\n\n    Returns\n        Validated and processed DataFrame\n\n    Raises\n        Various exceptions processing or validation steps\n\n    \"\"\"\n    processed_df = self.processor.process(self.data)\n\n    self.validator.run_validations(processed_df)\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataProcessor","title":"<code>DataProcessor(control_columns: list[str] | None, channel_columns: list[str], date_column: str = InputDataframeConstants.DATE_COL, response_column: str = InputDataframeConstants.RESPONSE_COL, revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL)</code>","text":"<p>Simple data processor for MMM evaluation.</p> <p>Handles data transformations like datetime casting, column renaming, etc.</p> <p>Initialize data processor.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column to parse and rename</p> <code>DATE_COL</code> <code>response_column</code> <code>str</code> <p>Name of the response column to parse and rename</p> <code>RESPONSE_COL</code> <code>revenue_column</code> <code>str</code> <p>Name of the revenue column to parse and rename</p> <code>MEDIA_CHANNEL_REVENUE_COL</code> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str = InputDataframeConstants.DATE_COL,\n    response_column: str = InputDataframeConstants.RESPONSE_COL,\n    revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,\n):\n    \"\"\"Initialize data processor.\n\n    Args:\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column to parse and rename\n        response_column: Name of the response column to parse and rename\n        revenue_column: Name of the revenue column to parse and rename\n\n    \"\"\"\n    self.date_column = date_column\n    self.response_column = response_column\n    self.revenue_column = revenue_column\n    self.control_columns = control_columns\n    self.channel_columns = channel_columns\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataProcessor-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataProcessor.process","title":"<code>process(df: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Process the DataFrame with configured transformations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>MissingRequiredColumnsError</code> <p>If the required columns are not present.</p> <code>InvalidDateFormatError</code> <p>If the date column cannot be parsed.</p> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process the DataFrame with configured transformations.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Processed DataFrame\n\n    Raises:\n        MissingRequiredColumnsError: If the required columns are not present.\n        InvalidDateFormatError: If the date column cannot be parsed.\n\n    \"\"\"\n    processed_df = df.copy()\n\n    # Validate that all required columns exist\n    self._validate_required_columns_present(\n        df=processed_df,\n        date_column=self.date_column,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n        control_columns=self.control_columns,\n        channel_columns=self.channel_columns,\n    )\n\n    # Parse date columns\n    processed_df = self._parse_date_columns(processed_df, self.date_column)\n\n    # Rename required columns\n    processed_df = self._rename_required_columns(\n        df=processed_df,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n    )\n\n    return processed_df.sort_values(self.date_column)\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataValidator","title":"<code>DataValidator(framework: str, date_column: str, response_column: str, revenue_column: str, control_columns: list[str] | None, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Validator for MMM data with configurable validation rules.</p> <p>Initialize validator with validation rules.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>a supported framework, one of <code>pymc_marketing</code> or <code>meridian</code></p> required <code>date_column</code> <code>str</code> <p>Name of the date column</p> required <code>response_column</code> <code>str</code> <p>Name of the response column</p> required <code>revenue_column</code> <code>str</code> <p>Name of the revenue column</p> required <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations for time series CV</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def __init__(\n    self,\n    framework: str,\n    date_column: str,\n    response_column: str,\n    revenue_column: str,\n    control_columns: list[str] | None,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize validator with validation rules.\n\n    Args:\n        framework: a supported framework, one of `pymc_marketing` or `meridian`\n        date_column: Name of the date column\n        response_column: Name of the response column\n        revenue_column: Name of the revenue column\n        control_columns: List of control columns\n        min_number_observations: Minimum required number of observations for time series CV\n\n    \"\"\"\n    self.framework = framework\n    self.date_column = date_column\n    self.response_column = response_column\n    self.revenue_column = revenue_column\n    self.min_number_observations = min_number_observations\n    self.control_columns = control_columns\n</code></pre>"},{"location":"api/data/#mmm_eval.data.DataValidator-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.DataValidator.run_validations","title":"<code>run_validations(df: pd.DataFrame) -&gt; None</code>","text":"<p>Run all validations on the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>None</code> <p>Validation result with all errors and warnings</p> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def run_validations(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Run all validations on the DataFrame.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Validation result with all errors and warnings\n\n    \"\"\"\n    # Run each validation in order\n    self._validate_not_empty(df)\n    self._validate_schema(df)\n    self._validate_data_size(df)\n    self._validate_response_and_revenue_columns_xor_zeroes(df)\n\n    # feature scaling is done automatically in Meridian\n    if self.control_columns and self.framework == \"pymc_marketing\":\n        self._check_control_variables_between_0_and_1(df=df, cols=self.control_columns)\n</code></pre>"},{"location":"api/data/#mmm_eval.data-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.generate_meridian_data","title":"<code>generate_meridian_data()</code>","text":"<p>Load and process a Meridian-compatible dataset for E2E testing.</p> <p>The Excel file should be placed at: mmm_eval/data/sample_data/geo_media.xlsx</p> <p>Returns     DataFrame containing Meridian-compatible data with media channels, controls, and     response variables</p> Source code in <code>mmm_eval/data/synth_data_generator.py</code> <pre><code>def generate_meridian_data():\n    \"\"\"Load and process a Meridian-compatible dataset for E2E testing.\n\n    The Excel file should be placed at: mmm_eval/data/sample_data/geo_media.xlsx\n\n    Returns\n        DataFrame containing Meridian-compatible data with media channels, controls, and\n        response variables\n\n    \"\"\"\n    # Path to the local Excel file\n    excel_path = Path(__file__).parent / \"sample_data\" / \"geo_media.xlsx\"\n\n    if not excel_path.exists():\n        raise FileNotFoundError(\n            f\"Meridian sample data file not found at {excel_path}. \"\n            \"Please download the file from \"\n            \"https://github.com/google/meridian/raw/main/meridian/data/simulated_data/xlsx/geo_media.xlsx\"\n            f\"and save it to {excel_path}.\"\n        )\n\n    df = pd.read_excel(excel_path, engine=\"openpyxl\")\n    df_mod = df.copy()\n    df_mod[\"revenue\"] = df_mod[\"revenue_per_conversion\"] * df_mod[\"conversions\"]\n\n    df_mod = df.iloc[:, 1:]\n    # restrict to only two geos\n    df_mod = df_mod[df_mod[\"geo\"].isin([\"Geo0\", \"Geo1\"])]\n    df_mod[\"revenue\"] = df_mod[\"revenue_per_conversion\"] * df_mod[\"conversions\"]\n    df_mod = df_mod.drop(columns=\"revenue_per_conversion\")\n\n    # restrict to only post-2023\n    df_mod = df_mod[pd.to_datetime(df_mod[\"time\"]) &gt; pd.Timestamp(\"2023-01-01\")]\n    df_mod = df_mod.rename(columns={\"time\": \"date\"})\n    return df_mod\n</code></pre>"},{"location":"api/data/#mmm_eval.data.generate_pymc_data","title":"<code>generate_pymc_data()</code>","text":"<p>Generate synthetic MMM data for testing purposes.</p> <p>Returns     DataFrame containing synthetic MMM data with media channels, controls, and response variables</p> Source code in <code>mmm_eval/data/synth_data_generator.py</code> <pre><code>def generate_pymc_data():\n    \"\"\"Generate synthetic MMM data for testing purposes.\n\n    Returns\n        DataFrame containing synthetic MMM data with media channels, controls, and response variables\n\n    \"\"\"\n    seed: int = sum(map(ord, \"mmm\"))\n    rng: np.random.Generator = np.random.default_rng(seed=seed)\n\n    # date range\n    min_date = pd.to_datetime(\"2018-04-01\")\n    max_date = pd.to_datetime(\"2021-09-01\")\n\n    df = pd.DataFrame(data={\"date_week\": pd.date_range(start=min_date, end=max_date, freq=\"W-MON\")}).assign(\n        year=lambda x: x[\"date_week\"].dt.year,\n        month=lambda x: x[\"date_week\"].dt.month,\n        dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n    )\n\n    n = df.shape[0]\n\n    # media spend data\n    channel_1 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_1\"] = np.where(channel_1 &gt; 90, channel_1, channel_1 / 2)\n\n    channel_2 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_2\"] = np.where(channel_2 &gt; 80, channel_2, 0)\n\n    # apply geometric adstock transformation\n    alpha1: float = 0.4\n    alpha2: float = 0.2\n\n    df[\"channel_1_adstock\"] = (\n        geometric_adstock(x=df[\"channel_1\"].to_numpy(), alpha=alpha1, l_max=8, normalize=True).eval().flatten()\n    )\n\n    df[\"channel_2_adstock\"] = (\n        geometric_adstock(x=df[\"channel_2\"].to_numpy(), alpha=alpha2, l_max=8, normalize=True).eval().flatten()\n    )\n\n    # apply saturation transformation\n    lam1: float = 4.0\n    lam2: float = 3.0\n\n    df[\"channel_1_adstock_saturated\"] = logistic_saturation(x=df[\"channel_1_adstock\"].to_numpy(), lam=lam1).eval()\n\n    df[\"channel_2_adstock_saturated\"] = logistic_saturation(x=df[\"channel_2_adstock\"].to_numpy(), lam=lam2).eval()\n\n    # trend + seasonal\n    df[\"trend\"] = (np.linspace(start=0.0, stop=50, num=n) + 10) ** (1 / 4) - 1\n\n    df[\"cs\"] = -np.sin(2 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"cc\"] = np.cos(1 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"seasonality\"] = 0.5 * (df[\"cs\"] + df[\"cc\"])\n\n    # controls\n    df[\"event_1\"] = (df[\"date_week\"] == \"2019-05-13\").astype(float)\n    df[\"event_2\"] = (df[\"date_week\"] == \"2020-09-14\").astype(float)\n\n    # generate quantity\n    df[\"intercept\"] = 1000.0  # Base quantity\n    # noise\n    df[\"epsilon\"] = rng.normal(loc=0.0, scale=50.0, size=n)\n\n    # amplitude = 1\n    beta_1 = 400\n    beta_2 = 150\n\n    # Generate price with seasonal fluctuations\n    base_price = 5\n    price_seasonality = 0.03 * (df[\"cs\"] + df[\"cc\"])\n    price_trend = np.linspace(0, 2, n)  # Gradual price increase\n    df[\"price\"] = base_price + price_seasonality + price_trend\n\n    df[\"quantity\"] = (\n        df[\"intercept\"]\n        + df[\"trend\"] * 100\n        + df[\"seasonality\"] * 200\n        + df[\"price\"] * -50\n        + 150 * df[\"event_1\"]\n        + 250 * df[\"event_2\"]\n        + beta_1 * df[\"channel_1_adstock_saturated\"]\n        + beta_2 * df[\"channel_2_adstock_saturated\"]\n        + df[\"epsilon\"]\n    )\n    # Calculate revenue\n    df[\"revenue\"] = df[\"price\"] * df[\"quantity\"]\n\n    columns_to_keep = [\n        \"date_week\",\n        \"quantity\",\n        \"price\",\n        \"revenue\",\n        \"channel_1\",\n        \"channel_2\",\n        \"event_1\",\n        \"event_2\",\n        \"dayofyear\",\n    ]\n\n    df = df[columns_to_keep]\n    return df\n</code></pre>"},{"location":"api/data/#mmm_eval.data-modules","title":"Modules","text":""},{"location":"api/data/#mmm_eval.data.constants","title":"<code>constants</code>","text":"<p>Defines the constants for the data pipeline.</p>"},{"location":"api/data/#mmm_eval.data.constants-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.constants.DataLoaderConstants","title":"<code>DataLoaderConstants</code>","text":"<p>Constants for the data loader.</p> Classes\u00b6 <code>ValidDataExtensions</code> \u00b6 <p>Valid data extensions.</p> Functions\u00b6 <code>all()</code> <code>classmethod</code> \u00b6 <p>Return list of all supported file extensions.</p> Source code in <code>mmm_eval/data/constants.py</code> <pre><code>@classmethod\ndef all(cls):\n    \"\"\"Return list of all supported file extensions.\"\"\"\n    return [cls.CSV, cls.PARQUET]\n</code></pre>"},{"location":"api/data/#mmm_eval.data.constants.DataPipelineConstants","title":"<code>DataPipelineConstants</code>","text":"<p>Constants for the data pipeline.</p>"},{"location":"api/data/#mmm_eval.data.constants.InputDataframeConstants","title":"<code>InputDataframeConstants</code>","text":"<p>Constants for the dataframe.</p>"},{"location":"api/data/#mmm_eval.data.exceptions","title":"<code>exceptions</code>","text":"<p>Custom exceptions for data validation and processing.</p>"},{"location":"api/data/#mmm_eval.data.exceptions-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.exceptions.DataValidationError","title":"<code>DataValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when data validation fails.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.EmptyDataFrameError","title":"<code>EmptyDataFrameError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when DataFrame is empty.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.InvalidDateFormatError","title":"<code>InvalidDateFormatError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when date parsing fails.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.MissingRequiredColumnsError","title":"<code>MissingRequiredColumnsError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when required columns are missing.</p>"},{"location":"api/data/#mmm_eval.data.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for validation errors.</p>"},{"location":"api/data/#mmm_eval.data.loaders","title":"<code>loaders</code>","text":"<p>Data loading utilities for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.loaders-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.loaders.DataLoader","title":"<code>DataLoader(data_path: str | Path)</code>","text":"<p>Simple data loader for MMM evaluation.</p> <p>Takes a data path and loads the data.</p> <p>Initialize data loader with data path.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str | Path</code> <p>Path to the data file (CSV, Parquet, etc.)</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the data file does not exist.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def __init__(self, data_path: str | Path):\n    \"\"\"Initialize data loader with data path.\n\n    Args:\n        data_path: Path to the data file (CSV, Parquet, etc.)\n\n    Raises:\n        FileNotFoundError: If the data file does not exist.\n\n    \"\"\"\n    self.data_path = Path(data_path)\n\n    if not self.data_path.exists():\n        raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n</code></pre> Functions\u00b6 <code>load() -&gt; pd.DataFrame</code> \u00b6 <p>Load data from the specified path.</p> <p>Returns     Loaded DataFrame</p> <p>Raises     ValueError: If the file format is not supported.</p> Source code in <code>mmm_eval/data/loaders.py</code> <pre><code>def load(self) -&gt; pd.DataFrame:\n    \"\"\"Load data from the specified path.\n\n    Returns\n        Loaded DataFrame\n\n    Raises\n        ValueError: If the file format is not supported.\n\n    \"\"\"\n    ext = self.data_path.suffix.lower().lstrip(\".\")\n    if ext not in DataLoaderConstants.ValidDataExtensions.all():\n        raise ValueError(f\"Unsupported file format: {self.data_path.suffix}\")\n\n    if ext == DataLoaderConstants.ValidDataExtensions.CSV:\n        return self._load_csv()\n    elif ext == DataLoaderConstants.ValidDataExtensions.PARQUET:\n        return self._load_parquet()\n</code></pre>"},{"location":"api/data/#mmm_eval.data.pipeline","title":"<code>pipeline</code>","text":"<p>Data pipeline for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.pipeline-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.pipeline.DataPipeline","title":"<code>DataPipeline(data: pd.DataFrame, framework: str, control_columns: list[str] | None, channel_columns: list[str], date_column: str, response_column: str, revenue_column: str, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Data pipeline that orchestrates loading, processing, and validation.</p> <p>Provides a simple interface to go from raw data file to validated DataFrame.</p> <p>Initialize data pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the data</p> required <code>framework</code> <code>str</code> <p>name of supported framework</p> required <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column</p> required <code>response_column</code> <code>str</code> <p>Name of the response column</p> required <code>revenue_column</code> <code>str</code> <p>Name of the revenue column</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    framework: str,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str,\n    response_column: str,\n    revenue_column: str,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize data pipeline.\n\n    Args:\n        data: DataFrame containing the data\n        framework: name of supported framework\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column\n        response_column: Name of the response column\n        revenue_column: Name of the revenue column\n        min_number_observations: Minimum required number of observations\n\n    \"\"\"\n    # Initialize components\n    self.data = data\n    self.processor = DataProcessor(\n        date_column=date_column,\n        response_column=response_column,\n        revenue_column=revenue_column,\n        control_columns=control_columns,\n        channel_columns=channel_columns,\n    )\n    self.validator = DataValidator(\n        framework=framework,\n        date_column=date_column,\n        response_column=InputDataframeConstants.RESPONSE_COL,\n        revenue_column=InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,\n        control_columns=control_columns,\n        min_number_observations=min_number_observations,\n    )\n</code></pre> Functions\u00b6 <code>run() -&gt; pd.DataFrame</code> \u00b6 <p>Run the complete data pipeline: process \u2192 validate.</p> <p>Returns     Validated and processed DataFrame</p> <p>Raises     Various exceptions processing or validation steps</p> Source code in <code>mmm_eval/data/pipeline.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"Run the complete data pipeline: process \u2192 validate.\n\n    Returns\n        Validated and processed DataFrame\n\n    Raises\n        Various exceptions processing or validation steps\n\n    \"\"\"\n    processed_df = self.processor.process(self.data)\n\n    self.validator.run_validations(processed_df)\n\n    return processed_df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.processor","title":"<code>processor</code>","text":"<p>Data processing utilities for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.processor-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.processor.DataProcessor","title":"<code>DataProcessor(control_columns: list[str] | None, channel_columns: list[str], date_column: str = InputDataframeConstants.DATE_COL, response_column: str = InputDataframeConstants.RESPONSE_COL, revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL)</code>","text":"<p>Simple data processor for MMM evaluation.</p> <p>Handles data transformations like datetime casting, column renaming, etc.</p> <p>Initialize data processor.</p> <p>Parameters:</p> Name Type Description Default <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>channel_columns</code> <code>list[str]</code> <p>List of channel columns</p> required <code>date_column</code> <code>str</code> <p>Name of the date column to parse and rename</p> <code>DATE_COL</code> <code>response_column</code> <code>str</code> <p>Name of the response column to parse and rename</p> <code>RESPONSE_COL</code> <code>revenue_column</code> <code>str</code> <p>Name of the revenue column to parse and rename</p> <code>MEDIA_CHANNEL_REVENUE_COL</code> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def __init__(\n    self,\n    control_columns: list[str] | None,\n    channel_columns: list[str],\n    date_column: str = InputDataframeConstants.DATE_COL,\n    response_column: str = InputDataframeConstants.RESPONSE_COL,\n    revenue_column: str = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,\n):\n    \"\"\"Initialize data processor.\n\n    Args:\n        control_columns: List of control columns\n        channel_columns: List of channel columns\n        date_column: Name of the date column to parse and rename\n        response_column: Name of the response column to parse and rename\n        revenue_column: Name of the revenue column to parse and rename\n\n    \"\"\"\n    self.date_column = date_column\n    self.response_column = response_column\n    self.revenue_column = revenue_column\n    self.control_columns = control_columns\n    self.channel_columns = channel_columns\n</code></pre> Functions\u00b6 <code>process(df: pd.DataFrame) -&gt; pd.DataFrame</code> \u00b6 <p>Process the DataFrame with configured transformations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>MissingRequiredColumnsError</code> <p>If the required columns are not present.</p> <code>InvalidDateFormatError</code> <p>If the date column cannot be parsed.</p> Source code in <code>mmm_eval/data/processor.py</code> <pre><code>def process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process the DataFrame with configured transformations.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Processed DataFrame\n\n    Raises:\n        MissingRequiredColumnsError: If the required columns are not present.\n        InvalidDateFormatError: If the date column cannot be parsed.\n\n    \"\"\"\n    processed_df = df.copy()\n\n    # Validate that all required columns exist\n    self._validate_required_columns_present(\n        df=processed_df,\n        date_column=self.date_column,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n        control_columns=self.control_columns,\n        channel_columns=self.channel_columns,\n    )\n\n    # Parse date columns\n    processed_df = self._parse_date_columns(processed_df, self.date_column)\n\n    # Rename required columns\n    processed_df = self._rename_required_columns(\n        df=processed_df,\n        response_column=self.response_column,\n        revenue_column=self.revenue_column,\n    )\n\n    return processed_df.sort_values(self.date_column)\n</code></pre>"},{"location":"api/data/#mmm_eval.data.schemas","title":"<code>schemas</code>","text":"<p>Pydantic schemas for MMM data validation.</p>"},{"location":"api/data/#mmm_eval.data.schemas-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.schemas.ValidatedDataSchema","title":"<code>ValidatedDataSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Schema for MMM data validation.</p> <p>Defines the bare minimum columns for MMM evaluation.</p> Classes\u00b6 <code>Config</code> \u00b6 <p>Config for the schema.</p>"},{"location":"api/data/#mmm_eval.data.synth_data_generator","title":"<code>synth_data_generator</code>","text":"<p>Generate synthetic data for testing.</p> <p>Based on: https://www.pymc-marketing.io/en/stable/notebooks/mmm/mmm_example.html</p>"},{"location":"api/data/#mmm_eval.data.synth_data_generator-functions","title":"Functions","text":""},{"location":"api/data/#mmm_eval.data.synth_data_generator.generate_meridian_data","title":"<code>generate_meridian_data()</code>","text":"<p>Load and process a Meridian-compatible dataset for E2E testing.</p> <p>The Excel file should be placed at: mmm_eval/data/sample_data/geo_media.xlsx</p> <p>Returns     DataFrame containing Meridian-compatible data with media channels, controls, and     response variables</p> Source code in <code>mmm_eval/data/synth_data_generator.py</code> <pre><code>def generate_meridian_data():\n    \"\"\"Load and process a Meridian-compatible dataset for E2E testing.\n\n    The Excel file should be placed at: mmm_eval/data/sample_data/geo_media.xlsx\n\n    Returns\n        DataFrame containing Meridian-compatible data with media channels, controls, and\n        response variables\n\n    \"\"\"\n    # Path to the local Excel file\n    excel_path = Path(__file__).parent / \"sample_data\" / \"geo_media.xlsx\"\n\n    if not excel_path.exists():\n        raise FileNotFoundError(\n            f\"Meridian sample data file not found at {excel_path}. \"\n            \"Please download the file from \"\n            \"https://github.com/google/meridian/raw/main/meridian/data/simulated_data/xlsx/geo_media.xlsx\"\n            f\"and save it to {excel_path}.\"\n        )\n\n    df = pd.read_excel(excel_path, engine=\"openpyxl\")\n    df_mod = df.copy()\n    df_mod[\"revenue\"] = df_mod[\"revenue_per_conversion\"] * df_mod[\"conversions\"]\n\n    df_mod = df.iloc[:, 1:]\n    # restrict to only two geos\n    df_mod = df_mod[df_mod[\"geo\"].isin([\"Geo0\", \"Geo1\"])]\n    df_mod[\"revenue\"] = df_mod[\"revenue_per_conversion\"] * df_mod[\"conversions\"]\n    df_mod = df_mod.drop(columns=\"revenue_per_conversion\")\n\n    # restrict to only post-2023\n    df_mod = df_mod[pd.to_datetime(df_mod[\"time\"]) &gt; pd.Timestamp(\"2023-01-01\")]\n    df_mod = df_mod.rename(columns={\"time\": \"date\"})\n    return df_mod\n</code></pre>"},{"location":"api/data/#mmm_eval.data.synth_data_generator.generate_pymc_data","title":"<code>generate_pymc_data()</code>","text":"<p>Generate synthetic MMM data for testing purposes.</p> <p>Returns     DataFrame containing synthetic MMM data with media channels, controls, and response variables</p> Source code in <code>mmm_eval/data/synth_data_generator.py</code> <pre><code>def generate_pymc_data():\n    \"\"\"Generate synthetic MMM data for testing purposes.\n\n    Returns\n        DataFrame containing synthetic MMM data with media channels, controls, and response variables\n\n    \"\"\"\n    seed: int = sum(map(ord, \"mmm\"))\n    rng: np.random.Generator = np.random.default_rng(seed=seed)\n\n    # date range\n    min_date = pd.to_datetime(\"2018-04-01\")\n    max_date = pd.to_datetime(\"2021-09-01\")\n\n    df = pd.DataFrame(data={\"date_week\": pd.date_range(start=min_date, end=max_date, freq=\"W-MON\")}).assign(\n        year=lambda x: x[\"date_week\"].dt.year,\n        month=lambda x: x[\"date_week\"].dt.month,\n        dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n    )\n\n    n = df.shape[0]\n\n    # media spend data\n    channel_1 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_1\"] = np.where(channel_1 &gt; 90, channel_1, channel_1 / 2)\n\n    channel_2 = 100 * rng.uniform(low=0.0, high=1, size=n)\n    df[\"channel_2\"] = np.where(channel_2 &gt; 80, channel_2, 0)\n\n    # apply geometric adstock transformation\n    alpha1: float = 0.4\n    alpha2: float = 0.2\n\n    df[\"channel_1_adstock\"] = (\n        geometric_adstock(x=df[\"channel_1\"].to_numpy(), alpha=alpha1, l_max=8, normalize=True).eval().flatten()\n    )\n\n    df[\"channel_2_adstock\"] = (\n        geometric_adstock(x=df[\"channel_2\"].to_numpy(), alpha=alpha2, l_max=8, normalize=True).eval().flatten()\n    )\n\n    # apply saturation transformation\n    lam1: float = 4.0\n    lam2: float = 3.0\n\n    df[\"channel_1_adstock_saturated\"] = logistic_saturation(x=df[\"channel_1_adstock\"].to_numpy(), lam=lam1).eval()\n\n    df[\"channel_2_adstock_saturated\"] = logistic_saturation(x=df[\"channel_2_adstock\"].to_numpy(), lam=lam2).eval()\n\n    # trend + seasonal\n    df[\"trend\"] = (np.linspace(start=0.0, stop=50, num=n) + 10) ** (1 / 4) - 1\n\n    df[\"cs\"] = -np.sin(2 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"cc\"] = np.cos(1 * 2 * np.pi * df[\"dayofyear\"] / 365.5)\n    df[\"seasonality\"] = 0.5 * (df[\"cs\"] + df[\"cc\"])\n\n    # controls\n    df[\"event_1\"] = (df[\"date_week\"] == \"2019-05-13\").astype(float)\n    df[\"event_2\"] = (df[\"date_week\"] == \"2020-09-14\").astype(float)\n\n    # generate quantity\n    df[\"intercept\"] = 1000.0  # Base quantity\n    # noise\n    df[\"epsilon\"] = rng.normal(loc=0.0, scale=50.0, size=n)\n\n    # amplitude = 1\n    beta_1 = 400\n    beta_2 = 150\n\n    # Generate price with seasonal fluctuations\n    base_price = 5\n    price_seasonality = 0.03 * (df[\"cs\"] + df[\"cc\"])\n    price_trend = np.linspace(0, 2, n)  # Gradual price increase\n    df[\"price\"] = base_price + price_seasonality + price_trend\n\n    df[\"quantity\"] = (\n        df[\"intercept\"]\n        + df[\"trend\"] * 100\n        + df[\"seasonality\"] * 200\n        + df[\"price\"] * -50\n        + 150 * df[\"event_1\"]\n        + 250 * df[\"event_2\"]\n        + beta_1 * df[\"channel_1_adstock_saturated\"]\n        + beta_2 * df[\"channel_2_adstock_saturated\"]\n        + df[\"epsilon\"]\n    )\n    # Calculate revenue\n    df[\"revenue\"] = df[\"price\"] * df[\"quantity\"]\n\n    columns_to_keep = [\n        \"date_week\",\n        \"quantity\",\n        \"price\",\n        \"revenue\",\n        \"channel_1\",\n        \"channel_2\",\n        \"event_1\",\n        \"event_2\",\n        \"dayofyear\",\n    ]\n\n    df = df[columns_to_keep]\n    return df\n</code></pre>"},{"location":"api/data/#mmm_eval.data.validation","title":"<code>validation</code>","text":"<p>Data validation for MMM evaluation.</p>"},{"location":"api/data/#mmm_eval.data.validation-classes","title":"Classes","text":""},{"location":"api/data/#mmm_eval.data.validation.DataValidator","title":"<code>DataValidator(framework: str, date_column: str, response_column: str, revenue_column: str, control_columns: list[str] | None, min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS)</code>","text":"<p>Validator for MMM data with configurable validation rules.</p> <p>Initialize validator with validation rules.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>a supported framework, one of <code>pymc_marketing</code> or <code>meridian</code></p> required <code>date_column</code> <code>str</code> <p>Name of the date column</p> required <code>response_column</code> <code>str</code> <p>Name of the response column</p> required <code>revenue_column</code> <code>str</code> <p>Name of the revenue column</p> required <code>control_columns</code> <code>list[str] | None</code> <p>List of control columns</p> required <code>min_number_observations</code> <code>int</code> <p>Minimum required number of observations for time series CV</p> <code>MIN_NUMBER_OBSERVATIONS</code> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def __init__(\n    self,\n    framework: str,\n    date_column: str,\n    response_column: str,\n    revenue_column: str,\n    control_columns: list[str] | None,\n    min_number_observations: int = DataPipelineConstants.MIN_NUMBER_OBSERVATIONS,\n):\n    \"\"\"Initialize validator with validation rules.\n\n    Args:\n        framework: a supported framework, one of `pymc_marketing` or `meridian`\n        date_column: Name of the date column\n        response_column: Name of the response column\n        revenue_column: Name of the revenue column\n        control_columns: List of control columns\n        min_number_observations: Minimum required number of observations for time series CV\n\n    \"\"\"\n    self.framework = framework\n    self.date_column = date_column\n    self.response_column = response_column\n    self.revenue_column = revenue_column\n    self.min_number_observations = min_number_observations\n    self.control_columns = control_columns\n</code></pre> Functions\u00b6 <code>run_validations(df: pd.DataFrame) -&gt; None</code> \u00b6 <p>Run all validations on the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>None</code> <p>Validation result with all errors and warnings</p> Source code in <code>mmm_eval/data/validation.py</code> <pre><code>def run_validations(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Run all validations on the DataFrame.\n\n    Args:\n        df: Input DataFrame\n\n    Returns:\n        Validation result with all errors and warnings\n\n    \"\"\"\n    # Run each validation in order\n    self._validate_not_empty(df)\n    self._validate_schema(df)\n    self._validate_data_size(df)\n    self._validate_response_and_revenue_columns_xor_zeroes(df)\n\n    # feature scaling is done automatically in Meridian\n    if self.control_columns and self.framework == \"pymc_marketing\":\n        self._check_control_variables_between_0_and_1(df=df, cols=self.control_columns)\n</code></pre>"},{"location":"api/metrics/","title":"Metrics Reference","text":""},{"location":"api/metrics/#mmm_eval.metrics","title":"<code>mmm_eval.metrics</code>","text":"<p>Accuracy metrics for MMM evaluation.</p>"},{"location":"api/metrics/#mmm_eval.metrics-functions","title":"Functions","text":""},{"location":"api/metrics/#mmm_eval.metrics.calculate_absolute_percentage_change","title":"<code>calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series</code>","text":"<p>Calculate the absolute percentage change between two series.</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series:\n    \"\"\"Calculate the absolute percentage change between two series.\"\"\"\n    return np.abs((comparison_series - baseline_series) / baseline_series)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_mean_for_singular_values_across_cross_validation_folds","title":"<code>calculate_mean_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the mean of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_mean_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the mean of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Mean value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.mean([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_means_for_series_across_cross_validation_folds","title":"<code>calculate_means_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the mean of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Mean Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_means_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the mean of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Mean Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).mean(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_std_for_singular_values_across_cross_validation_folds","title":"<code>calculate_std_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the standard deviation of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Standard deviation value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_std_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the standard deviation of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Standard deviation value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.std([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.calculate_stds_for_series_across_cross_validation_folds","title":"<code>calculate_stds_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the standard deviation of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Standard deviation Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_stds_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the standard deviation of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Standard deviation Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).std(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics-modules","title":"Modules","text":""},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions","title":"<code>accuracy_functions</code>","text":"<p>Accuracy metrics for MMM evaluation.</p>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions-functions","title":"Functions","text":""},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_absolute_percentage_change","title":"<code>calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series</code>","text":"<p>Calculate the absolute percentage change between two series.</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_absolute_percentage_change(baseline_series: pd.Series, comparison_series: pd.Series) -&gt; pd.Series:\n    \"\"\"Calculate the absolute percentage change between two series.\"\"\"\n    return np.abs((comparison_series - baseline_series) / baseline_series)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_mean_for_singular_values_across_cross_validation_folds","title":"<code>calculate_mean_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the mean of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_mean_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the mean of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Mean value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.mean([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_means_for_series_across_cross_validation_folds","title":"<code>calculate_means_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the mean of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Mean Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_means_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the mean of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Mean Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).mean(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_std_for_singular_values_across_cross_validation_folds","title":"<code>calculate_std_for_singular_values_across_cross_validation_folds(fold_metrics: list[AccuracyMetricResults], metric_name: AccuracyMetricNames) -&gt; float</code>","text":"<p>Calculate the standard deviation of the fold metrics for single values.</p> <p>Parameters:</p> Name Type Description Default <code>fold_metrics</code> <code>list[AccuracyMetricResults]</code> <p>List of metric result objects</p> required <code>metric_name</code> <code>AccuracyMetricNames</code> <p>Name of the metric attribute</p> required <p>Returns:</p> Type Description <code>float</code> <p>Standard deviation value as float</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_std_for_singular_values_across_cross_validation_folds(\n    fold_metrics: list[AccuracyMetricResults],\n    metric_name: AccuracyMetricNames,\n) -&gt; float:\n    \"\"\"Calculate the standard deviation of the fold metrics for single values.\n\n    Args:\n        fold_metrics: List of metric result objects\n        metric_name: Name of the metric attribute\n\n    Returns:\n        Standard deviation value as float\n\n    \"\"\"\n    metric_attr = metric_name.value\n    return np.std([getattr(fold_metric, metric_attr) for fold_metric in fold_metrics])\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.accuracy_functions.calculate_stds_for_series_across_cross_validation_folds","title":"<code>calculate_stds_for_series_across_cross_validation_folds(folds_of_series: list[pd.Series]) -&gt; pd.Series</code>","text":"<p>Calculate the standard deviation of pandas Series across folds.</p> <p>Parameters:</p> Name Type Description Default <code>folds_of_series</code> <code>list[Series]</code> <p>List of pandas Series (e.g., ROI series from different folds)</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Standard deviation Series with same index as input series</p> Source code in <code>mmm_eval/metrics/accuracy_functions.py</code> <pre><code>def calculate_stds_for_series_across_cross_validation_folds(\n    folds_of_series: list[pd.Series],\n) -&gt; pd.Series:\n    \"\"\"Calculate the standard deviation of pandas Series across folds.\n\n    Args:\n        folds_of_series: List of pandas Series (e.g., ROI series from different folds)\n\n    Returns:\n        Standard deviation Series with same index as input series\n\n    \"\"\"\n    return pd.concat(folds_of_series, axis=1).std(axis=1)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.exceptions","title":"<code>exceptions</code>","text":"<p>Exceptions for the metrics module.</p>"},{"location":"api/metrics/#mmm_eval.metrics.exceptions-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.exceptions.InvalidMetricNameException","title":"<code>InvalidMetricNameException</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Exception raised when an invalid metric name is provided.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models","title":"<code>metric_models</code>","text":""},{"location":"api/metrics/#mmm_eval.metrics.metric_models-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.metric_models.AccuracyMetricNames","title":"<code>AccuracyMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the accuracy metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.AccuracyMetricResults","title":"<code>AccuracyMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the accuracy metrics.</p> Functions\u00b6 <code>populate_object_with_metrics(actual: pd.Series, predicted: pd.Series) -&gt; AccuracyMetricResults</code> <code>classmethod</code> \u00b6 <p>Populate the object with the calculated metrics.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>Series</code> <p>The actual values</p> required <code>predicted</code> <code>Series</code> <p>The predicted values</p> required <p>Returns:</p> Type Description <code>AccuracyMetricResults</code> <p>AccuracyMetricResults object with the metrics</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>@classmethod\ndef populate_object_with_metrics(cls, actual: pd.Series, predicted: pd.Series) -&gt; \"AccuracyMetricResults\":\n    \"\"\"Populate the object with the calculated metrics.\n\n    Args:\n        actual: The actual values\n        predicted: The predicted values\n\n    Returns:\n        AccuracyMetricResults object with the metrics\n\n    \"\"\"\n    return cls(\n        mape=mean_absolute_percentage_error(actual, predicted),\n        r_squared=r2_score(actual, predicted),\n    )\n</code></pre> <code>to_df() -&gt; pd.DataFrame</code> \u00b6 <p>Convert the accuracy metric results to a long DataFrame format.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the accuracy metric results to a long DataFrame format.\"\"\"\n    df = pd.DataFrame(\n        [\n            self._create_single_metric_dataframe_row(\n                general_metric_name=AccuracyMetricNames.MAPE.value,\n                specific_metric_name=AccuracyMetricNames.MAPE.value,\n                metric_value=self.mape,\n            ),\n            self._create_single_metric_dataframe_row(\n                general_metric_name=AccuracyMetricNames.R_SQUARED.value,\n                specific_metric_name=AccuracyMetricNames.R_SQUARED.value,\n                metric_value=self.r_squared,\n            ),\n        ]\n    )\n    return self.add_pass_fail_column(df)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.CrossValidationMetricNames","title":"<code>CrossValidationMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the cross-validation metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.CrossValidationMetricResults","title":"<code>CrossValidationMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the cross-validation metrics.</p> Functions\u00b6 <code>to_df() -&gt; pd.DataFrame</code> \u00b6 <p>Convert the cross-validation metric results to a long DataFrame format.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the cross-validation metric results to a long DataFrame format.\"\"\"\n    df = pd.DataFrame(\n        [\n            self._create_single_metric_dataframe_row(\n                general_metric_name=CrossValidationMetricNames.MEAN_MAPE.value,\n                specific_metric_name=CrossValidationMetricNames.MEAN_MAPE.value,\n                metric_value=self.mean_mape,\n            ),\n            self._create_single_metric_dataframe_row(\n                general_metric_name=CrossValidationMetricNames.STD_MAPE.value,\n                specific_metric_name=CrossValidationMetricNames.STD_MAPE.value,\n                metric_value=self.std_mape,\n            ),\n            self._create_single_metric_dataframe_row(\n                general_metric_name=CrossValidationMetricNames.MEAN_R_SQUARED.value,\n                specific_metric_name=CrossValidationMetricNames.MEAN_R_SQUARED.value,\n                metric_value=self.mean_r_squared,\n            ),\n        ]\n    )\n    return self.add_pass_fail_column(df)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.MetricNamesBase","title":"<code>MetricNamesBase</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Base class for metric name enums.</p> Functions\u00b6 <code>to_list() -&gt; list[str]</code> <code>classmethod</code> \u00b6 <p>Convert the enum to a list of strings.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>@classmethod\ndef to_list(cls) -&gt; list[str]:\n    \"\"\"Convert the enum to a list of strings.\"\"\"\n    return [member.value for member in cls]\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.MetricResults","title":"<code>MetricResults</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Define the results of the metrics.</p> Functions\u00b6 <code>add_pass_fail_column(df: pd.DataFrame) -&gt; pd.DataFrame</code> \u00b6 <p>Add a pass/fail column to the DataFrame based on metric thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with general_metric_name and metric_value columns</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with additional metric_pass column</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def add_pass_fail_column(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Add a pass/fail column to the DataFrame based on metric thresholds.\n\n    Args:\n        df: DataFrame with general_metric_name and metric_value columns\n\n    Returns:\n        DataFrame with additional metric_pass column\n\n    \"\"\"\n    df_copy = df.copy()\n    df_copy[TestResultDFAttributes.METRIC_PASS.value] = df_copy.apply(\n        lambda row: self._check_metric_threshold(\n            row[TestResultDFAttributes.GENERAL_METRIC_NAME.value], row[TestResultDFAttributes.METRIC_VALUE.value]\n        ),\n        axis=1,\n    )\n    return df_copy\n</code></pre> <code>to_df() -&gt; pd.DataFrame</code> \u00b6 <p>Convert the class of test results to a flat DataFrame format.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the class of test results to a flat DataFrame format.\"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre> <code>to_dict() -&gt; dict[str, Any]</code> \u00b6 <p>Convert the class of test results to dictionary format.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert the class of test results to dictionary format.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.PerturbationMetricNames","title":"<code>PerturbationMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the perturbation metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.PerturbationMetricResults","title":"<code>PerturbationMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the perturbation metrics.</p> Functions\u00b6 <code>to_df() -&gt; pd.DataFrame</code> \u00b6 <p>Convert the perturbation metric results to a long DataFrame format.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the perturbation metric results to a long DataFrame format.\"\"\"\n    df = pd.DataFrame(\n        self._create_channel_based_metric_dataframe_rows(\n            channel_series=self.percentage_change_for_each_channel,\n            metric_name=PerturbationMetricNames.PERCENTAGE_CHANGE,\n        )\n    )\n    return self.add_pass_fail_column(df)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.RefreshStabilityMetricNames","title":"<code>RefreshStabilityMetricNames</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the names of the stability metrics.</p>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.RefreshStabilityMetricResults","title":"<code>RefreshStabilityMetricResults</code>","text":"<p>               Bases: <code>MetricResults</code></p> <p>Define the results of the refresh stability metrics.</p> Functions\u00b6 <code>to_df() -&gt; pd.DataFrame</code> \u00b6 <p>Convert the refresh stability metric results to a long DataFrame format.</p> Source code in <code>mmm_eval/metrics/metric_models.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the refresh stability metric results to a long DataFrame format.\"\"\"\n    rows = []\n\n    # Add mean and std percentage change for each channel\n    rows.extend(\n        self._create_channel_based_metric_dataframe_rows(\n            channel_series=self.mean_percentage_change_for_each_channel,\n            metric_name=RefreshStabilityMetricNames.MEAN_PERCENTAGE_CHANGE,\n        )\n    )\n    rows.extend(\n        self._create_channel_based_metric_dataframe_rows(\n            channel_series=self.std_percentage_change_for_each_channel,\n            metric_name=RefreshStabilityMetricNames.STD_PERCENTAGE_CHANGE,\n        )\n    )\n\n    df = pd.DataFrame(rows)\n    return self.add_pass_fail_column(df)\n</code></pre>"},{"location":"api/metrics/#mmm_eval.metrics.metric_models.TestResultDFAttributes","title":"<code>TestResultDFAttributes</code>","text":"<p>               Bases: <code>MetricNamesBase</code></p> <p>Define the attributes of the test result DataFrame.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants","title":"<code>threshold_constants</code>","text":""},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants-classes","title":"Classes","text":""},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.AccuracyThresholdConstants","title":"<code>AccuracyThresholdConstants</code>","text":"<p>Constants for the accuracy threshold.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.CrossValidationThresholdConstants","title":"<code>CrossValidationThresholdConstants</code>","text":"<p>Constants for the cross-validation threshold.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.PerturbationThresholdConstants","title":"<code>PerturbationThresholdConstants</code>","text":"<p>Constants for the perturbation threshold.</p>"},{"location":"api/metrics/#mmm_eval.metrics.threshold_constants.RefreshStabilityThresholdConstants","title":"<code>RefreshStabilityThresholdConstants</code>","text":"<p>Constants for the refresh stability threshold.</p>"},{"location":"development/contributing/","title":"Contributing to mmm-eval","text":"<p>We welcome contributions from the community! This guide will help you get started with contributing to mmm-eval</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ol> <li>Python 3.11+ - Required for development</li> <li>Git - For version control</li> <li>Poetry - For dependency management (recommended)</li> </ol>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository on GitHub</li> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/YOUR_USERNAME/mmm-eval.git\ncd mmm-eval\n</code></pre></p> </li> <li> <p>Set up the development environment:    <pre><code># Install dependencies\npoetry install\n\n# Activate the environment\npoetry shell\n</code></pre></p> </li> <li> <p>(Optional) Install pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write your code following the coding standards</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> </ul>"},{"location":"development/contributing/#3-test-your-changes","title":"3. Test Your Changes","text":"<pre><code># Run all tests\ntox\n\n# Run specific test categories\npytest tests/unit/\npytest tests/integration/\n\n# Run linting and formatting\nblack mmm_eval tests\nisort mmm_eval tests\nruff check mmm_eval tests\n</code></pre>"},{"location":"development/contributing/#4-commit-your-changes","title":"4. Commit Your Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add new feature description\"\n</code></pre> <p>Where possible, please follow the conventional commits format:</p> <ul> <li><code>feat:</code> New features</li> <li><code>fix:</code> Bug fixes</li> <li><code>docs:</code> Documentation changes</li> <li><code>style:</code> Code style changes</li> <li><code>refactor:</code> Code refactoring</li> <li><code>test:</code> Test changes</li> <li><code>chore:</code> Maintenance tasks</li> </ul>"},{"location":"development/contributing/#5-push-and-create-a-pull-request","title":"5. Push and Create a Pull Request","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"development/contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"development/contributing/#python-code-style","title":"Python Code Style","text":"<p>We use several tools to maintain code quality:</p> <ul> <li>Black - Code formatting</li> <li>isort - Import sorting</li> <li>Ruff - Linting and additional checks</li> <li>Pyright - Type checking</li> </ul>"},{"location":"development/contributing/#code-formatting","title":"Code Formatting","text":"<pre><code># Format code\nblack mmm_eval tests\n\n# Sort imports\nisort mmm_eval tests\n\n# Run linting\nruff check mmm_eval tests\nruff check --fix mmm_eval tests\n</code></pre>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>We use type hints throughout the codebase:</p> <pre><code>from typing import List, Optional, Dict, Any\n\ndef process_data(data: List[Dict[str, Any]]) -&gt; Optional[Dict[str, float]]:\n    \"\"\"Process the input data and return results.\"\"\"\n    pass\n</code></pre>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def calculate_mape(actual: List[float], predicted: List[float]) -&gt; float:\n    \"\"\"Calculate Mean Absolute Percentage Error.\n\n    Args:\n        actual: List of actual values\n        predicted: List of predicted values\n\n    Returns:\n        MAPE value as a float\n\n    Raises:\n        ValueError: If inputs are empty or have different lengths\n    \"\"\"\n    pass\n</code></pre>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=mmm_eval\n\n# Run specific test file\npytest tests/test_metrics.py\n\n# Run tests in parallel\npytest -n auto\n</code></pre>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Use descriptive test names</li> <li>Test both success and failure cases</li> <li>Use fixtures for common test data</li> </ul> <p>Example test:</p> <pre><code>import pytest\nfrom mmm_eval.metrics import calculate_mape\n\ndef test_calculate_mape_basic():\n    \"\"\"Test basic MAPE calculation.\"\"\"\n    actual = [100, 200, 300]\n    predicted = [110, 190, 310]\n\n    mape = calculate_mape(actual, predicted)\n\n    assert isinstance(mape, float)\n    assert mape &gt; 0\n\ndef test_calculate_mape_empty_input():\n    \"\"\"Test MAPE calculation with empty input.\"\"\"\n    with pytest.raises(ValueError):\n        calculate_mape([], [])\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#updating-documentation","title":"Updating Documentation","text":"<ol> <li>Update docstrings in the code</li> <li>Update markdown files in the <code>docs/</code> directory</li> <li>Build and test documentation:    <pre><code>mkdocs serve\n</code></pre></li> </ol>"},{"location":"development/contributing/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Keep documentation up to date with code changes</li> <li>Use proper markdown formatting</li> </ul>"},{"location":"development/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Ensure all tests pass</li> <li>Update documentation if needed</li> <li>Add tests for new functionality</li> <li>Follow coding standards</li> <li>Update CHANGELOG.md</li> </ol>"},{"location":"development/contributing/#pull-request-template","title":"Pull Request Template","text":"<p>Use the provided pull request template and fill in all sections:</p> <ul> <li>Description - What does this PR do?</li> <li>Type of change - Bug fix, feature, documentation, etc.</li> <li>Testing - How was this tested?</li> </ul>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated checks must pass</li> <li>Code review by maintainers</li> <li>Documentation review if needed</li> <li>Merge after approval</li> </ol>"},{"location":"development/contributing/#issue-reporting","title":"Issue Reporting","text":""},{"location":"development/contributing/#before-creating-an-issue","title":"Before Creating an Issue","text":"<ol> <li>Search existing issues to avoid duplicates</li> <li>Check documentation for solutions</li> <li>Update to the latest version of BenjaMMMin</li> </ol>"},{"location":"development/contributing/#issue-template","title":"Issue Template","text":"<p>Be sure to include:</p> <ul> <li>Description - Clear description of the problem</li> <li>Steps to reproduce - Detailed steps</li> <li>Expected behavior - What should happen</li> <li>Actual behavior - What actually happens</li> <li>Environment - OS, Python version, BenjaMMMin version</li> <li>Additional context - Any other relevant information</li> </ul>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive environment. Please:</p> <ul> <li>Be respectful and inclusive</li> <li>Use welcoming and inclusive language</li> <li>Be collaborative and constructive</li> <li>Focus on what is best for the community</li> </ul>"},{"location":"development/contributing/#communication","title":"Communication","text":"<ul> <li>GitHub Issues - For bug reports and feature requests</li> <li>GitHub Discussions - For questions and general discussion</li> <li>Pull Requests - For code contributions</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with contributing:</p> <ol> <li>Check the documentation first</li> <li>Search existing issues and discussions</li> <li>Create a new discussion for questions</li> <li>Join our community channels</li> </ol>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in:</p> <ul> <li>README.md - For significant contributions</li> <li>CHANGELOG.md - For all contributions</li> <li>GitHub contributors page</li> </ul> <p>Thank you for contributing to BenjaMMMin! \ud83c\udf89 </p>"},{"location":"development/setup/","title":"Development Setup","text":"<p>This guide will help you set up a development environment for contributing to BenjaMMMin.</p>"},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<p>Before setting up the development environment, ensure you have the following installed:</p> <ul> <li>Python 3.11+: The minimum supported Python version</li> <li>Poetry 2.x.x: For dependency management and packaging</li> <li>Git: For version control</li> </ul>"},{"location":"development/setup/#quick-setup","title":"Quick Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>poetry install\n</code></pre></p> </li> <li> <p>Verify the installation:    <pre><code>poetry run benjammmin --help\n</code></pre></p> </li> </ol>"},{"location":"development/setup/#development-environment","title":"Development Environment","text":""},{"location":"development/setup/#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<p>Poetry automatically creates and manages a virtual environment for the project:</p> <pre><code># Activate the virtual environment\npoetry shell\n\n# Run commands within the environment\npoetry run python -m pytest\n\n# Install additional development dependencies\npoetry add --group dev package-name\n</code></pre>"},{"location":"development/setup/#using-pip-alternative","title":"Using pip (Alternative)","text":"<p>If you prefer using pip directly:</p> <pre><code># Create a virtual environment\npython -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -r requirements-dev.txt  # If available\n</code></pre>"},{"location":"development/setup/#project-structure","title":"Project Structure","text":"<pre><code>mmm-eval/\n\u251c\u2500\u2500 mmm_eval/              # Main package\n\u2502   \u251c\u2500\u2500 adapters/          # Framework adapters\n\u2502   \u251c\u2500\u2500 cli/               # Command-line interface\n\u2502   \u251c\u2500\u2500 core/              # Core evaluation logic\n\u2502   \u251c\u2500\u2500 data/              # Data handling and validation\n\u2502   \u2514\u2500\u2500 metrics/           # Evaluation metrics\n\u251c\u2500\u2500 tests/                 # Test suite\n\u251c\u2500\u2500 docs/                  # Documentation\n\u251c\u2500\u2500 pyproject.toml         # Project configuration\n\u2514\u2500\u2500 README.md             # Project overview\n</code></pre>"},{"location":"development/setup/#code-quality-tools","title":"Code Quality Tools","text":"<p>The project uses several tools to maintain code quality:</p>"},{"location":"development/setup/#code-formatting","title":"Code Formatting","text":"<p>The project uses Black for code formatting:</p> <pre><code># Format all Python files\npoetry run black .\n\n# Check formatting without making changes\npoetry run black --check .\n</code></pre>"},{"location":"development/setup/#import-sorting","title":"Import Sorting","text":"<p>isort is used to organize imports:</p> <pre><code># Sort imports\npoetry run isort .\n\n# Check import sorting\npoetry run isort --check-only .\n</code></pre>"},{"location":"development/setup/#linting","title":"Linting","text":"<p>Multiple linters are configured:</p> <pre><code># Run all linters\npoetry run ruff check .\npoetry run flake8 .\n\n# Auto-fix issues where possible\npoetry run ruff check --fix .\n</code></pre>"},{"location":"development/setup/#type-checking","title":"Type Checking","text":"<p>Pyright is used for static type checking:</p> <pre><code># Run type checker\npoetry run pyright\n</code></pre>"},{"location":"development/setup/#optional-pre-commit-hooks","title":"(Optional) Pre-commit Hooks","text":"<p>Install pre-commit hooks to automatically format and lint your code:</p> <pre><code># Install pre-commit\npoetry add --group dev pre-commit\n\n# Install the git hook scripts\npre-commit install\n\n# Run against all files\npre-commit run --all-files\n</code></pre>"},{"location":"development/setup/#running-tests","title":"Running Tests","text":""},{"location":"development/setup/#unit-tests","title":"Unit Tests","text":"<pre><code># Run all tests\npoetry run pytest\n\n# Run tests with coverage\npoetry run pytest --cov=mmm_eval\n\n# Run tests in parallel\npoetry run pytest -n auto\n\n# Run specific test file\npoetry run pytest tests/test_core.py\n</code></pre>"},{"location":"development/setup/#test-coverage","title":"Test Coverage","text":"<pre><code># Generate coverage report\npoetry run pytest --cov=mmm_eval --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html  # On macOS\n# or\nstart htmlcov/index.html  # On Windows\n</code></pre>"},{"location":"development/setup/#documentation","title":"Documentation","text":""},{"location":"development/setup/#building-documentation","title":"Building Documentation","text":"<pre><code># Install documentation dependencies\npoetry install --with docs\n\n# Build documentation\npoetry run mkdocs build\n\n# Serve documentation locally\npoetry run mkdocs serve\n</code></pre> <p>The documentation will be available at <code>http://localhost:8000</code>.</p>"},{"location":"development/setup/#api-documentation","title":"API Documentation","text":"<p>API documentation is automatically generated from docstrings using <code>mkdocstrings</code>. To update the API docs:</p> <ol> <li>Ensure your code has proper docstrings</li> <li>Build the documentation: <code>poetry run mkdocs build</code></li> <li>The API reference will be updated automatically</li> </ol>"},{"location":"development/setup/#ide-configuration","title":"IDE Configuration","text":""},{"location":"development/setup/#vs-code","title":"VS Code","text":"<p>Recommended VS Code extensions:</p> <ul> <li>Python</li> <li>Pylance</li> <li>Black Formatter</li> <li>isort</li> <li>Ruff</li> </ul> <p>Add to your <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"python.defaultInterpreterPath\": \"./venv/bin/python\",\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.ruffEnabled\": true,\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.organizeImports\": true\n    }\n}\n</code></pre>"},{"location":"development/setup/#pycharm","title":"PyCharm","text":"<ol> <li>Open the project in PyCharm</li> <li>Configure the Python interpreter to use the Poetry virtual environment</li> <li>Enable auto-import organization</li> <li>Configure Black as the code formatter</li> </ol>"},{"location":"development/setup/#common-issues","title":"Common Issues","text":""},{"location":"development/setup/#poetry-installation-issues","title":"Poetry Installation Issues","text":"<p>If you encounter issues with Poetry:</p> <pre><code># Update Poetry to the latest version\npoetry self update\n\n# Clear Poetry cache\npoetry cache clear --all pypi\n</code></pre>"},{"location":"development/setup/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>If you encounter dependency conflicts:</p> <pre><code># Update all dependencies\npoetry update\n\n# Remove and reinstall dependencies\npoetry lock --no-update\npoetry install\n</code></pre>"},{"location":"development/setup/#test-failures","title":"Test Failures","text":"<p>If tests are failing:</p> <ol> <li>Ensure you're using the correct Python version (3.11+)</li> <li>Check that all dependencies are installed: <code>poetry install</code></li> <li>Run tests with verbose output: <code>poetry run pytest -v</code></li> <li>Check for any environment-specific issues</li> </ol>"},{"location":"development/setup/#next-steps","title":"Next Steps","text":"<p>Once your development environment is set up:</p> <ol> <li>Read the Contributing Guide for guidelines</li> <li>Check the Testing Guide for testing practices</li> <li>Look at existing issues and pull requests</li> <li>Start with a small contribution to get familiar with the codebase</li> </ol>"},{"location":"development/setup/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during setup:</p> <ol> <li>Check the GitHub Issues</li> <li>Review the Contributing Guide</li> <li>Ask questions in the project discussions </li> </ol>"},{"location":"development/testing/","title":"Testing","text":"<p>This guide covers testing practices and procedures for the BenjaMMMin project.</p>"},{"location":"development/testing/#testing-philosophy","title":"Testing Philosophy","text":"<p>We follow these testing principles:</p> <ul> <li>Comprehensive coverage: Aim for high test coverage across all modules</li> <li>Fast feedback: Tests should run quickly to enable rapid development</li> <li>Reliable: Tests should be deterministic and not flaky</li> <li>Maintainable: Tests should be easy to understand and modify</li> <li>Realistic: Tests should reflect real-world usage patterns</li> </ul>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<p>The test suite is organized as follows:</p> <pre><code>tests/\n\u251c\u2500\u2500 test_adapters/               # Framework adapter tests\n\u251c\u2500\u2500 test_configs/                # Configuration object tests\n\u251c\u2500\u2500 test_core/                   # Core functionality tests\n\u251c\u2500\u2500 test_data/                   # Data handling tests\n\u2514\u2500\u2500 test_validation_tests/       # Metrics calculation tests\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npoetry run pytest\n\n# Run tests with verbose output\npoetry run pytest -v\n\n# Run tests with coverage\npoetry run pytest --cov=mmm_eval\n\n# Run tests in parallel\npoetry run pytest -n auto\n</code></pre>"},{"location":"development/testing/#running-specific-test-categories","title":"Running Specific Test Categories","text":"<pre><code># Run only unit tests\npoetry run pytest tests/unit/\n\n# Run only integration tests\npoetry run pytest tests/integration/\n\n# Run tests for a specific module\npoetry run pytest tests/unit/test_core/\n\n# Run tests matching a pattern\npoetry run pytest -k \"test_accuracy\"\n</code></pre>"},{"location":"development/testing/#running-tests-with-markers","title":"Running Tests with Markers","text":"<pre><code># Run integration tests only\npoetry run pytest -m integration\n\n# Run slow tests only\npoetry run pytest -m slow\n\n# Skip slow tests\npoetry run pytest -m \"not slow\"\n</code></pre>"},{"location":"development/testing/#test-types","title":"Test Types","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Unit tests verify individual functions and classes in isolation. They should:</p> <ul> <li>Test one specific behavior or functionality</li> <li>Use mocks for external dependencies</li> <li>Be fast and deterministic</li> <li>Have clear, descriptive names</li> </ul> <p>Example unit test:</p> <pre><code>def test_calculate_mape_returns_correct_value():\n    \"\"\"Test that MAPE calculation returns expected results.\"\"\"\n    actual = [100, 200, 300]\n    predicted = [110, 190, 310]\n\n    result = calculate_mape(actual, predicted)\n\n    expected = 10.0  # 10% average error\n    assert result == pytest.approx(expected, rel=1e-2)\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests verify that multiple components work together correctly. They:</p> <ul> <li>Test the interaction between different modules</li> <li>Use real data and minimal mocking</li> <li>May take longer to run</li> <li>Are marked with the <code>@pytest.mark.integration</code> decorator</li> </ul> <p>Example integration test:</p> <pre><code>@pytest.mark.integration\ndef test_pymc_marketing_evaluation_workflow():\n    \"\"\"Test complete PyMC Marketing evaluation workflow.\"\"\"\n    # Setup test data\n    data = load_test_data()\n\n    # Run evaluation\n    result = evaluate_framework(\n        data=data,\n        framework=\"pymc-marketing\",\n        config=test_config\n    )\n\n    # Verify results\n    assert result.accuracy &gt; 0.8\n    assert result.cross_validation_score &gt; 0.7\n    assert result.refresh_stability &gt; 0.6\n</code></pre>"},{"location":"development/testing/#test-data-and-fixtures","title":"Test Data and Fixtures","text":""},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<p>Pytest fixtures provide reusable test data and setup:</p> <pre><code>@pytest.fixture\ndef sample_mmm_data():\n    \"\"\"Provide sample MMM data for testing.\"\"\"\n    return pd.DataFrame({\n        'date': pd.date_range('2023-01-01', periods=100),\n        'sales': np.random.normal(1000, 100, 100),\n        'tv_spend': np.random.uniform(0, 1000, 100),\n        'radio_spend': np.random.uniform(0, 500, 100),\n        'digital_spend': np.random.uniform(0, 800, 100)\n    })\n\ndef test_data_validation(sample_mmm_data):\n    \"\"\"Test data validation with sample data.\"\"\"\n    validator = DataValidator()\n    result = validator.validate(sample_mmm_data)\n    assert result.is_valid\n</code></pre>"},{"location":"development/testing/#test-data-management","title":"Test Data Management","text":"<ul> <li>Use realistic but synthetic data</li> <li>Keep test data files small and focused</li> <li>Document the structure and purpose of test data</li> </ul>"},{"location":"development/testing/#mocking-and-stubbing","title":"Mocking and Stubbing","text":""},{"location":"development/testing/#when-to-mock","title":"When to Mock","text":"<p>Mock external dependencies to:</p> <ul> <li>Speed up tests</li> <li>Avoid network calls</li> <li>Control test conditions</li> <li>Test error scenarios</li> </ul>"},{"location":"development/testing/#mocking-examples","title":"Mocking Examples","text":"<pre><code>from unittest.mock import Mock, patch\n\ndef test_api_call_with_mock():\n    \"\"\"Test API call with mocked response.\"\"\"\n    with patch('requests.get') as mock_get:\n        mock_get.return_value.json.return_value = {'status': 'success'}\n        mock_get.return_value.status_code = 200\n\n        result = fetch_data_from_api()\n\n        assert result['status'] == 'success'\n        mock_get.assert_called_once()\n</code></pre>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":""},{"location":"development/testing/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Minimum coverage: 80% for all modules</li> <li>Target coverage: 90% for critical modules</li> <li>Critical modules: Core evaluation logic, data validation, metrics calculation</li> </ul>"},{"location":"development/testing/#coverage-reports","title":"Coverage Reports","text":"<pre><code># Generate HTML coverage report\npoetry run pytest --cov=mmm_eval --cov-report=html\n\n# Generate XML coverage report (for CI)\npoetry run pytest --cov=mmm_eval --cov-report=xml\n\n# View coverage summary\npoetry run pytest --cov=mmm_eval --cov-report=term-missing\n</code></pre>"},{"location":"development/testing/#coverage-configuration","title":"Coverage Configuration","text":"<p>Configure coverage in <code>pyproject.toml</code>:</p> <pre><code>[tool.coverage.run]\nsource = [\"mmm_eval\"]\nomit = [\n    \"*/tests/*\",\n    \"*/test_*\",\n    \"*/__pycache__/*\"\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"if self.debug:\",\n    \"if settings.DEBUG\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n    \"if 0:\",\n    \"if __name__ == .__main__.:\",\n    \"class .*\\\\bProtocol\\\\):\",\n    \"@(abc\\\\.)?abstractmethod\"\n]\n</code></pre>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"development/testing/#benchmark-tests","title":"Benchmark Tests","text":"<p>For performance-critical code, use benchmark tests:</p> <pre><code>def test_mape_calculation_performance(benchmark):\n    \"\"\"Benchmark MAPE calculation performance.\"\"\"\n    actual = np.random.normal(1000, 100, 10000)\n    predicted = np.random.normal(1000, 100, 10000)\n\n    result = benchmark(lambda: calculate_mape(actual, predicted))\n\n    assert result &gt; 0\n</code></pre>"},{"location":"development/testing/#memory-usage-tests","title":"Memory Usage Tests","text":"<p>Monitor memory usage in tests:</p> <pre><code>import psutil\nimport os\n\ndef test_memory_usage():\n    \"\"\"Test that operations don't use excessive memory.\"\"\"\n    process = psutil.Process(os.getpid())\n    initial_memory = process.memory_info().rss\n\n    # Run memory-intensive operation\n    result = process_large_dataset()\n\n    final_memory = process.memory_info().rss\n    memory_increase = final_memory - initial_memory\n\n    # Memory increase should be reasonable (&lt; 100MB)\n    assert memory_increase &lt; 100 * 1024 * 1024\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions","title":"GitHub Actions","text":"<p>Tests run automatically on:</p> <ul> <li>Every pull request</li> <li>Every push to main branch</li> <li>Scheduled runs (nightly)</li> </ul>"},{"location":"development/testing/#ci-configuration","title":"CI Configuration","text":"<p>The CI pipeline includes:</p> <ol> <li>Linting: Code style and quality checks</li> <li>Type checking: Static type analysis</li> <li>Unit tests: Fast feedback on basic functionality</li> <li>Integration tests: Verify component interactions</li> <li>Coverage reporting: Track test coverage trends</li> </ol>"},{"location":"development/testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks to catch issues early:</p> <pre><code># Install pre-commit\npoetry add --group dev pre-commit\n\n# Install hooks\npre-commit install\n\n# Run all hooks\npre-commit run --all-files\n</code></pre>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":""},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code># Run with maximum verbosity\npoetry run pytest -vvv\n\n# Show local variables on failures\npoetry run pytest -l\n\n# Stop on first failure\npoetry run pytest -x\n</code></pre>"},{"location":"development/testing/#debugging-with-pdb","title":"Debugging with pdb","text":"<pre><code>def test_debug_example():\n    \"\"\"Example of using pdb for debugging.\"\"\"\n    import pdb; pdb.set_trace()  # Breakpoint\n    result = complex_calculation()\n    assert result &gt; 0\n</code></pre>"},{"location":"development/testing/#test-isolation","title":"Test Isolation","text":"<p>Ensure tests don't interfere with each other:</p> <pre><code>@pytest.fixture(autouse=True)\ndef reset_global_state():\n    \"\"\"Reset global state before each test.\"\"\"\n    # Setup\n    yield\n    # Teardown\n    cleanup_global_state()\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":""},{"location":"development/testing/#test-naming","title":"Test Naming","text":"<ul> <li>Use descriptive test names that explain the expected behavior</li> <li>Follow the pattern: <code>test_[function]_[scenario]_[expected_result]</code></li> <li>Include edge cases and error conditions</li> </ul>"},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<ul> <li>Group related tests in classes</li> <li>Use fixtures for common setup</li> <li>Keep tests focused and single-purpose</li> </ul>"},{"location":"development/testing/#assertions","title":"Assertions","text":"<ul> <li>Use specific assertions (<code>assert result == expected</code>)</li> <li>Avoid complex logic in assertions</li> <li>Use appropriate assertion methods (<code>assertIn</code>, <code>assertRaises</code>, etc.)</li> </ul>"},{"location":"development/testing/#test-data","title":"Test Data","text":"<ul> <li>Use realistic test data</li> <li>Avoid hardcoded magic numbers</li> <li>Document test data assumptions</li> </ul>"},{"location":"development/testing/#documentation","title":"Documentation","text":"<ul> <li>Write clear docstrings for test functions</li> <li>Explain complex test scenarios</li> <li>Document test data sources and assumptions</li> </ul>"},{"location":"development/testing/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"development/testing/#flaky-tests","title":"Flaky Tests","text":"<p>Avoid flaky tests by:</p> <ul> <li>Not relying on timing or external services</li> <li>Using deterministic random seeds</li> <li>Properly mocking external dependencies</li> <li>Avoiding shared state between tests</li> </ul>"},{"location":"development/testing/#slow-tests","title":"Slow Tests","text":"<p>Keep tests fast by:</p> <ul> <li>Using appropriate mocks</li> <li>Minimizing I/O operations</li> <li>Using efficient test data</li> <li>Running tests in parallel when possible</li> </ul>"},{"location":"development/testing/#over-mocking","title":"Over-Mocking","text":"<p>Don't over-mock:</p> <ul> <li>Test the actual behavior, not the implementation</li> <li>Mock only external dependencies</li> <li>Use real objects when possible</li> </ul>"},{"location":"development/testing/#getting-help","title":"Getting Help","text":"<p>If you encounter testing issues:</p> <ol> <li>Check the pytest documentation</li> <li>Review existing tests for examples</li> <li>Ask questions in project discussions</li> <li>Consult the Contributing Guide </li> </ol>"},{"location":"examples/basic-usage/","title":"Basic Usage Examples","text":"<p>This guide provides practical examples of how to use mmm-eval for different scenarios.</p> <p>For the sake of simplicity, all examples below exhibit use of PyMC-marketing, but the takeaways still apply when using other frameworks. For examples of how to configure a Meridian model, see the example notebook in the <code>examples/</code> directory.</p>"},{"location":"examples/basic-usage/#example-1-basic-evaluation","title":"Example 1: Basic Evaluation","text":"<pre><code>mmm-eval \\\n  --input-data-path marketing_data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path results/\n</code></pre> <p>This assumes your data has standard column names and a valid configuration file.</p>"},{"location":"examples/basic-usage/#example-2-custom-configuration","title":"Example 2: Custom Configuration","text":"<p>Create a configuration file <code>config.json</code>:</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\", \"holiday\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\",\n    \"yearly_seasonality\": 2\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.9,\n    \"draws\": 100,\n    \"tune\": 50,\n    \"chains\": 2,\n    \"random_seed\": 42\n  },\n  \"revenue_column\": \"revenue\",\n  \"response_column\": \"sales\"\n}\n</code></pre> <p>Run the evaluation:</p> <pre><code>mmm-eval \\\n  --input-data-path marketing_data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path ./results/\n</code></pre>"},{"location":"examples/basic-usage/#example-3-specific-tests-only","title":"Example 3: Specific Tests Only","text":"<p>Run only certain validation tests:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path ./results/ \\\n  --test-names accuracy cross_validation\n</code></pre> <p>Available tests: - <code>accuracy</code> - Model accuracy using holdout validation - <code>cross_validation</code> - Time series cross-validation - <code>refresh_stability</code> - Model stability over time - <code>perturbation</code> - Sensitivity to data changes</p>"},{"location":"examples/basic-usage/#example-4-verbose-output","title":"Example 4: Verbose Output","text":"<p>Get detailed information during execution:</p> <pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path ./results/ \\\n  --verbose\n</code></pre>"},{"location":"examples/basic-usage/#example-5-advanced-configuration","title":"Example 5: Advanced Configuration","text":"<p>For more complex models, use an advanced configuration:</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\", \"radio_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\", \"holiday\", \"competitor_promo\"],\n    \"adstock\": \"WeibullAdstock(l_max=6)\",\n    \"saturation\": \"HillSaturation()\",\n    \"yearly_seasonality\": 4,\n    \"time_varying_intercept\": true,\n    \"time_varying_media\": false\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.95,\n    \"draws\": 2000,\n    \"tune\": 1000,\n    \"chains\": 4,\n    \"random_seed\": 123,\n    \"progress_bar\": true,\n    \"return_inferencedata\": true\n  },\n  \"revenue_column\": \"revenue\",\n  \"response_column\": \"sales\"\n}\n</code></pre> <p>Run with verbose output:</p> <pre><code>mmm-eval \\\n  --input-data-path marketing_data.csv \\\n  --framework pymc-marketing \\\n  --config-path advanced_config.json \\\n  --test-names accuracy cross_validation refresh_stability perturbation \\\n  --output-path ./advanced_results/ \\\n  --verbose\n</code></pre>"},{"location":"examples/basic-usage/#data-format-examples","title":"Data Format Examples","text":""},{"location":"examples/basic-usage/#basic-csv-structure","title":"Basic CSV Structure","text":"<pre><code>date_week,quantity,revenue,channel_1,channel_2,price,event_1,event_2\n2023-01-01,1000,7000,5000,2000,10.99,0,0\n2023-01-08,1200,8000,5500,2200,10.99,0,0\n2023-01-15,1100,7500,5200,2100,11.99,1,0\n2023-01-22,1300,9000,6000,2400,11.99,0,1\n2023-01-29,1400,9500,6500,2600,12.99,0,0\n2023-02-05,1500,10000,7000,2800,12.99,0,0\n2023-02-12,1600,10500,7500,3000,13.99,1,0\n2023-02-19,1700,11000,8000,3200,13.99,0,1\n2023-02-26,1800,11500,8500,3400,14.99,0,0\n2023-03-05,1900,12000,9000,3600,14.99,0,0\n</code></pre>"},{"location":"examples/basic-usage/#with-more-channels","title":"With More Channels","text":"<pre><code>date_week,sales,revenue,tv_spend,digital_spend,print_spend,radio_spend,price,seasonality,holiday\n2023-01-01,1000,7000,5000,2000,1000,500,10.99,0.8,0\n2023-01-08,1200,8000,5500,2200,1100,550,10.99,0.9,0\n2023-01-15,1100,7500,5200,2100,1050,520,11.99,0.7,1\n2023-01-22,1300,9000,6000,2400,1200,600,11.99,0.8,0\n2023-01-29,1400,9500,6500,2600,1300,650,12.99,0.9,0\n</code></pre>"},{"location":"examples/basic-usage/#expected-output","title":"Expected Output","text":"<p>After running an evaluation, you'll find a CSV file in your output directory:</p> <pre><code>results/\n\u2514\u2500\u2500 mmm_eval_pymc-marketing_20241201_143022.csv\n</code></pre>"},{"location":"examples/basic-usage/#sample-results","title":"Sample Results","text":"<pre><code>test_name,metric_name,metric_value,metric_pass\naccuracy,mape,0.15,True\naccuracy,r_squared,0.85,True\ncross_validation,mape,0.18,True\ncross_validation,r_squared,0.82,True\nrefresh_stability,mean_percentage_change_for_each_channel:channel_1,0.05,True\nrefresh_stability,mean_percentage_change_for_each_channel:channel_2,0.03,True\nrefresh_stability,std_percentage_change_for_each_channel:channel_1,0.02,True\nrefresh_stability,std_percentage_change_for_each_channel:channel_2,0.01,True\nperturbation,percentage_change_for_each_channel:channel_1,0.02,True\nperturbation,percentage_change_for_each_channel:channel_2,0.01,True\n</code></pre>"},{"location":"examples/basic-usage/#performance-examples","title":"Performance Examples","text":""},{"location":"examples/basic-usage/#quick-testing","title":"Quick Testing","text":"<p>For development and testing:</p> <pre><code>mmm-eval \\\n  --input-data-path small_test_data.csv \\\n  --framework pymc-marketing \\\n  --config-path test_config.json \\\n  --output-path ./test_results/ \\\n  --test-names accuracy\n</code></pre> <p>Use minimal sampling parameters in your config: <pre><code>{\n  \"fit_config\": {\n    \"draws\": 50,\n    \"tune\": 25,\n    \"chains\": 1\n  }\n}\n</code></pre></p>"},{"location":"examples/basic-usage/#production-evaluation","title":"Production Evaluation","text":"<p>For production use:</p> <pre><code>mmm-eval \\\n  --input-data-path production_data.csv \\\n  --framework pymc-marketing \\\n  --config-path production_config.json \\\n  --output-path ./production_results/ \\\n  --test-names accuracy cross_validation refresh_stability perturbation \\\n  --verbose\n</code></pre> <p>Use robust sampling parameters: <pre><code>{\n  \"fit_config\": {\n    \"draws\": 2000,\n    \"tune\": 1000,\n    \"chains\": 4,\n    \"target_accept\": 0.95\n  }\n}\n</code></pre></p>"},{"location":"examples/basic-usage/#troubleshooting-examples","title":"Troubleshooting Examples","text":""},{"location":"examples/basic-usage/#missing-configuration-file","title":"Missing Configuration File","text":"<p>If you get a configuration error:</p> <pre><code># Create a basic config file\ncat &gt; config.json &lt;&lt; 'EOF'\n{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"channel_1\", \"channel_2\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\"\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.9,\n    \"draws\": 100,\n    \"tune\": 50,\n    \"chains\": 2,\n    \"random_seed\": 42\n  },\n  \"revenue_column\": \"revenue\"\n}\nEOF\n\n# Run evaluation\nmmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path ./results/\n</code></pre>"},{"location":"examples/basic-usage/#data-format-issues","title":"Data Format Issues","text":"<p>If you get data format errors, check your CSV structure:</p> <pre><code># Check your data format\nhead -5 data.csv\n\n# Ensure required columns exist\npython -c \"\nimport pandas as pd\ndf = pd.read_csv('data.csv')\nprint('Columns:', df.columns.tolist())\nprint('Shape:', df.shape)\nprint('Date range:', df['date_week'].min(), 'to', df['date_week'].max())\n\"\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data for different data structures</li> <li>Explore Configuration for advanced settings</li> <li>Check the CLI Reference for all available options </li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>mmm-eval uses framework-specific configurations to control model parameters, fitting settings, and data mappings. This guide explains how to create and use configurations for the PyMC-Marketing framework.</p>"},{"location":"getting-started/configuration/#creating-configurations","title":"Creating Configurations","text":"<p>There are two ways to create a configuration (config):</p> <ol> <li>From a model object (preferred).</li> </ol> <p>Info</p> <p>See the Meridian notebook in the <code>examples/</code> directory for a walkthrough of how to do this with Meridian.</p> <p><pre><code>from pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\nfrom mmm_eval.configs import PyMCConfig\n\n# Create a PyMC model\nmodel = MMM(\n    date_column=\"date_week\",\n    channel_columns=[\"channel_1\", \"channel_2\"],\n    adstock=GeometricAdstock(l_max=4),\n    saturation=LogisticSaturation(),\n    yearly_seasonality=2\n)\n\n# Create configuration\nconfig = PyMCConfig.from_model_object(\n    model_object=model,\n    fit_kwargs={\"target_accept\": 0.9, \"draws\": 100, \"chains\": 2},\n    revenue_column=\"revenue\",\n    response_column=\"quantity\"\n)\n\n# Save to JSON if you want\nconfig.save_model_object_to_json(\"./\", \"my_config\")\n</code></pre> 2. Manually, in a JSON file. </p> <p>Meridian Config Creation</p> <p>Option 2 is not supported for Meridian models due to the complexity of the model objects.</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"channel_1\", \"channel_2\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\",\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.9,\n    \"chains\": 2,\n  },\n  \"revenue_column\": \"revenue\",\n}\n</code></pre> <p>If you want to run mmm-eval from the CLI, you will need to save the config saved to a JSON file. We recommend Option 1 above to avoid any errors related to improper stringifiation of the model object in the manual approach.</p>"},{"location":"getting-started/configuration/#using-a-configuration","title":"Using a Configuration","text":"<p>If you have the config created from the model object, you can pass that directly to the evaluation suite (see Quick Start).</p> <p>Alternately, if you have the config saved to a JSON, you can pass the filepath via the CLI.</p> <pre><code>benjammmin --input-data-path data.csv --framework pymc-marketing --config-path config.json --output-path results/\n</code></pre> <p>If you have a saved config and you're in a notebook, you can load the config from the path, then run the evaluation. <pre><code>new_config = PyMCConfig.load_model_config_from_json(\"path/to/config.json\")\nresults = run_evaluation(new_config, ...)\n</code></pre></p>"},{"location":"getting-started/configuration/#pymc-marketing-configuration-structure","title":"PyMC-Marketing Configuration Structure","text":"<p>A PyMC-Marketing configuration file has the following structure:</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"channel_1\", \"channel_2\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\",\n  },\n  \"fit_config\": {\n    // Optional \n  },\n  \"revenue_column\": \"revenue\",\n}\n</code></pre>"},{"location":"getting-started/configuration/#model-configuration-pymc_model_config","title":"Model Configuration (pymc_model_config)","text":"<p>The <code>pymc_model_config</code> section defines the PyMC model structure and parameters:</p>"},{"location":"getting-started/configuration/#required-fields","title":"Required Fields","text":""},{"location":"getting-started/configuration/#date_column","title":"date_column","text":"<p>The column name containing the date/time variable.</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#channel_columns","title":"channel_columns","text":"<p>List of column names for marketing channels (media spend).</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#adstock","title":"adstock","text":"<p>The adstock transformation to apply to media channels. Must be a valid member of the AdstockTransformation class.</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"adstock\": \"GeometricAdstock(l_max=4)\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#saturation","title":"saturation","text":"<p>The saturation transformation to apply to media channels. Must be valid member of the SaturationTransformation class.</p> <pre><code>{\n  \"pymc_model_config\": {\n    \"saturation\": \"LogisticSaturation()\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#optional-fields","title":"Optional Fields","text":"<p>The set of optional inputs matches the optional inputs to the MMM class in PyMC.</p>"},{"location":"getting-started/configuration/#fit-configuration-fit_config","title":"Fit Configuration (fit_config)","text":"<p>The <code>fit_config</code> section defines the MCMC sampling parameters. These parameters will be passed to <code>.fit()</code>. Note, we do not require the <code>X</code> and <code>y</code> inputs as we derive those from the data you provide. Therefore, all parameters in this config are optional.</p>"},{"location":"getting-started/configuration/#sampling-parameters","title":"Sampling Parameters","text":""},{"location":"getting-started/configuration/#draws","title":"draws","text":"<p>Number of posterior samples to draw.</p> <pre><code>{\n  \"fit_config\": {\n    \"draws\": 100\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#tune","title":"tune","text":"<p>Number of tuning (warm-up) steps.</p> <pre><code>{\n  \"fit_config\": {\n    \"tune\": 50\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#chains","title":"chains","text":"<p>Number of MCMC chains to run.</p> <pre><code>{\n  \"fit_config\": {\n    \"chains\": 2\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#target_accept","title":"target_accept","text":"<p>Target acceptance rate for the sampler (0.0 to 1.0).</p> <pre><code>{\n  \"fit_config\": {\n    \"target_accept\": 0.9\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#random_seed","title":"random_seed","text":"<p>Random seed for reproducibility.</p> <pre><code>{\n  \"fit_config\": {\n    \"random_seed\": 42\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#progress_bar","title":"progress_bar","text":"<p>Whether to display the progress bar (default: false).</p> <pre><code>{\n  \"fit_config\": {\n    \"progress_bar\": false\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#return_inferencedata","title":"return_inferencedata","text":"<p>Whether to return arviz.InferenceData (default: false).</p> <pre><code>{\n  \"fit_config\": {\n    \"return_inferencedata\": true\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#data-mapping-configuration","title":"Data Mapping Configuration","text":""},{"location":"getting-started/configuration/#revenue_column","title":"revenue_column","text":"<p>The column name containing revenue data for ROI calculations.</p> <pre><code>{\n  \"revenue_column\": \"revenue\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#response_column","title":"response_column","text":"<p>The column name containing the target variable (optional, defaults to revenue if not provided).</p> <pre><code>{\n  \"response_column\": \"quantity\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#meridian-configuration-structure","title":"Meridian Configuration Structure","text":"<p>A Meridian configuration file has the following structure:</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"date_column\": \"date_week\",\n    \"media_channels\": [\"tv\", \"digital\", \"print\"],\n    \"channel_spend_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"],\n    \"response_column\": \"sales\"\n  },\n  \"model_spec_config\": {\n    \"prior\": \"PriorDistribution(...)\",\n    \"media_effects_dist\": \"log_normal\",\n    \"max_lag\": 8\n  },\n  \"sample_posterior_config\": {\n    \"n_chains\": 4,\n    \"n_keep\": 1000\n  },\n  \"revenue_column\": \"revenue\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#input-data-builder-configuration-input_data_builder_config","title":"Input Data Builder Configuration (input_data_builder_config)","text":"<p>The <code>input_data_builder_config</code> section defines how to construct the data object for the Meridian model:</p>"},{"location":"getting-started/configuration/#required-fields_1","title":"Required Fields","text":""},{"location":"getting-started/configuration/#date_column_1","title":"date_column","text":"<p>The column name containing the date/time variable.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"date_column\": \"date_week\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#media_channels","title":"media_channels","text":"<p>List of media channel names (not column names).</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"media_channels\": [\"tv\", \"digital\", \"print\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#channel_spend_columns","title":"channel_spend_columns","text":"<p>List of column names for media channel spend variables.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"channel_spend_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#response_column_1","title":"response_column","text":"<p>The column name containing the target variable.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"response_column\": \"sales\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#optional-fields_1","title":"Optional Fields","text":""},{"location":"getting-started/configuration/#channel_impressions_columns","title":"channel_impressions_columns","text":"<p>List of column names for media channel impressions variables.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"channel_impressions_columns\": [\"tv_impressions\", \"digital_impressions\", \"print_impressions\"]\n  }\n}\n</code></pre> <p>Mutual Exclusion</p> <p><code>channel_impressions_columns</code> and <code>channel_reach_columns</code> cannot both be provided.</p>"},{"location":"getting-started/configuration/#channel_reach_columns","title":"channel_reach_columns","text":"<p>List of column names for media channel reach variables.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"channel_reach_columns\": [\"tv_reach\", \"digital_reach\", \"print_reach\"]\n  }\n}\n</code></pre> <p>Paired Fields</p> <p><code>channel_reach_columns</code> and <code>channel_frequency_columns</code> must be provided together (both or neither).</p>"},{"location":"getting-started/configuration/#channel_frequency_columns","title":"channel_frequency_columns","text":"<p>List of column names for media channel frequency variables.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"channel_frequency_columns\": [\"tv_frequency\", \"digital_frequency\", \"print_frequency\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#organic_media_columns","title":"organic_media_columns","text":"<p>List of column names for organic media variables.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"organic_media_columns\": [\"organic_search\", \"direct_traffic\"]\n  }\n}\n</code></pre> <p>Paired Fields</p> <p><code>organic_media_columns</code> and <code>organic_media_channels</code> must be provided together (both or neither).</p>"},{"location":"getting-started/configuration/#organic_media_channels","title":"organic_media_channels","text":"<p>List of channel names for organic media variables.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"organic_media_channels\": [\"organic_search\", \"direct_traffic\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#non_media_treatment_columns","title":"non_media_treatment_columns","text":"<p>List of column names for non-media treatment variables.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"non_media_treatment_columns\": [\"price_promotion\", \"seasonal_event\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#control_columns","title":"control_columns","text":"<p>List of column names for control variables.</p> <pre><code>{\n  \"input_data_builder_config\": {\n    \"control_columns\": [\"price\", \"competitor_activity\", \"economic_indicator\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#model-specification-configuration-model_spec_config","title":"Model Specification Configuration (model_spec_config)","text":"<p>The <code>model_spec_config</code> section defines the Meridian model structure and parameters:</p>"},{"location":"getting-started/configuration/#required-fields_2","title":"Required Fields","text":""},{"location":"getting-started/configuration/#prior","title":"prior","text":"<p>The prior distribution configuration. This is a complex object that defines the prior distributions for model parameters.</p> <pre><code>{\n  \"model_spec_config\": {\n    \"prior\": \"PriorDistribution(roi_m=LogNormal(0.2, 0.9), ...)\"\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#optional-fields_2","title":"Optional Fields","text":"<p>The set of optional fields matches the optional inputs to the Meridian ModelSpec object.</p>"},{"location":"getting-started/configuration/#sample-posterior-configuration-sample_posterior_config","title":"Sample Posterior Configuration (sample_posterior_config)","text":"<p>The <code>sample_posterior_config</code> section defines the MCMC sampling parameters for Meridian models:</p>"},{"location":"getting-started/configuration/#sampling-parameters_1","title":"Sampling Parameters","text":""},{"location":"getting-started/configuration/#n_chains","title":"n_chains","text":"<p>Number of MCMC chains to run (default: 4).</p> <pre><code>{\n  \"sample_posterior_config\": {\n    \"n_chains\": 4\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#n_adapt","title":"n_adapt","text":"<p>Number of adaptation steps (default: 500).</p> <pre><code>{\n  \"sample_posterior_config\": {\n    \"n_adapt\": 500\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#n_burnin","title":"n_burnin","text":"<p>Number of burn-in steps (default: 500).</p> <pre><code>{\n  \"sample_posterior_config\": {\n    \"n_burnin\": 500\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#n_keep","title":"n_keep","text":"<p>Number of posterior samples to keep (default: 1000).</p> <pre><code>{\n  \"sample_posterior_config\": {\n    \"n_keep\": 1000\n  }\n}\n</code></pre> <p>Other optional arguments match the API of the Meridian.sample_posterior function</p>"},{"location":"getting-started/configuration/#example-configurations-pymc-marketing","title":"Example Configurations (PyMC-marketing)","text":""},{"location":"getting-started/configuration/#minimal-configuration","title":"Minimal Configuration","text":"<pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date\",\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\"],\n    \"adstock\": \"GeometricAdstock(l_max=4)\",\n    \"saturation\": \"LogisticSaturation()\"\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.9,\n    \"draws\": 100,\n    \"tune\": 50,\n    \"chains\": 2,\n    \"random_seed\": 42\n  },\n  \"revenue_column\": \"revenue\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>{\n  \"pymc_model_config\": {\n    \"date_column\": \"date_week\",\n    \"channel_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\", \"radio_spend\"],\n    \"control_columns\": [\"price\", \"seasonality\", \"holiday\", \"competitor_promo\"],\n    \"adstock\": \"WeibullAdstock(l_max=6)\",\n    \"saturation\": \"HillSaturation()\",\n    \"yearly_seasonality\": 4,\n    \"time_varying_intercept\": true,\n    \"time_varying_media\": false\n  },\n  \"fit_config\": {\n    \"target_accept\": 0.95,\n    \"draws\": 2000,\n    \"tune\": 1000,\n    \"chains\": 4,\n    \"random_seed\": 123,\n    \"progress_bar\": true,\n    \"return_inferencedata\": true\n  },\n  \"revenue_column\": \"revenue\",\n  \"response_column\": \"sales\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#example-configurations-meridian","title":"Example Configurations (Meridian)","text":"<p>Warning</p> <p>As stated above, we strongly suggest you create the Meridian config objects programatically to ensure the config can be de-seralized properly.</p>"},{"location":"getting-started/configuration/#minimal-configuration_1","title":"Minimal Configuration","text":"<pre><code>{\n  \"input_data_builder_config\": {\n    \"date_column\": \"date_week\",\n    \"media_channels\": [\"tv\", \"digital\"],\n    \"channel_spend_columns\": [\"tv_spend\", \"digital_spend\"],\n    \"response_column\": \"sales\"\n  },\n  \"model_spec_config\": {\n    \"prior\": \"PriorDistribution(roi_m=LogNormal(0.2, 0.9))\",\n    \"media_effects_dist\": \"log_normal\",\n    \"max_lag\": 8\n  },\n  \"sample_posterior_config\": {\n    \"n_chains\": 4,\n    \"n_keep\": 1000,\n    \"n_adapt\": 500,\n    \"n_burnin\": 500\n  },\n  \"revenue_column\": \"revenue\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration_1","title":"Advanced Configuration","text":"<pre><code>{\n  \"input_data_builder_config\": {\n    \"date_column\": \"date_week\",\n    \"media_channels\": [\"tv\", \"digital\", \"print\", \"radio\"],\n    \"channel_spend_columns\": [\"tv_spend\", \"digital_spend\", \"print_spend\", \"radio_spend\"],\n    \"channel_reach_columns\": [\"tv_reach\", \"digital_reach\", \"print_reach\", \"radio_reach\"],\n    \"channel_frequency_columns\": [\"tv_frequency\", \"digital_frequency\", \"print_frequency\", \"radio_frequency\"],\n    \"organic_media_columns\": [\"organic_search\", \"direct_traffic\"],\n    \"organic_media_channels\": [\"organic_search\", \"direct_traffic\"],\n    \"non_media_treatment_columns\": [\"price_promotion\", \"seasonal_event\"],\n    \"control_columns\": [\"price\", \"competitor_activity\", \"economic_indicator\"],\n    \"response_column\": \"sales\"\n  },\n  \"model_spec_config\": {\n    \"prior\": \"PriorDistribution(roi_m=LogNormal(0.2, 0.9), roi_rf=LogNormal(0.1, 0.5))\",\n    \"media_effects_dist\": \"log_normal\",\n    \"hill_before_adstock\": true,\n    \"max_lag\": 12,\n    \"unique_sigma_for_each_geo\": true,\n    \"media_prior_type\": \"roi\",\n    \"rf_prior_type\": \"roi\",\n    \"paid_media_prior_type\": \"roi\",\n    \"roi_calibration_period\": [0.8, 1.2],\n    \"rf_roi_calibration_period\": [0.9, 1.1],\n    \"organic_media_prior_type\": \"contribution\",\n    \"organic_rf_prior_type\": \"contribution\",\n    \"non_media_treatments_prior_type\": \"contribution\",\n    \"non_media_baseline_values\": [1.0, 0.5],\n    \"knots\": 10,\n    \"baseline_geo\": \"US\",\n    \"holdout_id\": [1, 2, 3],\n    \"control_population_scaling_id\": [1, 2],\n    \"non_media_population_scaling_id\": [1, 2]\n  },\n  \"sample_posterior_config\": {\n    \"n_chains\": 4,\n    \"n_adapt\": 1000,\n    \"n_burnin\": 1000,\n    \"n_keep\": 2000,\n    \"max_tree_depth\": 12,\n    \"max_energy_diff\": 1000.0,\n    \"unrolled_leapfrog_steps\": 2,\n    \"parallel_iterations\": 20,\n    \"seed\": 123\n  },\n  \"revenue_column\": \"revenue\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#configuration-best-practices","title":"Configuration Best Practices","text":""},{"location":"getting-started/configuration/#sampling-parameters_2","title":"Sampling Parameters","text":"<ul> <li>More draws: Use 1000+ draws for production models</li> <li>Multiple chains: Use 2-4 chains for reliable convergence</li> <li>Adequate tuning: Set <code>tune</code> to 50-100% of draws for complex models</li> <li>Acceptance rate: Target 0.9-0.95 for optimal sampling efficiency</li> </ul>"},{"location":"getting-started/configuration/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Data size: Larger datasets require more sampling iterations</li> <li>Model complexity: More parameters increase computation time</li> <li>Hardware: More CPU cores can speed up multi-chain sampling</li> </ul>"},{"location":"getting-started/configuration/#configuration-validation","title":"Configuration Validation","text":"<p>mmm-eval validates your configuration file and will raise errors for:</p> <ul> <li>Missing required fields</li> <li>Invalid field types</li> <li>Unsupported adstock or saturation functions</li> <li>Invalid parameter ranges</li> </ul>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data for different data structures</li> <li>Explore Examples for configuration use cases</li> <li>Check the CLI Reference for command-line options </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install mmm-eval on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>mmm-eval requires Python 3.11 or higher. Make sure you have Python installed on your system.</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<p>The recommended way to install mmm-eval is using Poetry:</p> <pre><code># Install Poetry if you haven't already\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Add mmm-eval to your project\npoetry add git+https://github.com/Mutiny-Group/mmm-eval.git\n</code></pre>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install git+https://github.com/Mutiny-Group/mmm-eval.git\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\n\n# Install using Poetry\npoetry install\n\n# Or install using pip\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>mmm-eval has the following key dependencies:</p>"},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>PyMC-Marketing: For PyMC-Marketing framework support</li> <li>Meridian: For Google Meridian framework support</li> <li>Pandas: For data manipulation</li> <li>NumPy: For numerical computations</li> <li>SciPy: For scientific computations</li> </ul>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<ul> <li>Matplotlib: For plotting (optional)</li> <li>Seaborn: For enhanced plotting (optional)</li> </ul>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify that mmm-eval is working correctly:</p> <pre><code># Check if mmm-eval is installed\npython -c \"import mmm_eval; print(f'mmm-eval version: {mmm_eval.__version__}')\"\n\n# Test the CLI\nbenjammmin --help\n</code></pre>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>If you want to contribute to mmm-eval, set up a development environment:</p>"},{"location":"getting-started/installation/#using-poetry","title":"Using Poetry","text":"<pre><code># Clone the repository\ngit clone https://github.com/Mutiny-Group/mmm-eval.git\ncd mmm-eval\n\n# Install dependencies\npoetry install\n\n# Activate the environment\npoetry shell\n\n# Test the installation\npoetry run benjammmin --help\n</code></pre>"},{"location":"getting-started/installation/#using-virtual-environment","title":"Using Virtual Environment","text":"<pre><code># Create virtual environment\npython -m venv mmm-eval-env\nsource mmm-eval-env/bin/activate  # On Windows: mmm-eval-env\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Test the installation\nbenjammmin --help\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li>Import errors: Make sure you're using Python 3.11+</li> <li>CLI not found: Ensure the package is installed correctly</li> <li>Dependency conflicts: Use a virtual environment</li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter installation issues:</p> <ul> <li>Check the GitHub Issues</li> <li>Join our Discussions</li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have mmm-eval installed, check out the Quick Start guide to begin using it. </p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you get started with mmm-eval quickly by walking through an example notebook. You'll learn the evaluation workflow and how to interpret the results. To see how to run mmm-eval from the command line, check out CLI.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ol> <li>mmm-eval installed - See Installation if you haven't installed it yet</li> <li>Your MMM data - A CSV or Parquet file with your marketing mix model data</li> <li>A supported framework - Currently, Meridian and PyMC-Marketing are supported</li> </ol>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quick-start/#prepare-your-data","title":"Prepare Your Data","text":"<p>Your data should contain the following columns (see Data Requirements for more)</p> <ul> <li>Date/time period</li> <li>Revenue variable (for calculating ROI)</li> <li>Marketing spend $ by channel (e.g., TV, digital, print)</li> <li><code>OPTIONAL</code> Response variable (e.g., sales, conversions, units). If not provided, revenue will be used as the target.</li> <li><code>OPTIONAL</code> Control variables (e.g., price, seasonality)</li> </ul> <p>Meridian Data Inputs</p> <p>Meridian supports multiple types of controls and media treatments, as well as a geography field. For details, see the Meridian notebook in the <code>examples/</code> directory for a full walkthrough and their documentation here.</p> <p>Example data structure:</p> <pre><code>date_week,quantity,revenue,TV,radio,price,event_1,event_2\n2023-01-01,1000,7000,5000,2000,10.99,0,0\n2023-01-08,1200,8000,5500,2200,10.99,0,0\n2023-01-15,1100,7500,5200,2100,11.99,1,0\n2023-01-22,1300,9000,6000,2400,11.99,0,1\n2023-01-29,1400,9500,6500,2600,12.99,0,0\n</code></pre>"},{"location":"getting-started/quick-start/#evaluating-a-pymc-mmm-follow-along-in-examplespymc_evalipynb","title":"Evaluating a PyMC MMM (follow along in <code>examples/pymc_eval.ipynb</code>)","text":"<p>First, load your data <pre><code>data = pd.read_csv(\"data/example_data.csv\")\n</code></pre></p> <p>and fit a PyMC-Marketing MMM <pre><code>X = data.drop(columns=[\"revenue\",\"quantity\"])\ny = data[\"quantity\"]\n\nmodel = MMM(\n    date_column=\"date_week\" ,\n    channel_columns=[\"TV\",\"radio\"],\n    adstock=GeometricAdstock(l_max=4),\n    saturation=LogisticSaturation()\n)\n\nmodel.fit(X=X, y=y, chains=4, target_accept=0.85)\n</code></pre></p>"},{"location":"getting-started/quick-start/#now-we-evaluate-just-create-a-config","title":"Now we evaluate! Just create a config","text":"<pre><code>fit_kwargs = { \n    \"chains\": 4,\n    \"target_accept\": 0.85,\n}\n\nconfig = PyMCConfig.from_model_object(base_model, fit_kwargs=fit_kwargs, response_column=\"quantity\", revenue_column=\"revenue\")\n\n# Save this for later if you want to run from CLI!\nconfig.save_model_object_to_json(save_path=\"data/\", file_name=\"saved_config\")\n</code></pre> <p>And we can run the evaluation suite, which returns a dataframe. <pre><code>result = run_evaluation(framework=\"pymc-marketing\", config=config, data=data)\n</code></pre></p>"},{"location":"getting-started/quick-start/#evaluating-a-meridian-mmm-follow-along-in-examplesmeridian_evalipynb","title":"Evaluating a Meridian MMM (follow along in <code>examples/meridian_eval.ipynb</code>)","text":""},{"location":"getting-started/quick-start/#load-data-and-convert-to-meridian-data-object","title":"Load data and convert to Meridian data object","text":"<pre><code>df = pd.read_excel(\n    'https://github.com/google/meridian/raw/main/meridian/data/simulated_data/xlsx/geo_media.xlsx',\n    engine='openpyxl',\n)builder = (\n    data_builder.DataFrameInputDataBuilder(kpi_type='non_revenue')\n        .with_kpi(df, kpi_col=\"conversions\")\n        .with_revenue_per_kpi(df, revenue_per_kpi_col=\"revenue_per_conversion\")\n        .with_population(df)\n        .with_controls(df, control_cols=[\"GQV\", \"Discount\", \"Competitor_Sales\"])\n)\nchannels = [\"Channel0\", \"Channel1\", \"Channel2\", \"Channel3\", \"Channel4\", \"Channel5\"]\nbuilder = builder.with_media(\n    df,\n    media_cols=[f\"{channel}_impression\" for channel in channels],\n    media_spend_cols=[f\"{channel}_spend\" for channel in channels],\n    media_channels=channels,\n)\n\ndata = builder.build()\n</code></pre>"},{"location":"getting-started/quick-start/#define-a-meridian-mmm","title":"Define a Meridian MMM","text":"<pre><code>roi_mu = 0.2     # Mu for ROI prior for each media channel.\nroi_sigma = 0.9  # Sigma for ROI prior for each media channel.\nprior = prior_distribution.PriorDistribution(\n    roi_m=tfp.distributions.LogNormal(roi_mu, roi_sigma, name=constants.ROI_M)\n)\nmodel_spec = spec.ModelSpec(prior=prior)\n# sampling from the posterior is not required prior to evaluation\nmmm = model.Meridian(input_data=data, model_spec=model_spec)\n</code></pre>"},{"location":"getting-started/quick-start/#set-up-meridian-input-data-builder-config","title":"Set up Meridian input data builder config","text":"<pre><code>data_preproc = df.copy()\ndata_preproc[\"revenue\"] = data_preproc[\"revenue_per_conversion\"]*data_preproc[\"conversions\"]\n\nchannels = [\"Channel0\", \"Channel1\", \"Channel2\", \"Channel3\", \"Channel4\", \"Channel5\"]\ninput_data_builder_config = MeridianInputDataBuilderSchema(\n    date_column=\"time\",\n    media_channels=channels,\n    channel_spend_columns=[f\"{col}_spend\" for col in channels],\n    channel_impressions_columns=[f\"{col}_impression\" for col in channels],\n    response_column=\"conversions\",\n    control_columns=[\"GQV\", \"Competitor_Sales\", \"Discount\"],\n)\n</code></pre>"},{"location":"getting-started/quick-start/#create-a-config-and-evaluate","title":"Create a config and evaluate","text":"<pre><code># specify a larger number of samples if you want quality results\nsample_posterior_kwargs = dict(n_chains=1, n_adapt=10, n_burnin=10, n_keep=10)\nconfig = MeridianConfig.from_model_object(mmm, input_data_builder_config=input_data_builder_config,\n                                          revenue_column=\"revenue\", sample_posterior_kwargs=sample_posterior_kwargs)\n# Run the evaluation suite!\nresult = run_evaluation(framework=\"meridian\", config=config, data=data_preproc)\n</code></pre>"},{"location":"getting-started/quick-start/#done","title":"\u2728 Done \u2728","text":""},{"location":"getting-started/quick-start/#whats-in-result","title":"What's in <code>result</code>?","text":"<p>The evaluation suite runs 4 tests, each of which answers a distinct question about the quality of your model: </p> <ul> <li>Accuracy Test: \"How well does my model predict on unseen data?\"</li> <li>Cross-Validation: \"How consistent are my model's predictions across different splits of unseen data?\"</li> <li>Refresh Stability: \"How much does marketing attribution change when I add new data to my model?\"</li> <li>Perturbation: \"How sensitive is my model is to noise in the marketing inputs?\"</li> </ul> <p>Details on the implementation of the tests can be found in Tests. For each test, we compute multiple metrics to give as much insight into the test result as possible. These can be viewed in detail in Metrics. For example:</p> <ul> <li> <p>MAPE (Mean Absolute Percentage Error) <code>MAPE = (100 / n) * \u03a3 |(y_i - \u0177_i) / y_i|</code></p> </li> <li> <p>R-squared (Coefficient of Determination) <code>R\u00b2 = 1 - (\u03a3 (y_i - \u0177_i)^2) / (\u03a3 (y_i - \u0233)^2)</code></p> </li> </ul> <p>If we look at the evaluation output <code>display(results)</code>, we'll see something like the following:</p> test_name metric_name metric_value metric_pass accuracy mape 0.121 False accuracy r_squared -0.547 False cross_validation mean_mape 0.084 False cross_validation std_mape 0.058 False cross_validation mean_r_squared -7.141 False cross_validation std_r_squared 9.686 False refresh_stability mean_percentage_change_for_each_channel:TV 0.021 False refresh_stability mean_percentage_change_for_each_channel:radio 0.369 False refresh_stability std_percentage_change_for_each_channel:TV 0.021 False refresh_stability std_percentage_change_for_each_channel:radio 0.397 False perturbation percentage_change_for_each_channel:TV 0.005 False perturbation percentage_change_for_each_channel:radio 0.112 False <p>Notice that our model is failing every test. Seems we have some work to do!</p>"},{"location":"getting-started/quick-start/#changing-the-thresholds","title":"Changing the Thresholds","text":"<p>Default metric thresholds in <code>mmm_eval/metrics/threshold_constants.py</code> can be overwritten in-place to change the pass/fail cutoff for each metric.</p>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#common-issues","title":"Common Issues","text":"<ol> <li>Data Format: Ensure your data has the required columns and proper format</li> <li>Configuration Errors: Check that your config file is valid JSON</li> <li>Memory Issues: For large datasets, try reducing the number of chains or draws</li> </ol>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ul> <li>Check the CLI Reference for all available options</li> <li>Look at Examples for similar use cases</li> <li>Join our Discussions for community support</li> </ul>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first evaluation:</p> <ol> <li>Explore the User Guide for detailed CLI options</li> <li>Check out Examples for more complex scenarios</li> <li>Learn about Data for different data structures</li> <li>Review Configuration for advanced settings</li> </ol>"},{"location":"user-guide/cli/","title":"CLI Reference","text":"<p>mmm-eval provides a command-line interface (CLI) for running MMM evaluations. This guide covers all available options and usage patterns.</p>"},{"location":"user-guide/cli/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/cli/#command-structure","title":"Command Structure","text":"<pre><code>mmm-eval [OPTIONS] --input-data-path PATH --framework FRAMEWORK --config-path PATH --output-path PATH\n</code></pre>"},{"location":"user-guide/cli/#required-arguments","title":"Required Arguments","text":"<ul> <li><code>--input-data-path</code>: Path to your input data file (CSV or Parquet)</li> <li><code>--framework</code>: MMM framework to use (<code>pymc-marketing</code> or <code>meridian</code>)</li> <li><code>--config-path</code>: Path to your configuration file (JSON)</li> <li><code>--output-path</code>: Directory where results will be saved</li> </ul>"},{"location":"user-guide/cli/#example-commands","title":"Example Commands","text":"<pre><code># Basic evaluation\nmmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json --output-path results/\n\n# Run specific tests only\nmmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json --output-path results/ --test-names accuracy cross_validation\n</code></pre>"},{"location":"user-guide/cli/#command-options","title":"Command Options","text":""},{"location":"user-guide/cli/#input-options","title":"Input Options","text":"<ul> <li><code>--input-data-path</code>: Path to input data file (required)</li> <li><code>--config-path</code>: Path to configuration file (required)</li> <li><code>--framework</code>: MMM framework to use (required)</li> <li>Options: <code>pymc-marketing</code>, <code>meridian</code></li> </ul>"},{"location":"user-guide/cli/#output-options","title":"Output Options","text":"<ul> <li><code>--output-path</code>: Directory for output files (required)</li> <li><code>--test-names</code>: Specific tests to run (optional)</li> <li>Options: <code>accuracy</code>, <code>cross_validation</code>, <code>refresh_stability</code>, <code>performance</code></li> <li>Default: All tests</li> </ul>"},{"location":"user-guide/cli/#advanced-options","title":"Advanced Options","text":"<ul> <li><code>--random-seed</code>: Random seed for reproducibility (optional)</li> <li><code>--verbose</code>: Enable verbose output (optional)</li> <li><code>--help</code>: Show help message</li> </ul>"},{"location":"user-guide/cli/#examples","title":"Examples","text":""},{"location":"user-guide/cli/#basic-evaluation","title":"Basic Evaluation","text":"<pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path results/\n</code></pre>"},{"location":"user-guide/cli/#run-specific-tests","title":"Run Specific Tests","text":"<pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path results/ \\\n  --test-names accuracy cross_validation\n</code></pre>"},{"location":"user-guide/cli/#with-custom-random-seed","title":"With Custom Random Seed","text":"<pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path results/ \\\n  --random-seed 42\n</code></pre>"},{"location":"user-guide/cli/#verbose-output","title":"Verbose Output","text":"<pre><code>mmm-eval \\\n  --input-data-path data.csv \\\n  --framework pymc-marketing \\\n  --config-path config.json \\\n  --output-path results/ \\\n  --verbose\n</code></pre>"},{"location":"user-guide/cli/#output-structure","title":"Output Structure","text":"<p>The CLI creates the following output structure:</p> <pre><code>results/\n\u251c\u2500\u2500 accuracy/\n\u2502   \u251c\u2500\u2500 metrics.json\n\u2502   \u2514\u2500\u2500 plots/\n\u251c\u2500\u2500 cross_validation/\n\u2502   \u251c\u2500\u2500 metrics.json\n\u2502   \u2514\u2500\u2500 plots/\n\u251c\u2500\u2500 refresh_stability/\n\u2502   \u251c\u2500\u2500 metrics.json\n\u2502   \u2514\u2500\u2500 plots/\n\u251c\u2500\u2500 performance/\n\u2502   \u251c\u2500\u2500 metrics.json\n\u2502   \u2514\u2500\u2500 plots/\n\u2514\u2500\u2500 summary.json\n</code></pre>"},{"location":"user-guide/cli/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/cli/#common-errors","title":"Common Errors","text":"<ol> <li>File not found: Ensure all file paths are correct</li> <li>Invalid configuration: Check your config file format</li> <li>Framework not supported: Verify framework name</li> <li>Data format issues: Check data requirements</li> </ol>"},{"location":"user-guide/cli/#getting-help","title":"Getting Help","text":"<pre><code># Show help\nmmm-eval --help\n\n# Show version\nmmm-eval --version\n</code></pre>"},{"location":"user-guide/cli/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data Requirements for input format</li> <li>Check Configuration for setup</li> <li>Explore Examples for use cases </li> </ul>"},{"location":"user-guide/data/","title":"Data","text":"<p>mmm-eval expects your data to be in a specific format. This guide explains the required structure and provides examples for preparing your marketing mix modeling data.</p>"},{"location":"user-guide/data/#data-requirements","title":"Data Requirements","text":""},{"location":"user-guide/data/#quantity-requirements","title":"Quantity Requirements","text":"<ul> <li>Observations: At least 40 data points required, but 100+ is highly recommended</li> <li>Frequency: Consistent frequency with no gaps (weekly is recommended)</li> <li>Media channels: At least 2-3 channels for meaningful analysis</li> <li>Revenue data: Required for ROI calculations</li> </ul>"},{"location":"user-guide/data/#quality-requirements","title":"Quality Requirements","text":""},{"location":"user-guide/data/#completeness","title":"Completeness","text":"<ul> <li>No missing values in required columns</li> <li>Complete time series (no gaps in dates)</li> <li>Non-negative values for spend columns</li> </ul>"},{"location":"user-guide/data/#consistency","title":"Consistency","text":"<ul> <li>Same units throughout (e.g., all spend in same currency)</li> <li>Consistent date format</li> <li>Consistent naming conventions</li> </ul>"},{"location":"user-guide/data/#reasonableness","title":"Reasonableness","text":"<ul> <li>Values within expected ranges</li> <li>No obvious outliers or errors</li> <li>Logical relationships between variables</li> </ul>"},{"location":"user-guide/data/#data-format","title":"Data Format","text":"<p>mmm-eval accepts CSV and Parquet files with the following structure:</p>"},{"location":"user-guide/data/#required-columns","title":"Required Columns","text":"<ul> <li>Date column: Time series data with consistent date format</li> <li>Target column: The variable you want to predict (e.g., sales, conversions)</li> <li>Revenue column: Revenue data for calculating ROI and efficiency metrics</li> <li>Media columns: Marketing channel spend or activity data</li> </ul>"},{"location":"user-guide/data/#optional-columns","title":"Optional columns","text":"<ul> <li>Control columns: Additional variables that may affect the target</li> </ul> <p>Meridian Data Inputs</p> <p>In addition to the above, Meridian supports multiple types of controls and media treatments, as well as a geography field. For details, see the Meridian notebook in the <code>examples/</code> directory for a full walkthrough, as well as their documentation here.</p>"},{"location":"user-guide/data/#column-types","title":"Column Types","text":""},{"location":"user-guide/data/#date-column","title":"Date Column","text":"<ul> <li>Purpose: Identifies the time period for each observation</li> <li>Format: Date in a consistent format (e.g., YYYY-MM-DD, MM/DD/YYYY)</li> <li>Requirements: </li> <li>Must be in chronological order</li> <li>No missing dates in the series</li> <li>Consistent format throughout</li> </ul>"},{"location":"user-guide/data/#target-column","title":"Target Column","text":"<ul> <li>Purpose: The dependent variable you want to model (e.g., sales, conversions)</li> <li>Format: Numeric values</li> <li>Requirements:</li> <li>No missing values</li> <li>Positive values (for most use cases)</li> <li>Reasonable scale for your business</li> </ul>"},{"location":"user-guide/data/#revenue-column","title":"Revenue Column","text":"<ul> <li>Purpose: Revenue data for calculating ROI and efficiency metrics</li> <li>Format: Numeric values</li> <li>Requirements:</li> <li>No missing values</li> <li>Positive values</li> <li>Same time period as target column</li> <li>Used for ROI calculations and efficiency analysis</li> </ul>"},{"location":"user-guide/data/#media-columns","title":"Media Columns","text":"<ul> <li>Purpose: Marketing channel spend or activity data</li> <li>Format: Numeric values</li> <li>Requirements:</li> <li>No missing values (use 0 for periods with no spend)</li> <li>Non-negative values</li> <li>Consistent units (e.g., all in dollars, all in thousands)</li> </ul>"},{"location":"user-guide/data/#control-columns","title":"Control Columns","text":"<ul> <li>Purpose: Additional variables that may affect the target</li> <li>Format: Numeric or categorical values</li> <li>Examples: Price, seasonality indicators, holiday flags, competitor activity</li> </ul>"},{"location":"user-guide/data/#example-data-structure","title":"Example Data Structure","text":""},{"location":"user-guide/data/#basic-example","title":"Basic Example","text":"<pre><code>date,sales,revenue,tv_spend,digital_spend,print_spend\n2023-01-01,1000,7000,5000,2000,1000\n2023-01-02,1200,8000,5500,2200,1100\n2023-01-03,1100,7500,5200,2100,1050\n2023-01-04,1300,9000,6000,2400,1200\n2023-01-05,1400,9500,6500,2600,1300\n</code></pre>"},{"location":"user-guide/data/#advanced-example-with-controls","title":"Advanced Example with Controls","text":"<pre><code>date,sales,revenue,tv_spend,digital_spend,print_spend,price,seasonality,holiday\n2023-01-01,1000,7000,5000,2000,1000,10.99,0.8,0\n2023-01-02,1200,8000,5500,2200,1100,10.99,0.9,0\n2023-01-03,1100,7500,5200,2100,1050,11.99,0.7,1\n2023-01-04,1300,9000,6000,2400,1200,11.99,0.8,0\n2023-01-05,1400,9500,6500,2600,1300,12.99,0.9,0\n</code></pre>"},{"location":"user-guide/data/#real-world-example","title":"Real-World Example","text":"<pre><code>date,sales,revenue,tv_spend,digital_spend,social_spend,search_spend,email_spend,price,holiday_flag,competitor_promo\n2023-01-01,1250,8750,15000,8000,3000,5000,2000,12.99,0,0\n2023-01-02,1320,9240,16000,8500,3200,5200,2100,12.99,0,0\n2023-01-03,1180,8260,14000,7500,2800,4800,1900,13.99,1,1\n2023-01-04,1450,10150,18000,9500,3800,6000,2400,13.99,0,0\n2023-01-05,1520,10640,19000,10000,4000,6300,2500,14.99,0,0\n</code></pre>"},{"location":"user-guide/data/#date-formats","title":"Date Formats","text":"<p>Please provide your dates in ISO format, e.g. <code>YYYY-MM-DD</code>.</p>"},{"location":"user-guide/data/#validation","title":"Validation","text":"<p>mmm-eval performs several validation checks:</p>"},{"location":"user-guide/data/#automatic-validation","title":"Automatic Validation","text":"<ol> <li>Missing values: Checks for missing data in required columns</li> <li>Date consistency: Ensures dates are in chronological order</li> <li>Data types: Verifies numeric columns contain valid numbers</li> <li>Value ranges: Checks for negative values in spend columns</li> <li>Revenue consistency: Ensures revenue data is available and positive</li> </ol>"},{"location":"user-guide/data/#data-preparation-tips","title":"Data Preparation Tips","text":""},{"location":"user-guide/data/#before-running-mmm-eval","title":"Before Running mmm-eval","text":""},{"location":"user-guide/data/#clean-your-data","title":"Clean your data","text":"<ul> <li>Remove any test or dummy data</li> <li>Handle missing values appropriately</li> <li>Check for and remove outliers if necessary</li> </ul>"},{"location":"user-guide/data/#standardize-formats","title":"Standardize formats","text":"<ul> <li>Ensure consistent date format</li> <li>Use consistent units (e.g., thousands of dollars)</li> <li>Standardize column names</li> </ul>"},{"location":"user-guide/data/#validate-relationships","title":"Validate relationships","text":"<ul> <li>Check that spend and sales have logical relationships</li> <li>Verify that revenue data is consistent with sales</li> <li>Ensure control variables make sense</li> </ul>"},{"location":"user-guide/data/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/data/#missing-values","title":"Missing Values","text":"<pre><code># Problem: Missing values in media columns\ndate,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,\n2023-01-02,1200,8000,,2000\n\n# Solution: Fill with zeros or appropriate values\ndate,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,0\n2023-01-02,1200,8000,0,2000\n</code></pre>"},{"location":"user-guide/data/#inconsistent-date-formats","title":"Inconsistent Date Formats","text":"<pre><code># Problem: Mixed date formats\ndate,sales,revenue\n01/01/2023,1000,7000\n2023-01-02,1200,8000\n\n# Solution: Standardize to one format\ndate,sales,revenue\n2023-01-01,1000,7000\n2023-01-02,1200,8000\n</code></pre>"},{"location":"user-guide/data/#missing-revenue-data","title":"Missing Revenue Data","text":"<pre><code># Problem: No revenue column\ndate,sales,tv_spend,digital_spend\n2023-01-01,1000,5000,2000\n2023-01-02,1200,5500,2200\n\n# Solution: Add revenue column\ndate,sales,revenue,tv_spend,digital_spend\n2023-01-01,1000,7000,5000,2000\n2023-01-02,1200,8000,5500,2200\n</code></pre>"},{"location":"user-guide/data/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/data/#data-collection","title":"Data Collection","text":"<ul> <li>Consistent timing: Collect data at the same time each period</li> <li>Complete coverage: Ensure all channels are captured</li> <li>Quality control: Implement data validation at source</li> <li>Documentation: Keep records of any data changes or anomalies</li> </ul>"},{"location":"user-guide/data/#data-analysis","title":"Data Analysis","text":"<ul> <li>Start simple: Begin with basic models before adding complexity</li> <li>Validate assumptions: Check that your data meets model requirements</li> <li>Monitor quality: Regularly review data for issues</li> <li>Document decisions: Keep records of data preparation choices</li> </ul>"},{"location":"user-guide/data/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/data/#common-error-messages","title":"Common Error Messages","text":"<ul> <li>\"Missing required column\": Ensure all required columns are present</li> <li>\"Invalid date format\": Check your date format specification</li> <li>\"Negative values in spend columns\": Replace negative values with zeros</li> <li>\"Missing revenue data\": Add revenue column to your dataset</li> </ul>"},{"location":"user-guide/data/#getting-help","title":"Getting Help","text":"<p>If you encounter data format issues:</p> <ul> <li>Check the CLI Reference for all available options</li> <li>Review the Examples for similar use cases</li> <li>Join our Discussions for community support </li> </ul>"},{"location":"user-guide/frameworks/","title":"Frameworks","text":"<p>mmm-eval supports multiple Marketing Mix Modeling (MMM) frameworks. This guide explains each supported framework and their features.</p>"},{"location":"user-guide/frameworks/#supported-frameworks","title":"Supported Frameworks","text":""},{"location":"user-guide/frameworks/#google-meridian","title":"Google Meridian","text":"<p>Google's Meridian framework provides advanced MMM capabilities with support for reach and frequency data.</p>"},{"location":"user-guide/frameworks/#features","title":"Features","text":"<ul> <li>Reach and Frequency: Support for reach/frequency data in addition to spend</li> <li>Organic Media: Modeling of organic media channels</li> <li>Non-Media Treatments: Support for non-media variables</li> <li>Geographic Modeling: Multi-geography support</li> <li>Advanced Priors: Sophisticated prior distribution system</li> </ul>"},{"location":"user-guide/frameworks/#usage","title":"Usage","text":"<pre><code>mmm-eval --input-data-path data.csv --config-path config.json --framework meridian --output-path results/\n</code></pre>"},{"location":"user-guide/frameworks/#configuration","title":"Configuration","text":"<p>Meridian requires specific configuration for: - Input data builder settings - Model specification parameters - Sample posterior configuration</p> <p>See Configuration for detailed setup.</p>"},{"location":"user-guide/frameworks/#pymc-marketing","title":"PyMC-Marketing","text":"<p>PyMC-Marketing is a Bayesian MMM framework built on PyMC.</p>"},{"location":"user-guide/frameworks/#features_1","title":"Features","text":"<ul> <li>Bayesian Inference: Full posterior distributions</li> <li>Flexible Adstock: Multiple adstock transformation options</li> <li>Saturation Models: Various saturation functions</li> <li>Control Variables: Support for additional regressors</li> <li>Seasonality: Built-in seasonal modeling</li> </ul>"},{"location":"user-guide/frameworks/#usage_1","title":"Usage","text":"<pre><code>mmm-eval --input-data-path data.csv --config-path config.json --framework pymc-marketing --output-path results/\n</code></pre>"},{"location":"user-guide/frameworks/#configuration_1","title":"Configuration","text":"<p>PyMC-Marketing requires configuration for: - Model parameters (adstock, saturation, etc.) - Fit parameters (chains, draws, etc.) - Data mapping</p> <p>See Configuration for detailed setup.</p>"},{"location":"user-guide/frameworks/#framework-comparison","title":"Framework Comparison","text":"Feature Meridian PyMC-Marketing Reach/Frequency \u2705 Supported \u274c Not supported Organic Media \u2705 Supported \u274c Not supported Geographic \u2705 Supported \u274c Not supported Bayesian \u2705 Full posterior \u2705 Full posterior Adstock Built-in Multiple options Saturation Built-in Multiple options Control Variables \u2705 Supported \u2705 Supported Seasonality Built-in Fourier modes"},{"location":"user-guide/frameworks/#choosing-a-framework","title":"Choosing a Framework","text":""},{"location":"user-guide/frameworks/#when-to-use-meridian","title":"When to Use Meridian","text":"<ul> <li>Reach/Frequency Data: If you have reach and frequency data</li> <li>Organic Media: If modeling organic channels is important</li> <li>Geographic Analysis: If you need multi-geography support</li> <li>Advanced Features: If you need sophisticated prior distributions</li> </ul>"},{"location":"user-guide/frameworks/#when-to-use-pymc-marketing","title":"When to Use PyMC-Marketing","text":"<ul> <li>Standard MMM: For traditional spend-based modeling</li> <li>Flexible Modeling: If you need custom adstock/saturation</li> <li>Bayesian Workflow: If you're familiar with PyMC</li> <li>Open Source: If you prefer open-source frameworks</li> </ul>"},{"location":"user-guide/frameworks/#framework-specific-considerations","title":"Framework-Specific Considerations","text":""},{"location":"user-guide/frameworks/#meridian","title":"Meridian","text":""},{"location":"user-guide/frameworks/#data-requirements","title":"Data Requirements","text":"<ul> <li>Media Channels: Channel names and spend columns</li> <li>Reach/Frequency: Optional reach and frequency columns</li> <li>Organic Media: Optional organic media columns</li> <li>Non-Media: Optional non-media treatment columns</li> <li>Geography: Optional geography column</li> </ul>"},{"location":"user-guide/frameworks/#configuration-complexity","title":"Configuration Complexity","text":"<p>Meridian requires more complex configuration due to: - Prior distribution specification - Multiple data input types - Advanced model parameters</p>"},{"location":"user-guide/frameworks/#pymc-marketing_1","title":"PyMC-Marketing","text":""},{"location":"user-guide/frameworks/#data-requirements_1","title":"Data Requirements","text":"<ul> <li>Media Channels: Spend columns only</li> <li>Control Variables: Optional control columns</li> <li>Date Column: Required for time series</li> <li>Target Column: Response variable</li> </ul>"},{"location":"user-guide/frameworks/#configuration-simplicity","title":"Configuration Simplicity","text":"<p>PyMC-Marketing has simpler configuration: - Standard adstock/saturation choices - Basic fit parameters - Straightforward data mapping</p>"},{"location":"user-guide/frameworks/#migration-between-frameworks","title":"Migration Between Frameworks","text":""},{"location":"user-guide/frameworks/#from-pymc-marketing-to-meridian","title":"From PyMC-Marketing to Meridian","text":"<ol> <li>Add Reach/Frequency Data: If available</li> <li>Configure Priors: Set up PriorDistribution objects</li> <li>Update Configuration: Use Meridian-specific config structure</li> <li>Test Gradually: Start with basic configuration</li> </ol>"},{"location":"user-guide/frameworks/#from-meridian-to-pymc-marketing","title":"From Meridian to PyMC-Marketing","text":"<ol> <li>Remove Reach/Frequency: Use spend data only</li> <li>Simplify Configuration: Use PyMC-Marketing config structure</li> <li>Adjust Parameters: Set adstock and saturation</li> <li>Validate Results: Compare performance</li> </ol>"},{"location":"user-guide/frameworks/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/frameworks/#framework-selection","title":"Framework Selection","text":"<ul> <li>Start Simple: Begin with PyMC-Marketing for basic needs</li> <li>Add Complexity: Move to Meridian for advanced features</li> <li>Data-Driven: Choose based on available data</li> <li>Expertise: Consider team familiarity</li> </ul>"},{"location":"user-guide/frameworks/#configuration-management","title":"Configuration Management","text":"<ul> <li>Version Control: Track configuration changes</li> <li>Documentation: Document framework choices</li> <li>Testing: Test configurations thoroughly</li> <li>Backup: Keep working configurations</li> </ul>"},{"location":"user-guide/frameworks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/frameworks/#common-issues","title":"Common Issues","text":"<ol> <li>Configuration Errors: Check framework-specific requirements</li> <li>Data Format Issues: Verify data matches framework expectations</li> <li>Performance Problems: Adjust framework-specific parameters</li> <li>Memory Issues: Consider data size and model complexity</li> </ol>"},{"location":"user-guide/frameworks/#getting-help","title":"Getting Help","text":"<ul> <li>Check Configuration for setup</li> <li>Review Examples for use cases</li> <li>Join Discussions for support </li> </ul>"},{"location":"user-guide/metrics/","title":"Metrics","text":"<p>Note: To render math equations, enable <code>pymdownx.arithmatex</code> in your <code>mkdocs.yml</code> and include MathJax. See the user guide for details.</p> <p>mmm-eval provides a comprehensive set of metrics to evaluate MMM performance. This guide explains each metric and how to interpret the results.</p>"},{"location":"user-guide/metrics/#overview","title":"Overview","text":"<p>mmm-eval provides a comprehensive set of metrics to evaluate MMM performance. This guide explains each metric and how to interpret the results.</p>"},{"location":"user-guide/metrics/#available-metrics","title":"Available Metrics","text":"<p>mmm-eval calculates several key metrics across different validation tests:</p>"},{"location":"user-guide/metrics/#accuracy-metrics","title":"Accuracy Metrics","text":"<ul> <li>MAPE (Mean Absolute Percentage Error): Average percentage error between predictions and actual values</li> <li>RMSE (Root Mean Square Error): Standard deviation of prediction errors</li> <li>R-squared: Proportion of variance explained by the model</li> <li>MAE (Mean Absolute Error): Average absolute prediction error</li> </ul>"},{"location":"user-guide/metrics/#stability-metrics","title":"Stability Metrics","text":"<ul> <li>Parameter Change: Percentage change in model parameters</li> <li>Channel Stability: Stability of media channel coefficients</li> <li>Intercept Stability: Stability of baseline parameters</li> </ul>"},{"location":"user-guide/metrics/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Training Time: Time required to fit the model</li> <li>Memory Usage: Peak memory consumption during training</li> <li>Prediction Time: Time to generate predictions</li> <li>Convergence: Number of iterations to reach convergence</li> </ul>"},{"location":"user-guide/metrics/#metric-definitions","title":"Metric Definitions","text":""},{"location":"user-guide/metrics/#mape-mean-absolute-percentage-error","title":"MAPE (Mean Absolute Percentage Error)","text":"<pre><code>MAPE = (1/n) * \u03a3 |(y_i - \u0177_i) / y_i|\n</code></pre> <p>Interpretation: - Lower is better: 0% = perfect predictions - Industry benchmark: &lt; 20% is generally good - Scale: Expressed as percentage (0-100%)</p>"},{"location":"user-guide/metrics/#rmse-root-mean-square-error","title":"RMSE (Root Mean Square Error)","text":"<pre><code>RMSE = \u221a(\u03a3(y_i - \u0177_i)\u00b2 / n)\n</code></pre> <p>Interpretation: - Lower is better: 0 = perfect predictions - Units: Same as target variable - Sensitivity: More sensitive to large errors than MAPE</p>"},{"location":"user-guide/metrics/#r-squared-coefficient-of-determination","title":"R-squared (Coefficient of Determination)","text":"<pre><code>R\u00b2 = 1 - (\u03a3(y_i - \u0177_i)\u00b2 / \u03a3(y_i - \u0233)\u00b2)\n</code></pre> <p>Interpretation: - Range: 0 to 1 (higher is better) - Scale: 1 = perfect fit, 0 = no predictive power - Benchmark: &gt; 0.8 is generally good</p>"},{"location":"user-guide/metrics/#mae-mean-absolute-error","title":"MAE (Mean Absolute Error)","text":"<pre><code>MAE = (1/n) * \u03a3 |y_i - \u0177_i|\n</code></pre> <p>Interpretation: - Lower is better: 0 = perfect predictions - Units: Same as target variable - Robustness: Less sensitive to outliers than RMSE</p>"},{"location":"user-guide/metrics/#test-specific-metrics","title":"Test-Specific Metrics","text":""},{"location":"user-guide/metrics/#accuracy-test-metrics","title":"Accuracy Test Metrics","text":"<ul> <li>MAPE: Overall prediction accuracy</li> <li>RMSE: Error magnitude</li> <li>R-squared: Model fit quality</li> <li>MAE: Absolute error magnitude</li> </ul>"},{"location":"user-guide/metrics/#cross-validation-metrics","title":"Cross-Validation Metrics","text":"<ul> <li>Mean MAPE: Average out-of-sample accuracy</li> <li>Std MAPE: Consistency of accuracy across folds</li> <li>Mean R-squared: Average out-of-sample fit</li> <li>Std R-squared: Consistency of fit across folds</li> </ul>"},{"location":"user-guide/metrics/#refresh-stability-metrics","title":"Refresh Stability Metrics","text":"<ul> <li>Mean Percentage Change: Average parameter change</li> <li>Std Percentage Change: Consistency of parameter changes</li> <li>Channel-specific Stability: Stability per media channel</li> </ul>"},{"location":"user-guide/metrics/#performance-metrics_1","title":"Performance Metrics","text":"<ul> <li>Training Time: Model fitting efficiency</li> <li>Memory Usage: Resource utilization</li> <li>Prediction Time: Inference speed</li> <li>Convergence Iterations: Optimization efficiency</li> </ul>"},{"location":"user-guide/metrics/#interpreting-results","title":"Interpreting Results","text":""},{"location":"user-guide/metrics/#good-performance-indicators","title":"Good Performance Indicators","text":"<ul> <li>MAPE &lt; 20%: Good prediction accuracy</li> <li>R-squared &gt; 0.8: Strong model fit</li> <li>Low parameter changes: Stable model</li> <li>Reasonable training time: Efficient computation</li> </ul>"},{"location":"user-guide/metrics/#warning-signs","title":"Warning Signs","text":"<ul> <li>MAPE &gt; 30%: Poor prediction accuracy</li> <li>R-squared &lt; 0.5: Weak model fit</li> <li>High parameter changes: Unstable model</li> <li>Excessive training time: Computational issues</li> </ul>"},{"location":"user-guide/metrics/#thresholds-and-benchmarks","title":"Thresholds and Benchmarks","text":""},{"location":"user-guide/metrics/#default-thresholds","title":"Default Thresholds","text":"<pre><code># Accuracy thresholds\nMAPE_THRESHOLD = 0.20  # 20%\nR_SQUARED_THRESHOLD = 0.80  # 80%\n\n# Stability thresholds\nPARAMETER_CHANGE_THRESHOLD = 0.10  # 10%\n\n# Performance thresholds\nTRAINING_TIME_THRESHOLD = 300  # seconds\nMEMORY_USAGE_THRESHOLD = 8  # GB\n</code></pre>"},{"location":"user-guide/metrics/#industry-benchmarks","title":"Industry Benchmarks","text":"Metric Excellent Good Acceptable Poor MAPE &lt; 10% 10-20% 20-30% &gt; 30% R-squared &gt; 0.9 0.8-0.9 0.6-0.8 &lt; 0.6 Parameter Change &lt; 5% 5-10% 10-20% &gt; 20%"},{"location":"user-guide/metrics/#customizing-metrics","title":"Customizing Metrics","text":""},{"location":"user-guide/metrics/#modifying-thresholds","title":"Modifying Thresholds","text":"<p>You can customize metric thresholds in your configuration:</p> <pre><code>{\n  \"metrics\": {\n    \"mape_threshold\": 0.15,\n    \"r_squared_threshold\": 0.85,\n    \"parameter_change_threshold\": 0.08\n  }\n}\n</code></pre>"},{"location":"user-guide/metrics/#adding-custom-metrics","title":"Adding Custom Metrics","text":"<p>To add custom metrics, extend the metrics module:</p> <pre><code>from mmm_eval.metrics import BaseMetric\n\nclass CustomMetric(BaseMetric):\n    def calculate(self, y_true, y_pred):\n        # Your custom calculation\n        return custom_value\n</code></pre>"},{"location":"user-guide/metrics/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/metrics/#metric-selection","title":"Metric Selection","text":"<ul> <li>Start with MAPE: Most intuitive for business users</li> <li>Include R-squared: Technical measure of fit quality</li> <li>Monitor stability: Critical for production models</li> <li>Track performance: Important for scalability</li> </ul>"},{"location":"user-guide/metrics/#result-analysis","title":"Result Analysis","text":"<ul> <li>Compare across frameworks: Use same metrics for fair comparison</li> <li>Track over time: Monitor performance as data grows</li> <li>Set business thresholds: Align with business requirements</li> <li>Document decisions: Record metric choices and rationale</li> </ul>"},{"location":"user-guide/metrics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/metrics/#common-issues","title":"Common Issues","text":"<ol> <li>Extreme MAPE values: Check for zero or near-zero actual values</li> <li>Negative R-squared: Model performs worse than baseline</li> <li>Inconsistent metrics: Verify data preprocessing</li> <li>Missing metrics: Check test configuration</li> </ol>"},{"location":"user-guide/metrics/#getting-help","title":"Getting Help","text":"<ul> <li>Review Tests for metric context</li> <li>Check Configuration for settings</li> <li>Join Discussions for support </li> </ul>"},{"location":"user-guide/tests/","title":"Tests","text":"<p>mmm-eval provides a comprehensive suite of validation tests to evaluate MMM performance. This guide explains each test and how to interpret the results.</p>"},{"location":"user-guide/tests/#overview","title":"Overview","text":"<p>mmm-eval includes four main types of validation tests:</p> <ol> <li>Accuracy Tests: Measure how well the model fits the data</li> <li>Cross-Validation Tests: Assess model generalization</li> <li>Refresh Stability Tests: Evaluate model stability over time</li> <li>Performance Tests: Measure computational efficiency</li> </ol>"},{"location":"user-guide/tests/#accuracy-tests","title":"Accuracy Tests","text":"<p>Accuracy tests evaluate how well the model fits the training data.</p>"},{"location":"user-guide/tests/#metrics","title":"Metrics","text":"<ul> <li>MAPE (Mean Absolute Percentage Error): Average percentage error</li> <li>RMSE (Root Mean Square Error): Standard deviation of prediction errors</li> <li>R-squared: Proportion of variance explained by the model</li> <li>MAE (Mean Absolute Error): Average absolute prediction error</li> </ul>"},{"location":"user-guide/tests/#interpretation","title":"Interpretation","text":"<ul> <li>Lower MAPE/RMSE/MAE: Better model performance</li> <li>Higher R-squared: Better model fit (0-1 scale)</li> <li>Industry benchmarks: MAPE &lt; 20% is generally good</li> </ul>"},{"location":"user-guide/tests/#example-results","title":"Example Results","text":"<pre><code>{\n  \"accuracy\": {\n    \"mape\": 0.15,\n    \"rmse\": 125.5,\n    \"r_squared\": 0.85,\n    \"mae\": 98.2\n  }\n}\n</code></pre>"},{"location":"user-guide/tests/#cross-validation-tests","title":"Cross-Validation Tests","text":"<p>Cross-validation tests assess how well the model generalizes to unseen data.</p>"},{"location":"user-guide/tests/#process","title":"Process","text":"<ol> <li>Time Series Split: Data is split chronologically</li> <li>Rolling Window: Model is trained on expanding windows</li> <li>Out-of-Sample Prediction: Predictions made on held-out data</li> <li>Performance Metrics: Calculated on out-of-sample predictions</li> </ol>"},{"location":"user-guide/tests/#metrics_1","title":"Metrics","text":"<ul> <li>MAPE: Out-of-sample prediction accuracy</li> <li>RMSE: Out-of-sample error magnitude</li> <li>R-squared: Out-of-sample explanatory power</li> <li>MAE: Out-of-sample absolute error</li> </ul>"},{"location":"user-guide/tests/#interpretation_1","title":"Interpretation","text":"<ul> <li>Consistent performance: Similar in-sample and out-of-sample metrics</li> <li>Overfitting: Much better in-sample than out-of-sample performance</li> <li>Underfitting: Poor performance on both in-sample and out-of-sample data</li> </ul>"},{"location":"user-guide/tests/#refresh-stability-tests","title":"Refresh Stability Tests","text":"<p>Refresh stability tests evaluate how model parameters change when new data is added.</p>"},{"location":"user-guide/tests/#process_1","title":"Process","text":"<ol> <li>Baseline Model: Train on initial dataset</li> <li>Incremental Updates: Add new data periods</li> <li>Parameter Comparison: Compare parameter estimates</li> <li>Stability Metrics: Calculate change percentages</li> </ol>"},{"location":"user-guide/tests/#metrics_2","title":"Metrics","text":"<ul> <li>Mean Percentage Change: Average change in parameter estimates</li> <li>Channel Stability: Stability of media channel parameters</li> <li>Intercept Stability: Stability of baseline parameters</li> <li>Seasonality Stability: Stability of seasonal components</li> </ul>"},{"location":"user-guide/tests/#interpretation_2","title":"Interpretation","text":"<ul> <li>Low percentage changes: Stable model parameters</li> <li>High percentage changes: Unstable model (may need more data)</li> <li>Channel-specific stability: Some channels more stable than others</li> </ul>"},{"location":"user-guide/tests/#performance-tests","title":"Performance Tests","text":"<p>Performance tests measure computational efficiency and resource usage.</p>"},{"location":"user-guide/tests/#metrics_3","title":"Metrics","text":"<ul> <li>Training Time: Time to fit the model</li> <li>Memory Usage: Peak memory consumption</li> <li>Prediction Time: Time to generate predictions</li> <li>Convergence: Number of iterations to convergence</li> </ul>"},{"location":"user-guide/tests/#interpretation_3","title":"Interpretation","text":"<ul> <li>Faster training: More efficient model</li> <li>Lower memory: Better resource utilization</li> <li>Faster predictions: Better for real-time applications</li> <li>Fewer iterations: Better convergence properties</li> </ul>"},{"location":"user-guide/tests/#running-tests","title":"Running Tests","text":""},{"location":"user-guide/tests/#all-tests-default","title":"All Tests (Default)","text":"<pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json --output-path results/\n</code></pre>"},{"location":"user-guide/tests/#specific-tests","title":"Specific Tests","text":"<pre><code>mmm-eval --input-data-path data.csv --framework pymc-marketing --config-path config.json --output-path results/ --test-names accuracy cross_validation\n</code></pre>"},{"location":"user-guide/tests/#available-test-names","title":"Available Test Names","text":"<ul> <li><code>accuracy</code>: Accuracy tests only</li> <li><code>cross_validation</code>: Cross-validation tests only</li> <li><code>refresh_stability</code>: Refresh stability tests only</li> <li><code>performance</code>: Performance tests only</li> </ul>"},{"location":"user-guide/tests/#test-configuration","title":"Test Configuration","text":""},{"location":"user-guide/tests/#accuracy-test-settings","title":"Accuracy Test Settings","text":"<pre><code>{\n  \"accuracy\": {\n    \"train_test_split\": 0.8,\n    \"random_seed\": 42\n  }\n}\n</code></pre>"},{"location":"user-guide/tests/#cross-validation-settings","title":"Cross-Validation Settings","text":"<pre><code>{\n  \"cross_validation\": {\n    \"n_splits\": 5,\n    \"test_size\": 0.2,\n    \"gap\": 0\n  }\n}\n</code></pre>"},{"location":"user-guide/tests/#refresh-stability-settings","title":"Refresh Stability Settings","text":"<pre><code>{\n  \"refresh_stability\": {\n    \"baseline_periods\": 52,\n    \"update_frequency\": 4,\n    \"max_updates\": 12\n  }\n}\n</code></pre>"},{"location":"user-guide/tests/#interpreting-results","title":"Interpreting Results","text":""},{"location":"user-guide/tests/#good-model-indicators","title":"Good Model Indicators","text":"<ul> <li>Accuracy: MAPE &lt; 20%, R-squared &gt; 0.8</li> <li>Cross-Validation: Out-of-sample MAPE similar to in-sample</li> <li>Stability: Parameter changes &lt; 10%</li> <li>Performance: Reasonable training times</li> </ul>"},{"location":"user-guide/tests/#warning-signs","title":"Warning Signs","text":"<ul> <li>Overfitting: Much better in-sample than out-of-sample performance</li> <li>Instability: Large parameter changes with new data</li> <li>Poor Performance: High MAPE or low R-squared</li> <li>Slow Training: Excessive computation time</li> </ul>"},{"location":"user-guide/tests/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/tests/#test-selection","title":"Test Selection","text":"<ul> <li>Start with accuracy: Always run accuracy tests first</li> <li>Add cross-validation: For generalization assessment</li> <li>Include stability: For production models</li> <li>Monitor performance: For computational constraints</li> </ul>"},{"location":"user-guide/tests/#result-analysis","title":"Result Analysis","text":"<ul> <li>Compare frameworks: Run same tests on different frameworks</li> <li>Track over time: Monitor performance as data grows</li> <li>Set thresholds: Define acceptable performance levels</li> <li>Document decisions: Record test choices and rationale</li> </ul>"},{"location":"user-guide/tests/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/tests/#common-issues","title":"Common Issues","text":"<ol> <li>Slow tests: Reduce data size or simplify model</li> <li>Memory errors: Use smaller datasets or more efficient settings</li> <li>Convergence issues: Check model configuration</li> <li>Inconsistent results: Verify random seed settings</li> </ol>"},{"location":"user-guide/tests/#getting-help","title":"Getting Help","text":"<ul> <li>Check Configuration for test settings</li> <li>Review Examples for similar cases</li> <li>Join Discussions for support </li> </ul>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":""},{"location":"user-guide/troubleshooting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/troubleshooting/#common-issues-and-solves","title":"Common Issues and Solves","text":"<ol> <li>Slow performance: Reduce sampling parameters or use fewer tests</li> <li>Memory errors: Reduce data size or model complexity</li> <li>Convergence issues: Adjust sampling parameters or model configuration</li> <li>File permission errors: Check write permissions for output directory</li> </ol>"}]}