"""
Abstract base classes for MMM validation framework.
"""

from abc import ABC, abstractmethod
from typing import Any, Union

from sklearn.model_selection import TimeSeriesSplit, train_test_split
from mmm_eval.adapters.base import BaseAdapter
from mmm_eval.core.constants import (
    ValidationDataframeConstants,
    ValidationTestConstants,
)
from mmm_eval.core.validation_test_results import TestResult
from mmm_eval.core.exceptions import (
    ValidationError, 
    DataValidationError, 
    ModelValidationError, 
    MetricCalculationError, 
    TestExecutionError
)
import pandas as pd
import logging

logger = logging.getLogger(__name__)

from mmm_eval.data.input_dataframe_constants import InputDataframeConstants


class BaseValidationTest(ABC):
    """
    Abstract base class for validation tests.

    All validation tests must inherit from this class and implement
    the required methods to provide a unified testing interface.
    """

    def run_with_error_handling(self, adapter: BaseAdapter, data: pd.DataFrame) -> "TestResult":
        """
        Run the validation test with error handling.
        
        Args:
            adapter: The adapter to validate
            data: Input data for validation
            
        Returns:
            TestResult object containing test results
            
        Raises:
            DataValidationError: If data validation fails
            ModelValidationError: If model validation fails
            MetricCalculationError: If metric calculation fails
            TestExecutionError: If test execution fails
        """

        # todo(): Review these, they are generated by LLM
        try:
            return self.run(adapter, data)
        except ZeroDivisionError as e:
            raise MetricCalculationError(f"Division by zero in {self.test_name} test: {str(e)}")
        except KeyError as e:
            raise DataValidationError(f"Missing required columns in {self.test_name} test: {str(e)}")
        except ValueError as e:
            raise MetricCalculationError(f"Invalid metric input in {self.test_name} test: {str(e)}")
        except IndexError as e:
            raise DataValidationError(f"Invalid data format in {self.test_name} test: {str(e)}")
        except Exception as e:
            raise TestExecutionError(f"Unexpected error in {self.test_name} test: {str(e)}")

    @abstractmethod
    def run(self, adapter: BaseAdapter, data: pd.DataFrame) -> "TestResult":
        """
        Run the validation test.

        Args:
            adapter: The adapter to validate
            data: Input data for validation

        Returns:
            TestResult object containing test results
        """
        pass

    @property
    @abstractmethod
    def test_name(self) -> str:
        """
        Return the name of the test.

        Returns:
            Test name (e.g., 'accuracy', 'stability')
        """
        pass

    def _split_data_holdout(
        self, data: pd.DataFrame
    ) -> tuple[pd.DataFrame, pd.DataFrame]:
        """
        Split the data into train and test sets.

        Args:
            data: The data to split

        Returns:
            train: The train data
            test: The test data
        """

        logger.info(f"Splitting data into train and test sets for {self.test_name} test")

        train, test = train_test_split(
            data,
            test_size=ValidationTestConstants.TRAIN_TEST_SPLIT_RATIO,
            random_state=ValidationTestConstants.RANDOM_STATE,
        )

        return train, test

    def _split_data_time_series_cv(
        self, data: pd.DataFrame
    ) -> tuple[pd.DataFrame, pd.DataFrame]:
        """
        Split the data into train and test sets using time series cross-validation.

        Args:
            data: The data to split

        Returns:
            train: The train data
            test: The test data
        """

        logger.info(f"Splitting data into train and test sets for {self.test_name} test")

        cv = TimeSeriesSplit(
            n_splits=ValidationTestConstants.N_SPLITS,
            test_size=ValidationTestConstants.TIME_SERIES_CROSS_VALIDATION_TEST_SIZE,
        )

        return list(cv.split(data))

    def _add_calculated_roi_column(
        self,
        data: pd.DataFrame,
        spend_col: InputDataframeConstants = InputDataframeConstants.MEDIA_CHANNEL_SPEND_COL,
        return_col: InputDataframeConstants = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,
    ) -> pd.DataFrame:
        """Calculate the ROI for the data."""
        data[ValidationDataframeConstants.CALCULATED_ROI_COL] = (
            data[return_col] - data[spend_col]
        ) / data[spend_col]
        return data

    def _aggregate_by_channel_and_sum(
        self,
        data: pd.DataFrame,
        channel_col: InputDataframeConstants = InputDataframeConstants.MEDIA_CHANNEL_COL,
        return_col: InputDataframeConstants = InputDataframeConstants.MEDIA_CHANNEL_REVENUE_COL,
        spend_col: InputDataframeConstants = InputDataframeConstants.MEDIA_CHANNEL_SPEND_COL,
    ) -> pd.DataFrame:
        """Aggregate the data to the media spend per channel."""
        logger.info(f"Aggregating data to the media spend per channel for {self.test_name} test")
        return (
            data.groupby(channel_col, dropna=False)[[spend_col, return_col]]
            .sum()
            .reset_index()
        )

    def _combine_dataframes_by_channel(
        self,
        baseline_df: pd.DataFrame,
        comparison_df: pd.DataFrame,
        suffixes: tuple[str, str],
    ) -> pd.DataFrame:
        """Combine the current and refresh data."""
        logger.info(f"Combining the current and refresh data for {self.test_name} test")
        return baseline_df.merge(
            comparison_df,
            on=[InputDataframeConstants.MEDIA_CHANNEL_COL],
            suffixes=suffixes,
            how="inner",
        )

    def _get_mean_aggregate_channel_roi_pct_change(self, df: pd.DataFrame) -> float:
        """Get the mean aggregate channel ROI pct change."""
        logger.info(f"Getting the mean aggregate channel ROI pct change for {self.test_name} test")
        return df[
            ValidationDataframeConstants.PERCENTAGE_CHANGE_CHANNEL_CONTRIBUTION_COL
        ].mean()
